{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-interview-deck-optimized-2024",
  "deck_config_uuid": "nlp-deck-config-optimized",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-deck-config-optimized",
      "dyn": false,
      "name": "NLP Interview Optimized",
      "new": {
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 30
      },
      "rev": {
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "perDay": 100
      }
    }
  ],
  "desc": "Bite-sized NLP interview cards optimized for mobile learning.",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "NLP Interview Prep (Optimized)",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-model-basic",
      "css": "\n.card {\n    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;\n    font-size: 16px;\n    text-align: left;\n    color: #333;\n    background-color: #fff;\n    padding: 15px;\n    max-width: 600px;\n    margin: 0 auto;\n}\npre {\n    background-color: #f6f8fa;\n    color: #24292e;\n    padding: 12px;\n    border-radius: 6px;\n    overflow-x: auto;\n    font-size: 14px;\n    line-height: 1.45;\n    font-family: 'SF Mono', Monaco, Consolas, monospace;\n}\ncode {\n    background-color: #f3f4f6;\n    padding: 2px 6px;\n    border-radius: 3px;\n    color: #e01e5a;\n    font-size: 14px;\n}\nb {\n    color: #0969da;\n    font-weight: 600;\n}\n/* Mobile optimizations */\n@media (max-width: 600px) {\n    .card { font-size: 15px; padding: 10px; }\n    pre { font-size: 12px; padding: 8px; }\n    code { font-size: 13px; }\n}\n            ",
      "flds": [
        {
          "name": "Front",
          "ord": 0
        },
        {
          "name": "Back",
          "ord": 1
        },
        {
          "name": "Topic",
          "ord": 2
        },
        {
          "name": "Type",
          "ord": 3
        }
      ],
      "name": "NLP Interview Card (Optimized)",
      "tmpls": [
        {
          "afmt": "{{FrontSide}}<hr id=answer>{{Back}}<br><br><small style='color:#666'>{{Topic}} • {{Type}}</small>",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "{{Front}}"
        }
      ],
      "type": 0
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>What is the key insight?",
        "**Time: 25 minutes** Implement scaled dot-product self-attention mechanism. ```python...<br><br>Think: What algorithm/approach?",
        "Attention Mechanisms",
        "problem_understanding"
      ],
      "guid": "nlp_e7ccb2b0a581e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "attention_mechanisms",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>Implement: softmax()",
        "<pre><code>def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"\n    Numerically stable softmax implementation.\n    \n    Why stable? Subtracting max prevents overflow when exponentiating large numbers.\n    This is critical for attention weights which can have large values.\n    \"\"\"\n    # Subtract maximum value for numerical stability\n    # This doesn't change the relative probabilities but prevents exp() overflow\n    x_max = np.max(x, axis=axis, keepdims=True)\n    exp_x = np.exp(x - x_max)\n    \n    # Normalize to get probabilities (sum to 1 along specified axis)\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_8181215935be9",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "attention_mechanisms",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>Implement: create_causal_mask()",
        "<pre><code>def create_causal_mask(seq_len: int) -> np.ndarray:\n    \"\"\"\n    Create causal mask for autoregressive attention.\n    \n    Used in GPT to prevent \"looking into the future\" during training.\n    \n    Returns:\n        Mask matrix where 1 = mask (don't attend), 0 = allow\n    \"\"\"\n    # Create lower triangular matrix (1s below and on diagonal)\n    # This allows attending to current and previous positions only\n    lower_triangle = np.tril(np.ones((seq_len, seq_len)))\n    \n    # Convert to mask format: 0 = allow attention, 1 = mask\n    return 1 - lower_triangle</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_a34c6d4abadd1",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "attention_mechanisms",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>Write the Attention Formula",
        "Attention(Q,K,V) = softmax",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_8bde05e194a5c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>Write the Attention Formula",
        "attention_weights = softmax",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_8bde05e194a5c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>Write the Attention Formula",
        "attention_weights = softmax",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_8bde05e194a5c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>What's the complexity?",
        "Time Complexity: Time complexity: O(n²d)<br>Space Complexity: Space complexity: O(n²)",
        "Attention Mechanisms",
        "complexity"
      ],
      "guid": "nlp_28d5ad7933b6b",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "attention_mechanisms",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>What is the key insight?",
        "**Time: 20 minutes** Implement a basic bag-of-words vectorizer. ```python...<br><br>Think: What algorithm/approach?",
        "BoW Vectors",
        "problem_understanding"
      ],
      "guid": "nlp_1eb6511f387f1",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "bow_vectors",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>Implement: analyze_vocabulary_distribution()",
        "<pre><code>def analyze_vocabulary_distribution(documents: List[str]) -> Dict[str, int]:\n    \"\"\"\n    Analyze vocabulary characteristics - good for follow-up questions.\n    \n    INTERVIEW INSIGHTS:\n    - Most words appear very rarely (Zipf's law)\n    - Top 100 words account for ~50% of text\n    - Vocabulary size grows with more documents\n    \"\"\"\n    vocab, _ = create_bow_vector(documents)\n    \n    # Count how often each word appears across documents\n    word_doc_counts = {}\n    for word in vocab:\n        count = sum(1 for doc in documents if word in doc.lower())\n        word_doc_counts[word] = count\n    \n    return word_doc_counts</code></pre>",
        "BoW Vectors",
        "implementation"
      ],
      "guid": "nlp_035b36dcccfa4",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "bow_vectors",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>What's the complexity?",
        "Time Complexity: Time: O(d × n × v)<br>Space Complexity: Space: O(d × v)",
        "BoW Vectors",
        "complexity"
      ],
      "guid": "nlp_45fe588623ed1",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "bow_vectors",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>What edge cases to handle?",
        "<code>if not documents:\n        return<br>if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return</code>",
        "BoW Vectors",
        "edge_cases"
      ],
      "guid": "nlp_5854ab5d92cb8",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "bow_vectors",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>What is the key insight?",
        "**Time: 25 minutes** Implement a simple CNN for text classification using basic operations. ```python...<br><br>Think: What algorithm/approach?",
        "CNN Text",
        "problem_understanding"
      ],
      "guid": "nlp_c973c04d2145c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "cnn_text",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>Implement: text_to_sequence()",
        "<pre><code>def text_to_sequence(text: str, vocab: Dict[str, int], max_len: int = 10) -> List[int]:\n    \"\"\"Convert text to padded integer sequence.\"\"\"\n    words = text.lower().split()\n    sequence = [vocab.get(word, 0) for word in words]  # 0 for unknown words\n    \n    # Pad or truncate to max_len\n    if len(sequence) < max_len:\n        sequence += [0] * (max_len - len(sequence))\n    else:\n        sequence = sequence[:max_len]\n    \n    return sequence</code></pre>",
        "CNN Text",
        "implementation"
      ],
      "guid": "nlp_3b0a1aee22da4",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "cnn_text",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>Implement: embedding_lookup()",
        "<pre><code>def embedding_lookup(sequence: List[int], embedding_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Look up embeddings for sequence.\"\"\"\n    return np.array([embedding_matrix[idx] for idx in sequence])</code></pre>",
        "CNN Text",
        "implementation"
      ],
      "guid": "nlp_ff8e6e18b0e35",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "cnn_text",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>Implement: conv1d()",
        "<pre><code>def conv1d(embeddings: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n    \"\"\"Apply 1D convolution with kernel size 3.\"\"\"\n    seq_len, embed_dim = embeddings.shape\n    kernel_size = len(kernel)\n    \n    conv_output = []\n    \n    # Slide kernel over sequence\n    for i in range(seq_len - kernel_size + 1):\n        window = embeddings[i:i + kernel_size]  # Shape: (3, embed_dim)\n        \n        # Element-wise multiply and sum\n        conv_value = np.sum(window * kernel[:, np.newaxis])\n        conv_output.append(max(0, conv_value))  # ReLU activation\n    \n    return np.array(conv_output)</code></pre>",
        "CNN Text",
        "implementation"
      ],
      "guid": "nlp_3234617acdc4a",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "cnn_text",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>Implement: max_pool()",
        "<pre><code>def max_pool(conv_output: np.ndarray) -> float:\n    \"\"\"Global max pooling.\"\"\"\n    return np.max(conv_output) if len(conv_output) > 0 else 0.0</code></pre>",
        "CNN Text",
        "implementation"
      ],
      "guid": "nlp_6b536ab1dc1dc",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "cnn_text",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>Implement: sigmoid()",
        "<pre><code>def sigmoid(x: float) -> float:\n    \"\"\"Sigmoid activation.\"\"\"\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))</code></pre>",
        "CNN Text",
        "implementation"
      ],
      "guid": "nlp_67bfacd53e88a",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "cnn_text",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip-gram</b><br>What is the key insight?",
        "**Time: 20 minutes** Implement the core Skip-gram training step for Word2Vec. ```python...<br><br>Think: What algorithm/approach?",
        "Embeddings",
        "problem_understanding"
      ],
      "guid": "nlp_019776a1751bb",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "embeddings",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip-gram</b><br>Implement: sigmoid()",
        "<pre><code>def sigmoid(x: float) -> float:\n    \"\"\"\n    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n    \n    Used in Word2Vec to convert dot products to probabilities.\n    Clamp input to prevent numerical overflow/underflow.\n    \"\"\"\n    # Clamp x to prevent overflow in exp() function\n    # This is crucial for numerical stability\n    clamped_x = max(-500, min(500, x))\n    return 1 / (1 + math.exp(-clamped_x))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_9c5c7fb456569",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "embeddings",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip-gram</b><br>Implement: dot_product()",
        "<pre><code>def dot_product(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"\n    Calculate dot product between two vectors.\n    \n    Dot product measures similarity between vectors:\n    - High positive value = vectors point in same direction (similar)\n    - Zero = vectors are orthogonal (unrelated)\n    - Negative = vectors point in opposite directions (dissimilar)\n    \"\"\"\n    # Element-wise multiplication, then sum\n    return sum(a * b for a, b in zip(vec1, vec2))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_dadae64b59adf",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "embeddings",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip-gram</b><br>Write the Cosine Similarity",
        "Cosine similarity = dot_product(v1, v2) /",
        "Embeddings",
        "formula"
      ],
      "guid": "nlp_62caaeeada9db",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "embeddings",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki-Optimized Solution Structure</b><br>What is the key insight?",
        "**Time: 20 minutes** Implement a simple spell checker that: 1. Calculates edit distance between words...<br><br>Think: What algorithm/approach?",
        "Example Anki Refactor",
        "problem_understanding"
      ],
      "guid": "nlp_fbd157436e9c2",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "example_anki_refactor",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki-Optimized Solution Structure</b><br>Implement: spell_check_approach()",
        "<pre><code>def spell_check_approach():\n    \"\"\"\n    KEY: Use edit distance + frequency ranking\n    STEPS: 1) Find close words 2) Rank by frequency\n    INSIGHT: Most typos are 1-2 edits away\n    \"\"\"\n    pass</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_b4a7ef8f63b56",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "example_anki_refactor",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki-Optimized Solution Structure</b><br>Implement: edit_distance_recursive()",
        "<pre><code>def edit_distance_recursive(s1: str, s2: str) -> int:\n    \"\"\"\n    FORMULA: ED(i,j) = min(\n        ED(i-1,j) + 1,    # deletion\n        ED(i,j-1) + 1,    # insertion  \n        ED(i-1,j-1) + 0/1 # substitution\n    )\n    \"\"\"\n    if not s1: return len(s2)\n    if not s2: return len(s1)\n    \n    if s1[0] == s2[0]:\n        return edit_distance_recursive(s1[1:], s2[1:])\n    \n    return 1 + min(\n        edit_distance_recursive(s1[1:], s2),    # delete\n        edit_distance_recursive(s1, s2[1:]),    # insert\n        edit_distance_recursive(s1[1:], s2[1:]) # replace\n    )</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_e5a6df6430294",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "example_anki_refactor",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki-Optimized Solution Structure</b><br>Implement: rank_by_frequency()",
        "<pre><code>def rank_by_frequency(candidates: List[Tuple[str, int]], \n                     dictionary: Dict[str, int]) -> List[str]:\n    \"\"\"\n    FORMULA: score = frequency / (distance + 1)\n    KEY: Higher frequency, lower distance = better\n    \"\"\"\n    scored = []\n    \n    for word, distance in candidates:\n        freq = dictionary.get(word, 1)\n        score = freq / (distance + 1)\n        scored.append((word, score))\n    \n    # Sort by score descending\n    scored.sort(key=lambda x: x[1], reverse=True)\n    \n    return [word for word, _ in scored[:3]]</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_a4b3809808b4f",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "example_anki_refactor",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki-Optimized Solution Structure</b><br>What's the complexity?",
        "Time Complexity: time, O(m*n)",
        "Example Anki Refactor",
        "complexity"
      ],
      "guid": "nlp_7cb4c9ec2951a",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "example_anki_refactor",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki-Optimized Solution Structure</b><br>What edge cases to handle?",
        "<code>if not s1: return<br>if not s2: return<br>if not dictionary:\n        return</code>",
        "Example Anki Refactor",
        "edge_cases"
      ],
      "guid": "nlp_523203d4ce032",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "example_anki_refactor",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>What is the key insight?",
        "**Time: 25 minutes** Implement the key components for fine-tuning a pre-trained LLM for text classification. ```python...<br><br>Think: What algorithm/approach?",
        "Fine Tuning",
        "problem_understanding"
      ],
      "guid": "nlp_ffacf4a87aeaa",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "fine_tuning",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>Implement: add_classification_head()",
        "<pre><code>def add_classification_head(pretrained_model_dim: int, num_classes: int) -> Dict:\n    \"\"\"Add classification head to pretrained LLM.\"\"\"\n    \n    # Xavier/Glorot initialization for stable training\n    std = np.sqrt(2.0 / (pretrained_model_dim + num_classes))\n    \n    return {\n        'W_cls': np.random.randn(pretrained_model_dim, num_classes) * std,\n        'b_cls': np.zeros(num_classes)\n    }</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_3792ca16d1331",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "fine_tuning",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>Implement: compute_classification_loss()",
        "<pre><code>def compute_classification_loss(logits: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Compute cross-entropy loss with numerical stability.\"\"\"\n    batch_size, num_classes = logits.shape\n    \n    # Numerical stability: subtract max from logits\n    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n    \n    # Softmax probabilities\n    exp_logits = np.exp(logits_stable)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Cross-entropy loss\n    loss = 0.0\n    for i in range(batch_size):\n        true_class = labels[i]\n        loss += -np.log(probs[i, true_class] + 1e-10)  # Add small epsilon\n    \n    return loss / batch_size</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_e501062061c92",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "fine_tuning",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>Implement: freeze_layers()",
        "<pre><code>def freeze_layers(model_weights: Dict, freeze_ratio: float = 0.8) -> Dict:\n    \"\"\"Mark layers as frozen (simulate requires_grad=False).\"\"\"\n    frozen_info = {}\n    \n    # Sort layers by name to freeze bottom layers\n    layer_names = sorted([name for name in model_weights.keys() if 'layer_' in name])\n    \n    num_layers = len(layer_names)\n    num_frozen = int(num_layers * freeze_ratio)\n    \n    for i, layer_name in enumerate(layer_names):\n        frozen_info[layer_name] = i < num_frozen  # True if frozen\n    \n    # Never freeze classification head\n    for name in model_weights.keys():\n        if 'cls' in name:\n            frozen_info[name] = False\n    \n    return frozen_info</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_37486dfea9846",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "fine_tuning",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>Implement: compute_accuracy()",
        "<pre><code>def compute_accuracy(logits: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Compute classification accuracy.\"\"\"\n    predictions = np.argmax(logits, axis=1)\n    return np.mean(predictions == labels)</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_c9ea5d7cffe73",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "fine_tuning",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>Write the Loss Function",
        "loss += -np.log",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_d2109e4560ca3",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>Write the Loss Function",
        "loss = compute_classification_loss(log",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_d2109e4560ca3",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>What is the key insight?",
        "**Time: 30 minutes** Implement a single GPT transformer block with the standard architecture. ```python...<br><br>Think: What algorithm/approach?",
        "GPT Implementation",
        "problem_understanding"
      ],
      "guid": "nlp_0cd68d92c3681",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "gpt_implementation",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>Implement: gelu()",
        "<pre><code>def gelu(x: np.ndarray) -> np.ndarray:\n    \"\"\"GELU activation function used in GPT.\"\"\"\n    return 0.5 * x * (1 + np.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))</code></pre>",
        "GPT Implementation",
        "implementation"
      ],
      "guid": "nlp_87f5c6f770a04",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "gpt_implementation",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>Implement: layer_norm()",
        "<pre><code>def layer_norm(x: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n    \"\"\"Apply layer normalization.\"\"\"\n    # Calculate mean and variance along last dimension\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    \n    # Normalize\n    normalized = (x - mean) / np.sqrt(variance + eps)\n    \n    # Scale and shift\n    return gamma * normalized + beta</code></pre>",
        "GPT Implementation",
        "implementation"
      ],
      "guid": "nlp_100d89cbcd6d2",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "gpt_implementation",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>Implement: feed_forward()",
        "<pre><code>def feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray, \n                W2: np.ndarray, b2: np.ndarray) -> np.ndarray:\n    \"\"\"Feed-forward network with GELU activation.\"\"\"\n    # First layer\n    hidden = gelu(x @ W1 + b1)\n    \n    # Second layer  \n    output = hidden @ W2 + b2\n    \n    return output</code></pre>",
        "GPT Implementation",
        "implementation"
      ],
      "guid": "nlp_4f2bb443fb892",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "gpt_implementation",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>Implement: gpt_block()",
        "<pre><code>def gpt_block(x: np.ndarray, weights: Dict) -> np.ndarray:\n    \"\"\"Single GPT transformer block.\"\"\"\n    seq_len, d_model = x.shape\n    \n    # 1. Layer norm + self-attention + residual\n    norm1 = layer_norm(x, weights['ln1_gamma'], weights['ln1_beta'])\n    attn_out = causal_self_attention(norm1, weights['W_qkv'], weights['W_out'])\n    x = x + attn_out  # Residual connection\n    \n    # 2. Layer norm + feed-forward + residual  \n    norm2 = layer_norm(x, weights['ln2_gamma'], weights['ln2_beta'])\n    ffn_out = feed_forward(norm2, weights['W1'], weights['b1'], weights['W2'], weights['b2'])\n    x = x + ffn_out  # Residual connection\n    \n    return x</code></pre>",
        "GPT Implementation",
        "implementation"
      ],
      "guid": "nlp_17995da974baf",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "gpt_implementation",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Instruction Following Setup</b><br>What is the key insight?",
        "**Time: 20 minutes** Implement the data preparation and loss calculation for instruction fine-tuning. ```python...<br><br>Think: What algorithm/approach?",
        "Instruction Tuning",
        "problem_understanding"
      ],
      "guid": "nlp_0e79dced25855",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "instruction_tuning",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Instruction Following Setup</b><br>Implement: format_instruction_data()",
        "<pre><code>def format_instruction_data(instruction: str, response: str) -> str:\n    \"\"\"Format instruction-response pair for training.\"\"\"\n    return f\"### Instruction:\\n{instruction}\\n### Response:\\n{response}\"</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_3b353a60f6af9",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "instruction_tuning",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Instruction Following Setup</b><br>Write the Loss Function",
        "loss += -np.log",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_97f4699ed482c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Instruction Following Setup</b><br>Write the Loss Function",
        "loss = compute_instruction_loss(mock_log",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_97f4699ed482c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Generation with LLMs</b><br>What is the key insight?",
        "**Time: 25 minutes** Implement text generation from a trained language model with different decoding strategies. ```python...<br><br>Think: What algorithm/approach?",
        "LLM Fundamentals",
        "problem_understanding"
      ],
      "guid": "nlp_98c9d34f838c5",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "llm_fundamentals",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Generation with LLMs</b><br>Implement: softmax()",
        "<pre><code>def softmax(logits: List[float], temperature: float = 1.0) -> List[float]:\n    \"\"\"Convert logits to probabilities with temperature scaling.\"\"\"\n    if temperature != 1.0:\n        logits = [l / temperature for l in logits]\n    \n    max_logit = max(logits)\n    exp_logits = [math.exp(l - max_logit) for l in logits]\n    sum_exp = sum(exp_logits)\n    \n    return [exp_l / sum_exp for exp_l in exp_logits]</code></pre>",
        "LLM Fundamentals",
        "implementation"
      ],
      "guid": "nlp_8198db413f1c3",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "llm_fundamentals",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Generation with LLMs</b><br>Implement: mock_language_model()",
        "<pre><code>def mock_language_model(tokens: List[int]) -> List[float]:\n    \"\"\"Mock language model that returns random logits.\"\"\"\n    vocab_size = 50\n    np.random.seed(sum(tokens) % 100)  # Deterministic based on input\n    return np.random.randn(vocab_size).tolist()</code></pre>",
        "LLM Fundamentals",
        "implementation"
      ],
      "guid": "nlp_d772d90420c67",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "llm_fundamentals",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Evaluation Metrics</b><br>What is the key insight?",
        "**Time: 20 minutes** Implement key evaluation metrics for large language models. ```python...<br><br>Think: What algorithm/approach?",
        "Model Evaluation",
        "problem_understanding"
      ],
      "guid": "nlp_18ccc7bc1a495",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "model_evaluation",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Evaluation Metrics</b><br>Implement: get_ngrams()",
        "<pre><code>def get_ngrams(text: str, n: int) -> List[str]:\n    \"\"\"Extract n-grams from text.\"\"\"\n    words = text.lower().split()\n    ngrams = []\n    \n    for i in range(len(words) - n + 1):\n        ngram = ' '.join(words[i:i + n])\n        ngrams.append(ngram)\n    \n    return ngrams</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_e1ca1f4b754a2",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "model_evaluation",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Evaluation Metrics</b><br>Write the Evaluation Metric",
        "precision = matches /",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_76c88d8981979",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Evaluation Metrics</b><br>Write the Loss Function",
        "loss += -math.log",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_b42b94a9a4f2c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Evaluation Metrics</b><br>What edge cases to handle?",
        "<code>if not model_probs or not target_tokens:\n        return<br>if not cand_words:\n        return<br>if not logits or not targets or len(logits) != len(targets):\n        return</code>",
        "Model Evaluation",
        "edge_cases"
      ],
      "guid": "nlp_46b1371a0f0b1",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "model_evaluation",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Named Entity Recognition with Custom Entities</b><br>What is the key insight?",
        "Implement `extract_entities(text: str) -> Dict[str, List[str]]` that: 1. Extracts standard entities (PERSON, ORG, GPE, DATE, MONEY) 2. Returns entitie...<br><br>Think: What algorithm/approach?",
        "NER",
        "problem_understanding"
      ],
      "guid": "nlp_de97e548d1952",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "ner",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Named Entity Recognition with Custom Entities</b><br>Implement: extract_entities()",
        "<pre><code>def extract_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract named entities using spaCy.\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        entities[ent.label_].append(ent.text)\n    \n    # Remove duplicates while preserving order\n    for label in entities:\n        entities[label] = list(dict.fromkeys(entities[label]))\n    \n    return dict(entities)</code></pre>",
        "NER",
        "implementation"
      ],
      "guid": "nlp_017f2f7ca063f",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "ner",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: N-gram Language Model</b><br>What is the key insight?",
        "**Time: 25 minutes** Implement a simple bigram language model with probability calculation. ```python...<br><br>Think: What algorithm/approach?",
        "NGrams",
        "problem_understanding"
      ],
      "guid": "nlp_3485cd5bba422",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "ngrams",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: N-gram Language Model</b><br>Implement: calculate_probability()",
        "<pre><code>def calculate_probability(model: Dict, text: str) -> float:\n    \"\"\"Calculate probability of text under the model.\"\"\"\n    words = ['<START>'] + text.lower().split() + ['<END>']\n    \n    prob = 1.0\n    for i in range(len(words) - 1):\n        w1, w2 = words[i], words[i + 1]\n        \n        if w1 in model and w2 in model[w1]:\n            prob *= model[w1][w2]\n        else:\n            return 0.0  # Unseen bigram\n    \n    return prob</code></pre>",
        "NGrams",
        "implementation"
      ],
      "guid": "nlp_96765ce623452",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "ngrams",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: N-gram Language Model</b><br>What edge cases to handle?",
        "<code>if not texts:\n        return</code>",
        "NGrams",
        "edge_cases"
      ],
      "guid": "nlp_73d80b44d4e4e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "ngrams",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Neural Network from Scratch</b><br>What is the key insight?",
        "Implement basic neural networks from scratch: 1. `Perceptron(input_size: int, learning_rate: float)` - Single perceptron 2. `NeuralNetwork(layers: Lis...<br><br>Think: What algorithm/approach?",
        "Neural Fundamentals",
        "problem_understanding"
      ],
      "guid": "nlp_9027566139e42",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "neural_fundamentals",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Neural Network from Scratch</b><br>Implement: generate_xor_data()",
        "<pre><code>def generate_xor_data() -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate XOR problem data.\"\"\"\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n    y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n    return X, y</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_bc3dfa8e5199c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "neural_fundamentals",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Neural Network from Scratch</b><br>Implement: generate_spiral_data()",
        "<pre><code>def generate_spiral_data(n_samples: int = 100, n_classes: int = 3) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate spiral classification data.\"\"\"\n    X = np.zeros((n_samples * n_classes, 2))\n    y = np.zeros((n_samples * n_classes, n_classes))\n    \n    for class_idx in range(n_classes):\n        ix = range(n_samples * class_idx, n_samples * (class_idx + 1))\n        r = np.linspace(0.0, 1, n_samples)  # radius\n        t = np.linspace(class_idx * 4, (class_idx + 1) * 4, n_samples) + np.random.randn(n_samples) * 0.2\n        \n        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        y[ix, class_idx] = 1\n    \n    return X, y</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_183d2e0cf9eb5",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "neural_fundamentals",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Neural Network from Scratch</b><br>Implement: sigmoid()",
        "<pre><code>    def sigmoid(x: np.ndarray) -> np.ndarray:\n        \"\"\"Sigmoid activation function.\"\"\"\n        # Prevent overflow\n        x = np.clip(x, -500, 500)\n        return 1 / (1 + np.exp(-x))</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_adde5760cd01a",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "neural_fundamentals",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Part-of-Speech Tagging with Accuracy Metrics</b><br>What is the key insight?",
        "Implement `pos_tag_text(text: str) -> List[Tuple[str, str]]` that: 1. Tags each word with its part-of-speech 2. Handles ambiguous words correctly...<br><br>Think: What algorithm/approach?",
        "POS Tagging",
        "problem_understanding"
      ],
      "guid": "nlp_5e079b77aa061",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "pos_tagging",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Part-of-Speech Tagging with Accuracy Metrics</b><br>Implement: pos_tag_text()",
        "<pre><code>def pos_tag_text(text: str) -> List[Tuple[str, str]]:\n    \"\"\"POS tag text using NLTK's default tagger (Penn Treebank tagset).\"\"\"\n    tokens = nltk.word_tokenize(text)\n    return nltk.pos_tag(tokens)</code></pre>",
        "POS Tagging",
        "implementation"
      ],
      "guid": "nlp_1553ac908830b",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "pos_tagging",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Part-of-Speech Tagging with Accuracy Metrics</b><br>Implement: extract_pos()",
        "<pre><code>def extract_pos(tagged_text: List[Tuple[str, str]], pos_prefix: str) -> List[str]:\n    \"\"\"Extract words with specific POS tags.\n    \n    Args:\n        tagged_text: List of (word, pos) tuples\n        pos_prefix: POS tag prefix (e.g., 'NN' for nouns, 'VB' for verbs)\n    \"\"\"\n    return [word for word, pos in tagged_text if pos.startswith(pos_prefix)]</code></pre>",
        "POS Tagging",
        "implementation"
      ],
      "guid": "nlp_b7fa79873b60e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "pos_tagging",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Part-of-Speech Tagging with Accuracy Metrics</b><br>Implement: analyze_word_ambiguity()",
        "<pre><code>def analyze_word_ambiguity(word: str, corpus_name: str = 'brown') -> Dict[str, float]:\n    \"\"\"Analyze POS tag distribution for an ambiguous word.\"\"\"\n    from nltk.corpus import brown\n    \n    # Get all occurrences of the word with their tags\n    word_lower = word.lower()\n    pos_counts = Counter()\n    \n    for sent in brown.tagged_sents(tagset='universal')[:10000]:  # Sample for speed\n        for token, pos in sent:\n            if token.lower() == word_lower:\n                pos_counts[pos] += 1\n    \n    total = sum(pos_counts.values())\n    if total == 0:\n        return {}\n    \n    return {pos: count/total for pos, count in pos_counts.items()}</code></pre>",
        "POS Tagging",
        "implementation"
      ],
      "guid": "nlp_9d96474eff4c6",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "pos_tagging",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Part-of-Speech Tagging with Accuracy Metrics</b><br>Implement: compare_taggers()",
        "<pre><code>def compare_taggers(text: str) -> Dict[str, List[Tuple[str, str]]]:\n    \"\"\"Compare different POS taggers on the same text.\"\"\"\n    tokens = nltk.word_tokenize(text)\n    \n    results = {\n        'default': nltk.pos_tag(tokens),\n        'universal': nltk.pos_tag(tokens, tagset='universal')\n    }\n    \n    # Try to use spaCy if available\n    try:\n        import spacy\n        nlp = spacy.load('en_core_web_sm')\n        doc = nlp(text)\n        results['spacy'] = [(token.text, token.pos_) for token in doc]\n    except:\n        pass\n    \n    return results</code></pre>",
        "POS Tagging",
        "implementation"
      ],
      "guid": "nlp_74d7fd6e621f9",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "pos_tagging",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Regular Expressions for NLP</b><br>What is the key insight?",
        "Implement regex-based NLP functions: 1. `extract_entities_regex(text: str) -> Dict[str, List[str]]`    - Extract emails, phones, URLs, dates, money am...<br><br>Think: What algorithm/approach?",
        "Regex NLP",
        "problem_understanding"
      ],
      "guid": "nlp_e77a3ec7f4523",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "regex_nlp",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Regular Expressions for NLP</b><br>Implement: clean_text_regex()",
        "<pre><code>def clean_text_regex(text: str, rules: List[Tuple[str, str]]) -> str:\n    \"\"\"Apply multiple regex cleaning rules to text.\n    \n    Args:\n        text: Input text to clean\n        rules: List of (pattern, replacement) tuples\n    \"\"\"\n    cleaned_text = text\n    \n    for pattern, replacement in rules:\n        cleaned_text = re.sub(pattern, replacement, cleaned_text)\n    \n    return cleaned_text</code></pre>",
        "Regex NLP",
        "implementation"
      ],
      "guid": "nlp_5e4a69dbaf2ea",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "regex_nlp",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Rule-based Sentiment Analysis</b><br>What is the key insight?",
        "**Time: 20 minutes** Implement a simple rule-based sentiment analyzer. ```python...<br><br>Think: What algorithm/approach?",
        "Sentiment Analysis",
        "problem_understanding"
      ],
      "guid": "nlp_210c30aa03026",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sentiment_analysis",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Rule-based Sentiment Analysis</b><br>Implement: classify_sentiment()",
        "<pre><code>def classify_sentiment(sentiment_scores: Dict[str, float]) -> str:\n    \"\"\"\n    Convert sentiment scores to simple classification.\n    \n    CLASSIFICATION THRESHOLDS (VADER standard):\n    - compound >= 0.05: positive\n    - compound <= -0.05: negative  \n    - -0.05 < compound < 0.05: neutral\n    \n    These thresholds are empirically determined from testing.\n    \"\"\"\n    compound = sentiment_scores['compound']\n    \n    if compound >= 0.05:\n        return 'positive'\n    elif compound <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_60b85a554ce4a",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sentiment_analysis",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Rule-based Sentiment Analysis</b><br>What edge cases to handle?",
        "<code>if not text or not text.strip():\n        # Return neutral sentiment for empty text\n        return<br>if not words:\n        return</code>",
        "Sentiment Analysis",
        "edge_cases"
      ],
      "guid": "nlp_84e079d2a10d2",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sentiment_analysis",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>What is the key insight?",
        "**Time: 25 minutes** Implement a basic LSTM cell for sentiment classification. ```python...<br><br>Think: What algorithm/approach?",
        "Sequence Models",
        "problem_understanding"
      ],
      "guid": "nlp_0085f54f762dc",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sequence_models",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>Implement: sigmoid()",
        "<pre><code>def sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-max(-500, min(500, x))))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_a438af6b31c04",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sequence_models",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>Implement: tanh()",
        "<pre><code>def tanh(x: float) -> float:\n    return math.tanh(max(-500, min(500, x)))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_2523713c430cf",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sequence_models",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>Implement: dot_product()",
        "<pre><code>def dot_product(vec1: List[float], vec2: List[float]) -> float:\n    return sum(a * b for a, b in zip(vec1, vec2))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_14a7a8cbc4c83",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sequence_models",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>Implement: lstm_cell()",
        "<pre><code>def lstm_cell(x_t: List[float], h_prev: List[float], c_prev: List[float],\n              weights: Dict) -> Tuple[List[float], List[float]]:\n    \"\"\"Single LSTM cell forward pass.\"\"\"\n    hidden_size = len(h_prev)\n    combined = x_t + h_prev\n    \n    # Gates: forget, input, output  \n    f_t = [sigmoid(dot_product(combined, weights['Wf'][i]) + weights['bf'][i]) for i in range(hidden_size)]\n    i_t = [sigmoid(dot_product(combined, weights['Wi'][i]) + weights['bi'][i]) for i in range(hidden_size)]\n    o_t = [sigmoid(dot_product(combined, weights['Wo'][i]) + weights['bo'][i]) for i in range(hidden_size)]\n    \n    # Candidate cell state\n    c_candidate = [tanh(dot_product(combined, weights['Wc'][i]) + weights['bc'][i]) for i in range(hidden_size)]\n    \n    # Update cell and hidden states\n    c_t = [f_t[i] * c_prev[i] + i_t[i] * c_candidate[i] for i in range(hidden_size)]\n    h_t = [o_t[i] * tanh(c_t[i]) for i in range(hidden_size)]\n    \n    return h_t, c_t</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_0aa4974dc84ba",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sequence_models",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>Implement: lstm_sentiment()",
        "<pre><code>def lstm_sentiment(sequence: List[List[float]], weights: Dict) -> float:\n    \"\"\"Run LSTM over sequence and classify sentiment.\"\"\"\n    if not sequence:\n        return 0.5\n    \n    hidden_size = len(weights['bf'])\n    h_t = [0.0] * hidden_size\n    c_t = [0.0] * hidden_size\n    \n    # Process sequence\n    for x_t in sequence:\n        h_t, c_t = lstm_cell(x_t, h_t, c_t, weights)\n    \n    # Final classification\n    logit = dot_product(h_t, weights['W_output']) + weights['b_output']\n    return sigmoid(logit)</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_f391abd7fcf02",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sequence_models",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>What edge cases to handle?",
        "<code>if not sequence:\n        return</code>",
        "Sequence Models",
        "edge_cases"
      ],
      "guid": "nlp_5ab19c3136015",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "sequence_models",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Similarity Metrics</b><br>What is the key insight?",
        "Implement multiple similarity metrics: 1. `cosine_similarity(text1: str, text2: str) -> float` 2. `jaccard_similarity(text1: str, text2: str) -> float...<br><br>Think: What algorithm/approach?",
        "Similarity",
        "problem_understanding"
      ],
      "guid": "nlp_dc94ca24c79d8",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "similarity",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Similarity Metrics</b><br>Implement: tokenize()",
        "<pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"Simple tokenization.\"\"\"\n    return text.lower().split()</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_a733bc7ac85e4",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "similarity",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Similarity Metrics</b><br>Implement: jaccard_similarity()",
        "<pre><code>def jaccard_similarity(text1: str, text2: str, ngram_size: int = 1) -> float:\n    \"\"\"Calculate Jaccard similarity (intersection over union).\"\"\"\n    if ngram_size == 1:\n        # Word-level Jaccard\n        set1 = set(tokenize(text1))\n        set2 = set(tokenize(text2))\n    else:\n        # N-gram Jaccard\n        set1 = set(get_ngrams(text1, ngram_size))\n        set2 = set(get_ngrams(text2, ngram_size))\n    \n    if not set1 and not set2:\n        return 1.0\n    if not set1 or not set2:\n        return 0.0\n    \n    intersection = len(set1 & set2)\n    union = len(set1 | set2)\n    \n    return intersection / union</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_afcdc5da6810c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "similarity",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Similarity Metrics</b><br>Implement: get_ngrams()",
        "<pre><code>def get_ngrams(text: str, n: int) -> List[str]:\n    \"\"\"Extract n-grams from text.\"\"\"\n    tokens = tokenize(text)\n    ngrams = []\n    \n    for i in range(len(tokens) - n + 1):\n        ngram = ' '.join(tokens[i:i+n])\n        ngrams.append(ngram)\n    \n    return ngrams</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_e14f9107d3547",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "similarity",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Similarity Metrics</b><br>What edge cases to handle?",
        "<code>if not set1 and not set2:\n        return<br>if not set1 or not set2:\n        return<br>if not text1:\n        return</code>",
        "Similarity",
        "edge_cases"
      ],
      "guid": "nlp_96b243cde8dce",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "similarity",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Stemming vs Lemmatization</b><br>What is the key insight?",
        "Implement two functions: 1. `stem_words(words: List[str]) -> List[str]` - Porter stemming 2. `lemmatize_words(words: List[str], pos_tags: Optional[Lis...<br><br>Think: What algorithm/approach?",
        "Stemming Lemmatization",
        "problem_understanding"
      ],
      "guid": "nlp_ed0fd94654330",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "stemming_lemmatization",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Stemming vs Lemmatization</b><br>Implement: get_wordnet_pos()",
        "<pre><code>def get_wordnet_pos(treebank_tag):\n    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags.\"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN  # default</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_209915d7b1826",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "stemming_lemmatization",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Stemming vs Lemmatization</b><br>Implement: stem_words()",
        "<pre><code>def stem_words(words: List[str]) -> List[str]:\n    \"\"\"Apply Porter stemming to words.\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word.lower()) for word in words]</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_f85a4ba403b5c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "stemming_lemmatization",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Stemming vs Lemmatization</b><br>Implement: lemmatize_words()",
        "<pre><code>def lemmatize_words(words: List[str], pos_tags: Optional[List[str]] = None) -> List[str]:\n    \"\"\"Lemmatize words with optional POS tags for better accuracy.\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    \n    if pos_tags is None:\n        # Simple lemmatization without POS\n        return [lemmatizer.lemmatize(word.lower()) for word in words]\n    \n    # Lemmatize with POS tags\n    lemmatized = []\n    for word, pos in zip(words, pos_tags):\n        wordnet_pos = get_wordnet_pos(pos)\n        lemmatized.append(lemmatizer.lemmatize(word.lower(), pos=wordnet_pos))\n    \n    return lemmatized</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_dbb95350fdcca",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "stemming_lemmatization",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Stemming vs Lemmatization</b><br>Implement: compare_methods()",
        "<pre><code>def compare_methods(words: List[str]) -> dict:\n    \"\"\"Compare stemming vs lemmatization results.\"\"\"\n    import nltk\n    \n    # Get POS tags\n    pos_tags = [pos for _, pos in nltk.pos_tag(words)]\n    \n    stemmed = stem_words(words)\n    lemmatized_simple = lemmatize_words(words)\n    lemmatized_pos = lemmatize_words(words, pos_tags)\n    \n    return {\n        'original': words,\n        'stemmed': stemmed,\n        'lemmatized_simple': lemmatized_simple,\n        'lemmatized_with_pos': lemmatized_pos,\n        'pos_tags': pos_tags\n    }</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_c491998663463",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "stemming_lemmatization",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Remove Stopwords (Configurable)</b><br>What is the key insight?",
        "Implement `remove_stopwords(tokens: List[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]` that removes English stopwords from a token l...<br><br>Think: What algorithm/approach?",
        "Stop Word Removal",
        "problem_understanding"
      ],
      "guid": "nlp_1baaba24822d3",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "stop_word_removal",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Remove Stopwords (Configurable)</b><br>Implement: remove_stopwords()",
        "<pre><code>def remove_stopwords(tokens: Iterable[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]:\n    \"\"\"Remove stopwords, preserving order.\n\n    Case-insensitive membership check, preserves original casing in the output.\n    \"\"\"\n    stop_set = set(ENGLISH_STOPWORDS)\n    if extra_stopwords:\n        stop_set |= {w.lower() for w in extra_stopwords}\n\n    cleaned: List[str] = []\n    for token in tokens:\n        if token and token.lower() not in stop_set:\n            cleaned.append(token)\n    return cleaned</code></pre>",
        "Stop Word Removal",
        "implementation"
      ],
      "guid": "nlp_bd71019075bf7",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "stop_word_removal",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF-IDF Implementation</b><br>What is the key insight?",
        "**Time: 30 minutes** Implement TF-IDF from scratch to find document similarity. ```python...<br><br>Think: What algorithm/approach?",
        "TFIDF",
        "problem_understanding"
      ],
      "guid": "nlp_4d1c0cae2100e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tfidf",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF-IDF Implementation</b><br>Write the TF-IDF Formula",
        "IDF calculation: IDF = log(total_docs / doc_freq)",
        "TFIDF",
        "formula"
      ],
      "guid": "nlp_a0fa466b3825e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF-IDF Implementation</b><br>Write the TF-IDF Formula",
        "idf = math.log(num_docs / doc_freq[term])",
        "TFIDF",
        "formula"
      ],
      "guid": "nlp_a0fa466b3825e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF-IDF Implementation</b><br>Write the TF-IDF Formula",
        "idf = math.log(len(docs)",
        "TFIDF",
        "formula"
      ],
      "guid": "nlp_a0fa466b3825e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF-IDF Implementation</b><br>What's the complexity?",
        "Time Complexity: Time complexity: O(d×v)<br>Space Complexity: Space complexity: O(d×v)",
        "TFIDF",
        "complexity"
      ],
      "guid": "nlp_404856c9dac7b",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tfidf",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF-IDF Implementation</b><br>What edge cases to handle?",
        "<code>if not documents:\n        return<br>if not common_terms:\n        return</code>",
        "TFIDF",
        "edge_cases"
      ],
      "guid": "nlp_a73991b6c76af",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tfidf",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>What is the key insight?",
        "Build a complete text classification system: 1. `train_classifier(texts: List[str], labels: List[str]) -> ClassificationModel` 2. `predict(model: Clas...<br><br>Think: What algorithm/approach?",
        "Text Classification",
        "problem_understanding"
      ],
      "guid": "nlp_5f01131f31bb0",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "text_classification",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>Implement: predict_logistic()",
        "<pre><code>def predict_logistic(X: List[List[float]], model: Dict) -> List[int]:\n    \"\"\"Make predictions with trained logistic regression model.\"\"\"\n    X_np = np.array(X)\n    \n    # Add bias term (same as training)\n    X_with_bias = np.column_stack([np.ones(len(X)), X_np])\n    \n    # Calculate probabilities\n    logits = X_with_bias @ model['weights']\n    probabilities = 1 / (1 + np.exp(-logits))\n    \n    # Convert to binary predictions (threshold = 0.5)\n    predictions = (probabilities > 0.5).astype(int)\n    \n    return predictions.tolist()</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_f06edac958b30",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "text_classification",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>Write the TF-IDF Formula",
        "idf = math.log(len(texts)",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_8afda82946551",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>Write the Evaluation Metric",
        "precision = tp /",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_ad9511bd3affa",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>Write the Evaluation Metric",
        "recall = tp /",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_ad9511bd3affa",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>What is the key insight?",
        "**Time: 15 minutes** Implement a function that tokenizes text into words while handling edge cases. ```python...<br><br>Think: What algorithm/approach?",
        "Tokenization",
        "problem_understanding"
      ],
      "guid": "nlp_ca1ae90650580",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>Implement: tokenize_simple()",
        "<pre><code>def tokenize_simple(text: str) -> List[str]:\n    \"\"\"\n    Alternative approach: Replace-then-split method.\n    \n    Sometimes interviewers want to see multiple approaches.\n    This is simpler but less robust than regex.\n    \"\"\"\n    if not text:\n        return []\n    \n    # STEP 1: Replace punctuation with spaces (except apostrophes)\n    # This preserves contractions while isolating other punctuation\n    cleaned = re.sub(r\"[^\\w\\s']\", \" \", text)\n    \n    # STEP 2: Split on whitespace\n    # Simple but effective for basic cases\n    return cleaned.split()</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_76886bdc4707e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>What's the complexity?",
        "Time Complexity: Time complexity: O(n)<br>Space Complexity: Space complexity: O(n)",
        "Tokenization",
        "complexity"
      ],
      "guid": "nlp_06ddfdaebc5ab",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>What edge cases to handle?",
        "<code>if not text:\n        return<br>if not text:\n        return<br>if not text:\n        return</code>",
        "Tokenization",
        "edge_cases"
      ],
      "guid": "nlp_78a011e4bfe73",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>What is the key insight?",
        "**Time: 30 minutes** Implement a simplified BPE tokenizer for subword segmentation. ```python...<br><br>Think: What algorithm/approach?",
        "Tokenization Advanced",
        "problem_understanding"
      ],
      "guid": "nlp_83694b9b53df3",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization_advanced",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>Implement: get_word_frequencies()",
        "<pre><code>def get_word_frequencies(texts: List[str]) -> Dict[str, int]:\n    \"\"\"Get word frequencies with end-of-word marker.\"\"\"\n    word_freqs = Counter()\n    \n    for text in texts:\n        words = text.lower().split()\n        for word in words:\n            # Add end-of-word marker\n            word_with_marker = ' '.join(word) + ' </w>'\n            word_freqs[word_with_marker] += 1\n    \n    return dict(word_freqs)</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_3a22cbbd30b15",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization_advanced",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>Implement: get_pairs()",
        "<pre><code>def get_pairs(word_freqs: Dict[str, int]) -> Counter:\n    \"\"\"Get all adjacent character pairs with their frequencies.\"\"\"\n    pairs = Counter()\n    \n    for word, freq in word_freqs.items():\n        chars = word.split()\n        for i in range(len(chars) - 1):\n            pair = (chars[i], chars[i + 1])\n            pairs[pair] += freq\n    \n    return pairs</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_b5cf002d66079",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization_advanced",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>Implement: merge_vocab()",
        "<pre><code>def merge_vocab(pair: Tuple[str, str], word_freqs: Dict[str, int]) -> Dict[str, int]:\n    \"\"\"Merge most frequent pair in vocabulary.\"\"\"\n    new_word_freqs = {}\n    bigram = re.escape(' '.join(pair))\n    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n    \n    for word, freq in word_freqs.items():\n        # Replace the pair with merged version\n        new_word = pattern.sub(''.join(pair), word)\n        new_word_freqs[new_word] = freq\n    \n    return new_word_freqs</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_3e83d7c2a33b4",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization_advanced",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>What edge cases to handle?",
        "<code>if not texts:\n        return<br>if not text:\n        return<br>if not found:\n                # Fallback to unknown token (use ID 0)\n                tokens.append(0)\n                i += 1\n        \n        encoded.extend(tokens)\n    \n    return</code>",
        "Tokenization Advanced",
        "edge_cases"
      ],
      "guid": "nlp_bd968d99d76fb",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "tokenization_advanced",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Topic Modeling with LSA and LDA</b><br>What is the key insight?",
        "Implement topic modeling algorithms: 1. `perform_lsa(documents: List[str], num_topics: int = 5) -> LSAModel` 2. `perform_lda(documents: List[str], num...<br><br>Think: What algorithm/approach?",
        "TopicModeling",
        "problem_understanding"
      ],
      "guid": "nlp_7adc3ba1ceab8",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "topicmodeling",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Topic Modeling with LSA and LDA</b><br>Implement: perform_lsa()",
        "<pre><code>def perform_lsa(documents: List[str], num_topics: int = 5) -> LSAModel:\n    \"\"\"Perform Latent Semantic Analysis.\"\"\"\n    model = LSAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "TopicModeling",
        "implementation"
      ],
      "guid": "nlp_4b6e10eb1a041",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "topicmodeling",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Topic Modeling with LSA and LDA</b><br>Implement: perform_lda()",
        "<pre><code>def perform_lda(documents: List[str], num_topics: int = 5) -> LDAModel:\n    \"\"\"Perform Latent Dirichlet Allocation.\"\"\"\n    model = LDAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "TopicModeling",
        "implementation"
      ],
      "guid": "nlp_a8f0cbc7f904b",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "topicmodeling",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Topic Modeling with LSA and LDA</b><br>Implement: extract_topics()",
        "<pre><code>def extract_topics(model: Union[LSAModel, LDAModel], \n                  num_words: int = 10) -> List[List[Tuple[str, float]]]:\n    \"\"\"Extract topics from model.\"\"\"\n    return model.get_topics(num_words)</code></pre>",
        "TopicModeling",
        "implementation"
      ],
      "guid": "nlp_85c4154b77960",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "topicmodeling",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Topic Modeling with LSA and LDA</b><br>Implement: get_document_topics()",
        "<pre><code>def get_document_topics(model: Union[LSAModel, LDAModel], \n                       document: str) -> List[Tuple[int, float]]:\n    \"\"\"Get topic distribution for a document.\"\"\"\n    doc_topics = model.transform([document])[0]\n    \n    # Return as (topic_id, probability) pairs\n    topic_probs = []\n    for topic_idx, prob in enumerate(doc_topics):\n        if prob > 0.01:  # Threshold for relevance\n            topic_probs.append((topic_idx, prob))\n    \n    # Sort by probability\n    topic_probs.sort(key=lambda x: x[1], reverse=True)\n    return topic_probs</code></pre>",
        "TopicModeling",
        "implementation"
      ],
      "guid": "nlp_84472c8b262ef",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "topicmodeling",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: BERT Fine-tuning for Sentiment Analysis</b><br>What is the key insight?",
        "Implement BERT fine-tuning for sentiment classification: 1. `load_pretrained_bert(model_name: str = 'bert-base-uncased') -> Model` 2. `fine_tune_bert(...<br><br>Think: What algorithm/approach?",
        "Transformers",
        "problem_understanding"
      ],
      "guid": "nlp_4c87289ca3c5e",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "transformers",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: BERT Fine-tuning for Sentiment Analysis</b><br>Implement: load_pretrained_bert()",
        "<pre><code>def load_pretrained_bert(model_name: str = 'bert-base-uncased', \n                        task: str = 'custom') -> Tuple[Union[nn.Module, any], any]:\n    \"\"\"Load pre-trained BERT model and tokenizer.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return None, None\n    \n    if task == 'custom':\n        # Custom model with our classification head\n        model = BERTSentimentClassifier(model_name)\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n    else:\n        # Hugging Face model for sequence classification\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_name, num_labels=2\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    return model, tokenizer</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_7fb9ecbed938c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "transformers",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: BERT Fine-tuning for Sentiment Analysis</b><br>Implement: few_shot_sentiment()",
        "<pre><code>def few_shot_sentiment(model, tokenizer, examples: List[Tuple[str, str]], \n                      query: str) -> Tuple[str, float]:\n    \"\"\"Few-shot learning with prompts (for GPT-style models).\"\"\"\n    # Create prompt\n    prompt = \"Classify the sentiment of the following texts:\\n\\n\"\n    \n    for text, label in examples:\n        prompt += f\"Text: {text}\\nSentiment: {label}\\n\\n\"\n    \n    prompt += f\"Text: {query}\\nSentiment:\"\n    \n    # This is a simplified version - in practice, you'd use a generative model\n    # For BERT, we'll just use the fine-tuned classifier\n    predictions = predict_with_bert(model, [query], tokenizer, return_attention=False)\n    \n    if predictions:\n        sentiment, confidence, _ = predictions[0]\n        return sentiment, confidence\n    \n    return \"unknown\", 0.0</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_485be8d712f17",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "transformers",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: BERT Fine-tuning for Sentiment Analysis</b><br>Write the Loss Function",
        "loss = criterion(log",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_90150025a5eca",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: BERT Fine-tuning for Sentiment Analysis</b><br>What edge cases to handle?",
        "<code>if not TRANSFORMERS_AVAILABLE:\n        return<br>if not TRANSFORMERS_AVAILABLE:\n        return<br>if not TRANSFORMERS_AVAILABLE:\n        return</code>",
        "Transformers",
        "edge_cases"
      ],
      "guid": "nlp_5302a31ea8fc7",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "transformers",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Comprehensive Text Normalization Pipeline</b><br>What is the key insight?",
        "Build a complete text normalization system: 1. `normalize_text(text: str, options: Dict[str, bool]) -> str` 2. `clean_html(html: str) -> str`...<br><br>Think: What algorithm/approach?",
        "Utilities",
        "problem_understanding"
      ],
      "guid": "nlp_7afa92c143936",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "utilities",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Comprehensive Text Normalization Pipeline</b><br>Implement: expand_abbreviations()",
        "<pre><code>def expand_abbreviations(text: str) -> str:\n    \"\"\"Expand common abbreviations.\"\"\"\n    for abbr, expansion in ABBREVIATIONS.items():\n        # Handle period after abbreviation\n        pattern1 = r'\\b' + re.escape(abbr) + r'\\.\\b'\n        pattern2 = r'\\b' + re.escape(abbr) + r'\\b'\n        \n        text = re.sub(pattern1, expansion, text, flags=re.IGNORECASE)\n        text = re.sub(pattern2, expansion, text, flags=re.IGNORECASE)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_14e08eed97ac1",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "utilities",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Comprehensive Text Normalization Pipeline</b><br>Implement: clean_html()",
        "<pre><code>def clean_html(html_text: str) -> str:\n    \"\"\"Remove HTML tags and decode entities.\"\"\"\n    # Remove script and style content\n    html_text = re.sub(r'<script[^>]*>.*?</script>', '', html_text, flags=re.DOTALL | re.IGNORECASE)\n    html_text = re.sub(r'<style[^>]*>.*?</style>', '', html_text, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Remove HTML tags\n    html_text = re.sub(r'<[^>]+>', ' ', html_text)\n    \n    # Decode HTML entities\n    text = html.unescape(html_text)\n    \n    # Clean up whitespace\n    text = ' '.join(text.split())\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_e46e13ecfa02c",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "utilities",
        "function"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Comprehensive Text Normalization Pipeline</b><br>Implement: normalize_whitespace()",
        "<pre><code>def normalize_whitespace(text: str) -> str:\n    \"\"\"Normalize whitespace in text.\"\"\"\n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove space before punctuation\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n    \n    # Add space after punctuation if missing\n    text = re.sub(r'([.,!?;:])([A-Za-z])', r'\\1 \\2', text)\n    \n    # Remove leading/trailing whitespace\n    text = text.strip()\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_4c3fc0b915589",
      "note_model_uuid": "nlp-model-basic",
      "tags": [
        "utilities",
        "function"
      ]
    }
  ]
}