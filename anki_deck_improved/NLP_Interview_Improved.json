{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-interview-deck-improved-2024",
  "deck_config_uuid": "nlp-deck-config-improved",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-deck-config-improved",
      "dyn": false,
      "name": "NLP Interview Prep (Improved)",
      "new": {
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 30
      },
      "rev": {
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "perDay": 100
      }
    }
  ],
  "desc": "Interview-ready NLP cards (atomic, precise, mobile friendly).",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "NLP Interview Prep (Improved)",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-model-improved",
      "sortf": 0,
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "latexPost": "\\end{document}",
      "tags": [],
      "vers": [],
      "css": "\n.card {\n    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;\n    font-size: 16px;\n    text-align: left;\n    color: #2d3748;\n    background-color: #ffffff;\n    padding: 16px;\n}\npre {\n    background-color: #f7fafc;\n    color: #2d3748;\n    padding: 12px;\n    border-radius: 8px;\n    overflow-x: auto;\n    font-size: 14px;\n    line-height: 1.5;\n    font-family: 'SF Mono', Monaco, Consolas, monospace;\n    border-left: 4px solid #667eea;\n}\ncode {\n    background-color: #edf2f7;\n    padding: 2px 6px;\n    border-radius: 4px;\n    color: #d6336c;\n    font-size: 14px;\n}\n",
      "flds": [
        {
          "name": "Front",
          "ord": 0,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        },
        {
          "name": "Back",
          "ord": 1,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        },
        {
          "name": "Topic",
          "ord": 2,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        },
        {
          "name": "Type",
          "ord": 3,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        }
      ],
      "name": "NLP Interview Card (Improved)",
      "tmpls": [
        {
          "afmt": "{{FrontSide}}<hr id=answer>{{Back}}<br><br><small style='color:#718096'>{{Topic}} • {{Type}}</small>",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "{{Front}}"
        }
      ],
      "type": 0
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self-Attention from Scratch</b><br>What is the key idea?",
        "**Time: 25 minutes** Implement scaled dot-product self-attention mechanism. **Requirements:**",
        "Attention Mechanisms",
        "understanding"
      ],
      "guid": "nlp_fbdae260c6fd7",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Self-Attention from Scratch<br>Implement: <code>softmax()</code>",
        "<pre><code>def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"\n    Numerically stable softmax implementation.\n    \n    Why stable? Subtracting max prevents overflow when exponentiating large numbers.\n    This is critical for attention weights which can have large values.\n    \"\"\"\n    # Subtract maximum value for numerical stability\n    # This doesn't change the relative probabilities but prevents exp() overflow\n    x_max = np.max(x, axis=axis, keepdims=True)\n    exp_x = np.exp(x - x_max)\n    \n    # Normalize to get probabilities (sum to 1 along specified axis)\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_3b44f5af24d80",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Self-Attention from Scratch<br>Implement: <code>self_attention()</code>",
        "<pre><code>def self_attention(X: np.ndarray, d_k: int) -> np.ndarray:\n    \"\"\"\n    Implement scaled dot-product self-attention mechanism.\n    \n    This is the CORE of all transformer models (BERT, GPT, etc.)\n    \n    Formula: Attention(Q,K,V) = softmax(QK^T / √d_k)V\n    \n    Args:\n        X: Input embeddings (seq_len, d_model)\n        d_k: Dimension of queries and keys (for scaling)\n    \n    Returns:\n        Attention output (seq_len, d_model)\n    \"\"\"\n    seq_len, d_model = X.shape\n    \n    # STEP 1: Initialize weight matrices</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_ba35b18939b1b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Self-Attention from Scratch<br>Write the formula",
        "<pre>Attention(Q,K,V) = softmax(QK^T / √d_k) V</pre>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_1443615178e94",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Self-Attention from Scratch<br>List the algorithm steps",
        "<pre>1. Create Q, K, V matrices from X\n2. Compute attention scores: QK^T / sqrt(d_k)\n3. Apply softmax to get attention weights\n4. Return weighted sum of values: Attention(Q,K,V) = softmax(QK^T/√d_k)V\n5. Implement Q, K, V transformations using random weights\n6. Calculate attention scores with scaling</pre>",
        "Attention Mechanisms",
        "steps"
      ],
      "guid": "nlp_e07cf4bc8f773",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Self-Attention from Scratch<br>What's the complexity?",
        "<pre>Time complexity: O(n²d)\nSpace complexity: O(n²)</pre>",
        "Attention Mechanisms",
        "complexity"
      ],
      "guid": "nlp_dfdeb6fc1f389",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>What is the key idea?",
        "**Time: 20 minutes** Implement a basic bag-of-words vectorizer. **Requirements:**",
        "BoW Vectors",
        "understanding"
      ],
      "guid": "nlp_426a95449d923",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Bag of Words from Scratch<br>Implement: <code>create_bow_vector()</code>",
        "<pre><code>def create_bow_vector(documents: List[str]) -> Tuple[List[str], List[List[int]]]:\n    \"\"\"\n    Create bag-of-words representation from scratch.\n    \n    BAG-OF-WORDS CONCEPT:\n    - Represent text as vector of word counts\n    - \"Order doesn't matter, just word presence/frequency\"\n    - Foundation of many NLP systems before embeddings\n    \n    STEPS:\n    1. Build vocabulary (all unique words)\n    2. For each document, count occurrences of each vocab word\n    3. Return vocabulary and count vectors\n    \n    INTERVIEW INSIGHT: Simple but effective. Foundation for TF-IDF.\n    \"\"\"\n    \n    # STEP 1: Handle edge case</code></pre>",
        "BoW Vectors",
        "implementation"
      ],
      "guid": "nlp_be482ba02f5b0",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Bag of Words from Scratch<br>Implement: <code>cosine_similarity()</code>",
        "<pre><code>def cosine_similarity(vec1: List[int], vec2: List[int]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two BoW vectors.\n    \n    COSINE SIMILARITY INTUITION:\n    - Measures angle between vectors, not magnitude\n    - Good for text: \"I love cats\" vs \"I really really love cats\"\n    - Both have same direction (similar meaning) despite different lengths\n    \n    FORMULA: cos(θ) = (A·B) / (||A|| × ||B||)\n    \n    INTERVIEW TIP: Always explain why cosine > Euclidean for text\n    \"\"\"\n    \n    # STEP 1: Input validation\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0\n    </code></pre>",
        "BoW Vectors",
        "implementation"
      ],
      "guid": "nlp_a0c3b2d18232a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Bag of Words from Scratch<br>List the algorithm steps",
        "<pre>1. Build vocabulary from all documents\n2. Count word occurrences in each document\n3. Handle empty documents\n4. Implement cosine similarity for vector comparison\n5. Represent text as vector of word counts\n6. \"Order doesn't matter, just word presence/frequency\"</pre>",
        "BoW Vectors",
        "steps"
      ],
      "guid": "nlp_9c8b2170c7033",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Bag of Words from Scratch<br>What's the complexity?",
        "<pre>Time: O(d × n × v)\nSpace: O(d × v)</pre>",
        "BoW Vectors",
        "complexity"
      ],
      "guid": "nlp_841e0146545e9",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Bag of Words from Scratch<br>Common pitfalls?",
        "<pre># STEP 1: Handle edge case\n# STEP 4: Handle zero vectors (edge case)</pre>",
        "BoW Vectors",
        "pitfalls"
      ],
      "guid": "nlp_c9d9882fab7b8",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "pitfalls"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>What is the key idea?",
        "**Time: 25 minutes** Implement a simple CNN for text classification using basic operations. **Requirements:**",
        "CNN Text",
        "understanding"
      ],
      "guid": "nlp_3fc732c8489ea",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "cnn_text",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text CNN for Classification<br>Implement: <code>text_to_sequence()</code>",
        "<pre><code>def text_to_sequence(text: str, vocab: Dict[str, int], max_len: int = 10) -> List[int]:\n    \"\"\"Convert text to padded integer sequence.\"\"\"\n    words = text.lower().split()\n    sequence = [vocab.get(word, 0) for word in words]  # 0 for unknown words\n    \n    # Pad or truncate to max_len\n    if len(sequence) < max_len:\n        sequence += [0] * (max_len - len(sequence))\n    else:\n        sequence = sequence[:max_len]\n    \n    return sequence</code></pre>",
        "CNN Text",
        "implementation"
      ],
      "guid": "nlp_45d47f797eda7",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text CNN for Classification<br>Implement: <code>embedding_lookup()</code>",
        "<pre><code>def embedding_lookup(sequence: List[int], embedding_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Look up embeddings for sequence.\"\"\"\n    return np.array([embedding_matrix[idx] for idx in sequence])</code></pre>",
        "CNN Text",
        "implementation"
      ],
      "guid": "nlp_7733352a0bad0",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text CNN for Classification<br>List the algorithm steps",
        "<pre>1. Convert text to padded integer sequence\n2. Implement 1D convolution manually (kernel size 3)\n3. Apply max pooling across sequence\n4. Dense layer + sigmoid activation</pre>",
        "CNN Text",
        "steps"
      ],
      "guid": "nlp_8a3b67fc49fb5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "cnn_text",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip-gram</b><br>What is the key idea?",
        "**Time: 20 minutes** Implement the core Skip-gram training step for Word2Vec. **Requirements:**",
        "Embeddings",
        "understanding"
      ],
      "guid": "nlp_0200521a688be",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Word2Vec Skip-gram<br>Implement: <code>sigmoid()</code>",
        "<pre><code>def sigmoid(x: float) -> float:\n    \"\"\"\n    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n    \n    Used in Word2Vec to convert dot products to probabilities.\n    Clamp input to prevent numerical overflow/underflow.\n    \"\"\"\n    # Clamp x to prevent overflow in exp() function\n    # This is crucial for numerical stability\n    clamped_x = max(-500, min(500, x))\n    return 1 / (1 + math.exp(-clamped_x))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_a74f7d4bd0df1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Word2Vec Skip-gram<br>Implement: <code>dot_product()</code>",
        "<pre><code>def dot_product(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"\n    Calculate dot product between two vectors.\n    \n    Dot product measures similarity between vectors:\n    - High positive value = vectors point in same direction (similar)\n    - Zero = vectors are orthogonal (unrelated)\n    - Negative = vectors point in opposite directions (dissimilar)\n    \"\"\"\n    # Element-wise multiplication, then sum\n    return sum(a * b for a, b in zip(vec1, vec2))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_a2eab46f0c9d5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Word2Vec Skip-gram<br>Write the formula",
        "<pre>cosine_sim(A,B) = (A·B) / (||A|| × ||B||)</pre>",
        "Embeddings",
        "formula"
      ],
      "guid": "nlp_ac4fa51755d08",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Word2Vec Skip-gram<br>List the algorithm steps",
        "<pre>1. Implement sigmoid function\n2. Calculate gradients using dot product\n3. Update embeddings using gradient descent\n4. Handle missing words gracefully\n5. High positive value = vectors point in same direction (similar)\n6. Zero = vectors are orthogonal (unrelated)</pre>",
        "Embeddings",
        "steps"
      ],
      "guid": "nlp_8a36fb59c7409",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki-Optimized Solution Structure</b><br>What is the key idea?",
        "**Time: 20 minutes** Implement a simple spell checker that: 1. Calculates edit distance between words",
        "Example Anki Refactor",
        "understanding"
      ],
      "guid": "nlp_5bdec768be57d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Example: Anki-Optimized Solution Structure<br>Implement: <code>spell_check_approach()</code>",
        "<pre><code>def spell_check_approach():\n    \"\"\"\n    KEY: Use edit distance + frequency ranking\n    STEPS: 1) Find close words 2) Rank by frequency\n    INSIGHT: Most typos are 1-2 edits away\n    \"\"\"\n    pass</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_5d563d9485f3c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Example: Anki-Optimized Solution Structure<br>Implement: <code>edit_distance_recursive()</code>",
        "<pre><code>def edit_distance_recursive(s1: str, s2: str) -> int:\n    \"\"\"\n    FORMULA: ED(i,j) = min(\n        ED(i-1,j) + 1,    # deletion\n        ED(i,j-1) + 1,    # insertion  \n        ED(i-1,j-1) + 0/1 # substitution\n    )\n    \"\"\"\n    if not s1: return len(s2)\n    if not s2: return len(s1)\n    \n    if s1[0] == s2[0]:\n        return edit_distance_recursive(s1[1:], s2[1:])\n    \n    return 1 + min(\n        edit_distance_recursive(s1[1:], s2),    # delete\n        edit_distance_recursive(s1, s2[1:]),    # insert\n        edit_distance_recursive(s1[1:], s2[1:]) # replace</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_b3a467e306251",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Example: Anki-Optimized Solution Structure<br>List the algorithm steps",
        "<pre>1. Calculates edit distance between words\n2. Finds candidate corrections from a dictionary\n3. Ranks candidates by frequency\n4. Edit distance vs phonetic matching\n5. Memory vs speed (DP vs recursive)\n6. Pruning strategies for large dictionaries</pre>",
        "Example Anki Refactor",
        "steps"
      ],
      "guid": "nlp_8f1eb6163cd24",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Example: Anki-Optimized Solution Structure<br>What's the complexity?",
        "<pre>time, O(m*n)</pre>",
        "Example Anki Refactor",
        "complexity"
      ],
      "guid": "nlp_57408da2f811d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Example: Anki-Optimized Solution Structure<br>Common pitfalls?",
        "<pre>EDGE: Empty dictionary returns empty list\n# EDGE: Already correct word\n# EDGE: Empty dictionary\n# EDGE: No candidates found</pre>",
        "Example Anki Refactor",
        "pitfalls"
      ],
      "guid": "nlp_b6cdd04fd3d99",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "pitfalls"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine-tuning for Classification</b><br>What is the key idea?",
        "**Time: 25 minutes** Implement the key components for fine-tuning a pre-trained LLM for text classification. **Requirements:**",
        "Fine Tuning",
        "understanding"
      ],
      "guid": "nlp_cd0ce9d345001",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "fine_tuning",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: LLM Fine-tuning for Classification<br>Implement: <code>add_classification_head()</code>",
        "<pre><code>def add_classification_head(pretrained_model_dim: int, num_classes: int) -> Dict:\n    \"\"\"Add classification head to pretrained LLM.\"\"\"\n    \n    # Xavier/Glorot initialization for stable training\n    std = np.sqrt(2.0 / (pretrained_model_dim + num_classes))\n    \n    return {\n        'W_cls': np.random.randn(pretrained_model_dim, num_classes) * std,\n        'b_cls': np.zeros(num_classes)\n    }</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_09b43b0d321d4",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: LLM Fine-tuning for Classification<br>Implement: <code>compute_classification_loss()</code>",
        "<pre><code>def compute_classification_loss(logits: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Compute cross-entropy loss with numerical stability.\"\"\"\n    batch_size, num_classes = logits.shape\n    \n    # Numerical stability: subtract max from logits\n    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n    \n    # Softmax probabilities\n    exp_logits = np.exp(logits_stable)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Cross-entropy loss\n    loss = 0.0\n    for i in range(batch_size):\n        true_class = labels[i]\n        loss += -np.log(probs[i, true_class] + 1e-10)  # Add small epsilon\n    \n    return loss / batch_size</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_e158cdc54ff60",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: LLM Fine-tuning for Classification<br>List the algorithm steps",
        "<pre>1. Initialize classification head with proper scaling\n2. Implement stable cross-entropy loss with softmax\n3. Demonstrate layer freezing strategy\n4. Handle different learning rates for pretrained vs new layers\n5. A is (d, rank) and B is (rank, d)\n6. Only A and B are trainable (much fewer parameters)</pre>",
        "Fine Tuning",
        "steps"
      ],
      "guid": "nlp_2d2fe9e8767b1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "fine_tuning",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>What is the key idea?",
        "**Time: 30 minutes** Implement a single GPT transformer block with the standard architecture. **Requirements:**",
        "GPT Implementation",
        "understanding"
      ],
      "guid": "nlp_b3ad8336a66bd",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "gpt_implementation",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: GPT Transformer Block<br>Implement: <code>gelu()</code>",
        "<pre><code>def gelu(x: np.ndarray) -> np.ndarray:\n    \"\"\"GELU activation function used in GPT.\"\"\"\n    return 0.5 * x * (1 + np.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))</code></pre>",
        "GPT Implementation",
        "implementation"
      ],
      "guid": "nlp_cfc278cc57888",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: GPT Transformer Block<br>Implement: <code>layer_norm()</code>",
        "<pre><code>def layer_norm(x: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n    \"\"\"Apply layer normalization.\"\"\"\n    # Calculate mean and variance along last dimension\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    \n    # Normalize\n    normalized = (x - mean) / np.sqrt(variance + eps)\n    \n    # Scale and shift\n    return gamma * normalized + beta</code></pre>",
        "GPT Implementation",
        "implementation"
      ],
      "guid": "nlp_33466610f0b62",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: GPT Transformer Block<br>List the algorithm steps",
        "<pre>1. Implement layer normalization with learnable parameters\n2. Use GELU activation function in FFN\n3. Add residual connections around attention and FFN\n4. Apply causal masking in self-attention</pre>",
        "GPT Implementation",
        "steps"
      ],
      "guid": "nlp_35bf59f0ad948",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "gpt_implementation",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Instruction Following Setup</b><br>What is the key idea?",
        "**Time: 20 minutes** Implement the data preparation and loss calculation for instruction fine-tuning. **Requirements:**",
        "Instruction Tuning",
        "understanding"
      ],
      "guid": "nlp_71a53c67bda7c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "instruction_tuning",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Instruction Following Setup<br>Implement: <code>format_instruction_data()</code>",
        "<pre><code>def format_instruction_data(instruction: str, response: str) -> str:\n    \"\"\"Format instruction-response pair for training.\"\"\"\n    return f\"### Instruction:\\n{instruction}\\n### Response:\\n{response}\"</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_0ef6a4035c847",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Instruction Following Setup<br>Implement: <code>tokenize_instruction_response()</code>",
        "<pre><code>def tokenize_instruction_response(formatted_text: str, \n                                tokenizer_vocab: Dict[str, int]) -> Tuple[List[int], int]:\n    \"\"\"\n    Tokenize formatted instruction-response and return instruction length.\n    \n    Returns:\n        (token_ids, instruction_length)\n    \"\"\"\n    # Simple tokenization for demo\n    tokens = formatted_text.lower().split()\n    token_ids = [tokenizer_vocab.get(token, 0) for token in tokens]  # 0 = UNK\n    \n    # Find where instruction ends (look for \"### Response:\")\n    instruction_length = 0\n    response_marker = \"### response:\"\n    \n    # Reconstruct text to find marker\n    text_lower = formatted_text.lower()</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_3d576c31d4033",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Instruction Following Setup<br>List the algorithm steps",
        "<pre>1. Format data with clear instruction/response delimiters\n2. Mask instruction tokens during loss calculation\n3. Implement next-token prediction loss\n4. Handle variable-length instructions and responses</pre>",
        "Instruction Tuning",
        "steps"
      ],
      "guid": "nlp_c98e09d2e2a85",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "instruction_tuning",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Generation with LLMs</b><br>What is the key idea?",
        "**Time: 25 minutes** Implement text generation from a trained language model with different decoding strategies. **Requirements:**",
        "LLM Fundamentals",
        "understanding"
      ],
      "guid": "nlp_725113ac8c01f",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "llm_fundamentals",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Generation with LLMs<br>Implement: <code>softmax()</code>",
        "<pre><code>def softmax(logits: List[float], temperature: float = 1.0) -> List[float]:\n    \"\"\"Convert logits to probabilities with temperature scaling.\"\"\"\n    if temperature != 1.0:\n        logits = [l / temperature for l in logits]\n    \n    max_logit = max(logits)\n    exp_logits = [math.exp(l - max_logit) for l in logits]\n    sum_exp = sum(exp_logits)\n    \n    return [exp_l / sum_exp for exp_l in exp_logits]</code></pre>",
        "LLM Fundamentals",
        "implementation"
      ],
      "guid": "nlp_3c5b1d7363b81",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Generation with LLMs<br>Implement: <code>sample_token()</code>",
        "<pre><code>def sample_token(probs: List[float], strategy: str = 'greedy', \n                top_k: int = 5, top_p: float = 0.9) -> int:\n    \"\"\"Sample next token using different strategies.\"\"\"\n    \n    if strategy == 'greedy':\n        return probs.index(max(probs))\n    \n    elif strategy == 'random':\n        # Random sampling from full distribution\n        return np.random.choice(len(probs), p=probs)\n    \n    elif strategy == 'top_k':\n        # Sample from top k tokens only\n        top_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:top_k]\n        top_probs = [probs[i] for i in top_indices]\n        \n        # Renormalize\n        sum_top = sum(top_probs)</code></pre>",
        "LLM Fundamentals",
        "implementation"
      ],
      "guid": "nlp_5c35cde0c6783",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Generation with LLMs<br>List the algorithm steps",
        "<pre>1. Convert prompt to tokens, generate tokens iteratively\n2. Implement greedy, random, top-k, and top-p sampling\n3. Track probabilities for beam search scoring\n4. Handle end-of-sequence tokens properly</pre>",
        "LLM Fundamentals",
        "steps"
      ],
      "guid": "nlp_7d46709019f38",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "llm_fundamentals",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Evaluation Metrics</b><br>What is the key idea?",
        "**Time: 20 minutes** Implement key evaluation metrics for large language models. **Requirements:**",
        "Model Evaluation",
        "understanding"
      ],
      "guid": "nlp_81a1d3c690fa0",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "model_evaluation",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: LLM Evaluation Metrics<br>Implement: <code>calculate_perplexity()</code>",
        "<pre><code>def calculate_perplexity(model_probs: List[List[float]], \n                        target_tokens: List[int]) -> float:\n    \"\"\"Calculate perplexity from model probabilities.\"\"\"\n    if not model_probs or not target_tokens:\n        return float('inf')\n    \n    if len(model_probs) != len(target_tokens):\n        return float('inf')\n    \n    log_likelihood = 0.0\n    num_tokens = 0\n    \n    for probs, target_token in zip(model_probs, target_tokens):\n        if target_token < len(probs):\n            # Get probability of target token\n            prob = probs[target_token]\n            \n            # Add log probability (with small epsilon to avoid log(0))</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_524fcc6dfa79e",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: LLM Evaluation Metrics<br>Implement: <code>get_ngrams()</code>",
        "<pre><code>def get_ngrams(text: str, n: int) -> List[str]:\n    \"\"\"Extract n-grams from text.\"\"\"\n    words = text.lower().split()\n    ngrams = []\n    \n    for i in range(len(words) - n + 1):\n        ngram = ' '.join(words[i:i + n])\n        ngrams.append(ngram)\n    \n    return ngrams</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_10fcb23ba272a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: LLM Evaluation Metrics<br>List the algorithm steps",
        "<pre>1. Handle log probability calculations safely (avoid log(0))\n2. Implement n-gram precision for BLEU\n3. Add brevity penalty for BLEU\n4. Calculate confidence intervals for perplexity</pre>",
        "Model Evaluation",
        "steps"
      ],
      "guid": "nlp_bbbf48c9b4f0c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "model_evaluation",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Named Entity Recognition with Custom Entities</b><br>What is the key idea?",
        "Implement `extract_entities(text: str) -> Dict[str, List[str]]` that: 1. Extracts standard entities (PERSON, ORG, GPE, DATE, MONEY) 2. Returns entities grouped …",
        "NER",
        "understanding"
      ],
      "guid": "nlp_d080be7b0c750",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ner",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Named Entity Recognition with Custom Entities<br>Implement: <code>extract_entities()</code>",
        "<pre><code>def extract_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract named entities using spaCy.\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        entities[ent.label_].append(ent.text)\n    \n    # Remove duplicates while preserving order\n    for label in entities:\n        entities[label] = list(dict.fromkeys(entities[label]))\n    \n    return dict(entities)</code></pre>",
        "NER",
        "implementation"
      ],
      "guid": "nlp_5572e5bcff597",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Named Entity Recognition with Custom Entities<br>Implement: <code>extract_custom_entities()</code>",
        "<pre><code>def extract_custom_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract custom entities like emails, phones, URLs using regex.\"\"\"\n    entities = defaultdict(list)\n    \n    # Email pattern\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    entities['EMAIL'] = re.findall(email_pattern, text)\n    \n    # Phone pattern (US-style)\n    phone_pattern = r'\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b'\n    phones = re.findall(phone_pattern, text)\n    entities['PHONE'] = ['-'.join(groups) for groups in phones]\n    \n    # URL pattern\n    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&/=]*)'\n    entities['URL'] = re.findall(url_pattern, text)\n    \n    # Money pattern (simple)</code></pre>",
        "NER",
        "implementation"
      ],
      "guid": "nlp_fabb896c90494",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Named Entity Recognition with Custom Entities<br>List the algorithm steps",
        "<pre>1. Extracts standard entities (PERSON, ORG, GPE, DATE, MONEY)\n2. Returns entities grouped by type\n3. Handles overlapping entities\n4. Use spaCy or NLTK for NER\n5. Handle multi-word entities\n6. Implement custom entity detection for email/phone numbers</pre>",
        "NER",
        "steps"
      ],
      "guid": "nlp_69a5ae33cefa1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ner",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: N-gram Language Model</b><br>What is the key idea?",
        "**Time: 25 minutes** Implement a simple bigram language model with probability calculation. **Requirements:**",
        "NGrams",
        "understanding"
      ],
      "guid": "nlp_d8bbc39c60891",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ngrams",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: N-gram Language Model<br>Implement: <code>build_bigram_model()</code>",
        "<pre><code>def build_bigram_model(texts: List[str]) -> Dict[str, Dict[str, float]]:\n    \"\"\"Build bigram language model with probabilities.\"\"\"\n    if not texts:\n        return {}\n    \n    # Count bigrams\n    bigram_counts = defaultdict(Counter)\n    \n    for text in texts:\n        words = text.lower().split()\n        \n        # Add start token\n        words = ['<START>'] + words + ['<END>']\n        \n        # Count bigrams\n        for i in range(len(words) - 1):\n            w1, w2 = words[i], words[i + 1]\n            bigram_counts[w1][w2] += 1</code></pre>",
        "NGrams",
        "implementation"
      ],
      "guid": "nlp_09f9ed30189b5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: N-gram Language Model<br>Implement: <code>generate_text()</code>",
        "<pre><code>def generate_text(model: Dict, start_word: str = '<START>', length: int = 5) -> str:\n    \"\"\"Generate text using bigram model (deterministic - pick most probable).\"\"\"\n    if start_word not in model:\n        return \"\"\n    \n    words = []\n    current_word = start_word\n    \n    for _ in range(length):\n        if current_word not in model or not model[current_word]:\n            break\n        \n        # Pick most probable next word\n        next_word = max(model[current_word], key=model[current_word].get)\n        \n        if next_word == '<END>':\n            break\n        </code></pre>",
        "NGrams",
        "implementation"
      ],
      "guid": "nlp_76677450182ee",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: N-gram Language Model<br>List the algorithm steps",
        "<pre>1. Count bigram frequencies across all texts\n2. Calculate conditional probabilities P(w2|w1)\n3. Handle unseen bigrams (return empty dict)\n4. Generate coherent text sequences</pre>",
        "NGrams",
        "steps"
      ],
      "guid": "nlp_bbec121fc81bb",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ngrams",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Neural Network from Scratch</b><br>What is the key idea?",
        "Implement basic neural networks from scratch: 1. `Perceptron(input_size: int, learning_rate: float)` - Single perceptron 2. `NeuralNetwork(layers: List[int], ac…",
        "Neural Fundamentals",
        "understanding"
      ],
      "guid": "nlp_0b3f6268bb589",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "neural_fundamentals",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Neural Network from Scratch<br>Implement: <code>gradient_check()</code>",
        "<pre><code>def gradient_check(network: NeuralNetwork, X: np.ndarray, y: np.ndarray, epsilon: float = 1e-7) -> float:\n    \"\"\"Perform gradient checking to verify backpropagation implementation.\"\"\"\n    # Get gradients from backpropagation\n    y_pred = network.forward(X)\n    network.backward(X, y, y_pred)\n    \n    # Store analytical gradients\n    analytical_gradients = {}\n    for i in range(1, network.num_layers):\n        # Note: This is simplified - in practice you'd store gradients during backward pass\n        pass\n    \n    # Compute numerical gradients\n    numerical_gradients = {}\n    \n    for i in range(1, network.num_layers):\n        w_key = f'W{i}'\n        b_key = f'b{i}'</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_e464f36a8da7c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Neural Network from Scratch<br>Implement: <code>generate_xor_data()</code>",
        "<pre><code>def generate_xor_data() -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate XOR problem data.\"\"\"\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n    y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n    return X, y</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_528703e706471",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Neural Network from Scratch<br>List the algorithm steps",
        "<pre>1. `Perceptron(input_size: int, learning_rate: float)` - Single perceptron\n2. `NeuralNetwork(layers: List[int], activation: str)` - Multi-layer network\n3. `train(X: np.ndarray, y: np.ndarray, epochs: int)` - Training with backpropagation\n4. `predict(X: np.ndarray) -> np.ndarray` - Forward pass prediction\n5. Implement forward propagation\n6. Implement backpropagation with chain rule</pre>",
        "Neural Fundamentals",
        "steps"
      ],
      "guid": "nlp_13d9bb5436db6",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "neural_fundamentals",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Neural Network from Scratch<br>Common pitfalls?",
        "<pre>scatter = plt.scatter(X[:, 0], X[:, 1], c=y_labels, cmap=plt.cm.RdYlBu, edgecolors='black')</pre>",
        "Neural Fundamentals",
        "pitfalls"
      ],
      "guid": "nlp_8ff526f7c873c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "neural_fundamentals",
        "pitfalls"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Part-of-Speech Tagging with Accuracy Metrics</b><br>What is the key idea?",
        "Implement `pos_tag_text(text: str) -> List[Tuple[str, str]]` that: 1. Tags each word with its part-of-speech 2. Handles ambiguous words correctly",
        "POS Tagging",
        "understanding"
      ],
      "guid": "nlp_e71ea8caa085d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "pos_tagging",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Part-of-Speech Tagging with Accuracy Metrics<br>Implement: <code>pos_tag_text()</code>",
        "<pre><code>def pos_tag_text(text: str) -> List[Tuple[str, str]]:\n    \"\"\"POS tag text using NLTK's default tagger (Penn Treebank tagset).\"\"\"\n    tokens = nltk.word_tokenize(text)\n    return nltk.pos_tag(tokens)</code></pre>",
        "POS Tagging",
        "implementation"
      ],
      "guid": "nlp_14080a357ceeb",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Part-of-Speech Tagging with Accuracy Metrics<br>Implement: <code>extract_pos()</code>",
        "<pre><code>def extract_pos(tagged_text: List[Tuple[str, str]], pos_prefix: str) -> List[str]:\n    \"\"\"Extract words with specific POS tags.\n    \n    Args:\n        tagged_text: List of (word, pos) tuples\n        pos_prefix: POS tag prefix (e.g., 'NN' for nouns, 'VB' for verbs)\n    \"\"\"\n    return [word for word, pos in tagged_text if pos.startswith(pos_prefix)]</code></pre>",
        "POS Tagging",
        "implementation"
      ],
      "guid": "nlp_57bcaae09be12",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Part-of-Speech Tagging with Accuracy Metrics<br>List the algorithm steps",
        "<pre>1. Tags each word with its part-of-speech\n2. Handles ambiguous words correctly\n3. Returns (word, tag) tuples\n4. Use Penn Treebank tagset\n5. Handle sentences with punctuation\n6. Implement a function to get most common POS for ambiguous words</pre>",
        "POS Tagging",
        "steps"
      ],
      "guid": "nlp_01401679ce21b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "pos_tagging",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Regular Expressions for NLP</b><br>What is the key idea?",
        "Implement regex-based NLP functions: 1. `extract_entities_regex(text: str) -> Dict[str, List[str]]` - Extract emails, phones, URLs, dates, money amounts",
        "Regex NLP",
        "understanding"
      ],
      "guid": "nlp_a29c66e4c00e5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "regex_nlp",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Regular Expressions for NLP<br>Implement: <code>extract_entities_regex()</code>",
        "<pre><code>def extract_entities_regex(text: str, \n                          custom_patterns: Optional[Dict[str, str]] = None) -> Dict[str, List[str]]:\n    \"\"\"Extract various entities using regex patterns.\"\"\"\n    entities = defaultdict(list)\n    \n    # Use default patterns plus any custom ones\n    patterns = REGEX_PATTERNS.copy()\n    if custom_patterns:\n        patterns.update(custom_patterns)\n    \n    # Extract emails\n    emails = re.findall(patterns['email'], text, re.IGNORECASE)\n    if emails:\n        entities['EMAIL'] = list(set(emails))\n    \n    # Extract phone numbers\n    phones = re.findall(patterns['phone_us'], text)\n    phone_formatted = [f\"({area})-{prefix}-{number}\" for area, prefix, number in phones]</code></pre>",
        "Regex NLP",
        "implementation"
      ],
      "guid": "nlp_19a0da1a486d3",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Regular Expressions for NLP<br>Implement: <code>sentence_segmentation()</code>",
        "<pre><code>def sentence_segmentation(text: str, preserve_abbreviations: bool = True) -> List[str]:\n    \"\"\"Segment text into sentences using regex, handling abbreviations.\"\"\"\n    \n    # Common abbreviations that don't end sentences\n    abbreviations = {\n        'dr', 'mr', 'mrs', 'ms', 'prof', 'sr', 'jr', 'vs', 'etc', 'eg', 'ie',\n        'inc', 'ltd', 'corp', 'co', 'ave', 'st', 'blvd', 'rd', 'apt', 'dept',\n        'fig', 'vol', 'no', 'pp', 'cf', 'al', 'ca', 'ny', 'tx', 'fl'\n    }\n    \n    if preserve_abbreviations:\n        # Replace abbreviations temporarily\n        temp_text = text\n        abbrev_placeholders = {}\n        counter = 0\n        \n        for abbrev in abbreviations:\n            pattern = fr'\\b{re.escape(abbrev)}\\.(?!\\s*[A-Z])'</code></pre>",
        "Regex NLP",
        "implementation"
      ],
      "guid": "nlp_feef8d1825d01",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Regular Expressions for NLP<br>List the algorithm steps",
        "<pre>1. `extract_entities_regex(text: str) -> Dict[str, List[str]]`\n2. Extract emails, phones, URLs, dates, money amounts\n3. `sentence_segmentation(text: str) -> List[str]`\n4. Handle abbreviations, decimals, ellipsis\n5. `pattern_based_ner(text: str, patterns: Dict[str, str]) -> List[Tuple[str, str, int, int]]`\n6. Custom entity recognition with regex</pre>",
        "Regex NLP",
        "steps"
      ],
      "guid": "nlp_14f5abe6163f3",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "regex_nlp",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Rule-based Sentiment Analysis</b><br>What is the key idea?",
        "**Time: 20 minutes** Implement a simple rule-based sentiment analyzer. **Requirements:**",
        "Sentiment Analysis",
        "understanding"
      ],
      "guid": "nlp_243e1b850993e",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Rule-based Sentiment Analysis<br>Implement: <code>analyze_sentiment()</code>",
        "<pre><code>def analyze_sentiment(text: str) -> Dict[str, float]:\n    \"\"\"\n    Analyze sentiment using rule-based approach (VADER-style).\n    \n    RULE-BASED SENTIMENT ANALYSIS:\n    - Uses pre-built dictionary of word sentiments\n    - Applies grammatical rules (intensifiers, negations)\n    - Fast and interpretable (good for production)\n    - No training data needed\n    \n    ALGORITHM:\n    1. Tokenize text and look up word sentiments\n    2. Apply intensifier rules (\"very good\" > \"good\")\n    3. Apply negation rules (\"not good\" becomes negative)\n    4. Handle punctuation emphasis (\"great!!!\" > \"great\")\n    5. Normalize scores and return distribution\n    \"\"\"\n    </code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_0946a051d1b26",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Rule-based Sentiment Analysis<br>Implement: <code>classify_sentiment()</code>",
        "<pre><code>def classify_sentiment(sentiment_scores: Dict[str, float]) -> str:\n    \"\"\"\n    Convert sentiment scores to simple classification.\n    \n    CLASSIFICATION THRESHOLDS (VADER standard):\n    - compound >= 0.05: positive\n    - compound <= -0.05: negative  \n    - -0.05 < compound < 0.05: neutral\n    \n    These thresholds are empirically determined from testing.\n    \"\"\"\n    compound = sentiment_scores['compound']\n    \n    if compound >= 0.05:\n        return 'positive'\n    elif compound <= -0.05:\n        return 'negative'\n    else:</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_224b6d17ddd44",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Rule-based Sentiment Analysis<br>List the algorithm steps",
        "<pre>1. Use sentiment lexicon for base scores\n2. \"very/really\" intensifies sentiment (+30%)\n3. \"not/never\" flips sentiment (* -0.8)\n4. Multiple punctuation adds emphasis\n5. Create basic positive/negative word dictionary\n6. Handle intensifiers (\"very good\" -> higher positive score)</pre>",
        "Sentiment Analysis",
        "steps"
      ],
      "guid": "nlp_ab943863064fd",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Rule-based Sentiment Analysis<br>Common pitfalls?",
        "<pre># STEP 1: Handle edge cases first</pre>",
        "Sentiment Analysis",
        "pitfalls"
      ],
      "guid": "nlp_4569ddf1f7a85",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "pitfalls"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>What is the key idea?",
        "**Time: 25 minutes** Implement a basic LSTM cell for sentiment classification. **Requirements:**",
        "Sequence Models",
        "understanding"
      ],
      "guid": "nlp_e3c262ebc8864",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sequence_models",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Simple LSTM for Sentiment<br>Implement: <code>sigmoid()</code>",
        "<pre><code>def sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-max(-500, min(500, x))))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_49b7b251a6fa0",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Simple LSTM for Sentiment<br>Implement: <code>tanh()</code>",
        "<pre><code>def tanh(x: float) -> float:\n    return math.tanh(max(-500, min(500, x)))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_9cf9527127c99",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Simple LSTM for Sentiment<br>List the algorithm steps",
        "<pre>1. Implement forget, input, output gates using sigmoid\n2. Implement candidate cell state using tanh\n3. Process sequence step by step\n4. Final classification with sigmoid</pre>",
        "Sequence Models",
        "steps"
      ],
      "guid": "nlp_29b633f5d954b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sequence_models",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Similarity Metrics</b><br>What is the key idea?",
        "Implement multiple similarity metrics: 1. `cosine_similarity(text1: str, text2: str) -> float` 2. `jaccard_similarity(text1: str, text2: str) -> float`",
        "Similarity",
        "understanding"
      ],
      "guid": "nlp_049640ac75885",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "similarity",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Similarity Metrics<br>Implement: <code>tokenize()</code>",
        "<pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"Simple tokenization.\"\"\"\n    return text.lower().split()</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_6f25ab22d9aea",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Similarity Metrics<br>Implement: <code>cosine_similarity()</code>",
        "<pre><code>def cosine_similarity(text1: str, text2: str, method='tfidf') -> float:\n    \"\"\"Calculate cosine similarity between two texts.\"\"\"\n    if method == 'tfidf':\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n        return sklearn_cosine(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n    \n    elif method == 'bow':  # Bag of words\n        # Tokenize\n        tokens1 = tokenize(text1)\n        tokens2 = tokenize(text2)\n        \n        # Create vocabulary\n        vocab = set(tokens1 + tokens2)\n        \n        # Create vectors\n        vec1 = np.array([tokens1.count(word) for word in vocab])\n        vec2 = np.array([tokens2.count(word) for word in vocab])</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_5bc2efd06b54a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Similarity Metrics<br>Write the formula",
        "<pre>TF-IDF(t,d) = TF(t,d) × IDF(t); IDF(t)=log(N/df(t))</pre>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_6b316061b6ad8",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Similarity Metrics<br>List the algorithm steps",
        "<pre>1. `cosine_similarity(text1: str, text2: str) -> float`\n2. `jaccard_similarity(text1: str, text2: str) -> float`\n3. `semantic_similarity(text1: str, text2: str) -> float` # Using word embeddings\n4. Compare bag-of-words vs embeddings approaches\n5. Handle synonyms and semantic relationships\n6. Implement efficient similarity for large text collections</pre>",
        "Similarity",
        "steps"
      ],
      "guid": "nlp_7ba6f7411af72",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "similarity",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Stemming vs Lemmatization</b><br>What is the key idea?",
        "Implement two functions: 1. `stem_words(words: List[str]) -> List[str]` - Porter stemming 2. `lemmatize_words(words: List[str], pos_tags: Optional[List[str]] = …",
        "Stemming Lemmatization",
        "understanding"
      ],
      "guid": "nlp_eb8d2a94c8aa9",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stemming_lemmatization",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Stemming vs Lemmatization<br>Implement: <code>get_wordnet_pos()</code>",
        "<pre><code>def get_wordnet_pos(treebank_tag):\n    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags.\"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN  # default</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_1e89d9735a90b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Stemming vs Lemmatization<br>Implement: <code>stem_words()</code>",
        "<pre><code>def stem_words(words: List[str]) -> List[str]:\n    \"\"\"Apply Porter stemming to words.\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word.lower()) for word in words]</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_c12bb62cb9af7",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Stemming vs Lemmatization<br>List the algorithm steps",
        "<pre>1. `stem_words(words: List[str]) -> List[str]` - Porter stemming\n2. `lemmatize_words(words: List[str], pos_tags: Optional[List[str]] = None) -> List[str]` - With POS awareness\n3. Show the difference between stemming and lemmatization\n4. Handle POS tags for better lemmatization\n5. Compare outputs side by side\n6. Which method to use for information retrieval vs text classification?</pre>",
        "Stemming Lemmatization",
        "steps"
      ],
      "guid": "nlp_2d336297f3d8f",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stemming_lemmatization",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Remove Stopwords (Configurable)</b><br>What is the key idea?",
        "Implement `remove_stopwords(tokens: List[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]` that removes English stopwords from a token list. Examp…",
        "Stop Word Removal",
        "understanding"
      ],
      "guid": "nlp_01b81cfdabdc9",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stop_word_removal",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Remove Stopwords (Configurable)<br>Implement: <code>remove_stopwords()</code>",
        "<pre><code>def remove_stopwords(tokens: Iterable[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]:\n    \"\"\"Remove stopwords, preserving order.\n\n    Case-insensitive membership check, preserves original casing in the output.\n    \"\"\"\n    stop_set = set(ENGLISH_STOPWORDS)\n    if extra_stopwords:\n        stop_set |= {w.lower() for w in extra_stopwords}\n\n    cleaned: List[str] = []\n    for token in tokens:\n        if token and token.lower() not in stop_set:\n            cleaned.append(token)\n    return cleaned</code></pre>",
        "Stop Word Removal",
        "implementation"
      ],
      "guid": "nlp_69c2d96519c9c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stop_word_removal",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Remove Stopwords (Configurable)<br>List the algorithm steps",
        "<pre>1. Use a standard English stopword list.\n2. Allow passing custom stopwords via `extra_stopwords`.\n3. Preserve original token order.\n4. Case-insensitive removal while preserving original casing.\n5. Support multiple languages.</pre>",
        "Stop Word Removal",
        "steps"
      ],
      "guid": "nlp_a116bd877683f",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stop_word_removal",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF-IDF Implementation</b><br>What is the key idea?",
        "**Time: 30 minutes** Implement TF-IDF from scratch to find document similarity. **Requirements:**",
        "TFIDF",
        "understanding"
      ],
      "guid": "nlp_5b3344339b3d5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: TF-IDF Implementation<br>Implement: <code>compute_tfidf()</code>",
        "<pre><code>def compute_tfidf(documents: List[str]) -> List[Dict[str, float]]:\n    \"\"\"\n    Compute TF-IDF vectors for documents.\n    \n    TF-IDF = Term Frequency × Inverse Document Frequency\n    - Emphasizes important words that appear frequently in a document\n    - But rarely across the entire collection\n    \"\"\"\n    # STEP 1: Handle edge cases\n    if not documents:\n        return []\n    \n    # STEP 2: Tokenize all documents (simple whitespace splitting)\n    # In real interviews, discuss more sophisticated tokenization\n    tokenized_docs = [doc.lower().split() for doc in documents]\n    \n    # STEP 3: Build vocabulary from all unique words\n    # This creates our feature space - each unique word becomes a dimension</code></pre>",
        "TFIDF",
        "implementation"
      ],
      "guid": "nlp_9c2ce62609f77",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: TF-IDF Implementation<br>Implement: <code>cosine_similarity()</code>",
        "<pre><code>def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two TF-IDF vectors.\n    \n    Cosine similarity = dot_product(v1, v2) / (||v1|| × ||v2||)\n    - Returns value between 0 and 1 (since TF-IDF values are non-negative)\n    - 1.0 = identical vectors, 0.0 = completely different\n    \"\"\"\n    # STEP 1: Find common terms between the two vectors\n    # Only these contribute to the dot product\n    common_terms = set(vec1.keys()) & set(vec2.keys())\n    \n    if not common_terms:\n        return 0.0  # No overlap = no similarity\n    \n    # STEP 2: Calculate dot product\n    # Sum of element-wise multiplication for common terms\n    dot_product = sum(vec1[term] * vec2[term] for term in common_terms)</code></pre>",
        "TFIDF",
        "implementation"
      ],
      "guid": "nlp_82531cfc19620",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: TF-IDF Implementation<br>Write the formula",
        "<pre>TF-IDF(t,d) = TF(t,d) × IDF(t); IDF(t)=log(N/df(t))</pre>",
        "TFIDF",
        "formula"
      ],
      "guid": "nlp_1f860c4527690",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: TF-IDF Implementation<br>Write the formula",
        "<pre>cosine_sim(A,B) = (A·B) / (||A|| × ||B||)</pre>",
        "TFIDF",
        "formula"
      ],
      "guid": "nlp_1f860c4527690",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: TF-IDF Implementation<br>List the algorithm steps",
        "<pre>1. Implement TF-IDF calculation from scratch\n2. Use cosine similarity for document comparison\n3. Handle empty documents gracefully\n4. Emphasizes important words that appear frequently in a document\n5. But rarely across the entire collection\n6. Returns value between 0 and 1 (since TF-IDF values are non-negative)</pre>",
        "TFIDF",
        "steps"
      ],
      "guid": "nlp_2742260902e6c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: TF-IDF Implementation<br>What's the complexity?",
        "<pre>Time complexity: O(d×v)\nSpace complexity: O(d×v)</pre>",
        "TFIDF",
        "complexity"
      ],
      "guid": "nlp_b04f69a0543b0",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: TF-IDF Implementation<br>Common pitfalls?",
        "<pre># STEP 1: Handle edge cases\n# STEP 4: Handle edge case of zero vectors</pre>",
        "TFIDF",
        "pitfalls"
      ],
      "guid": "nlp_c2625778f7580",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "pitfalls"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>What is the key idea?",
        "Build a complete text classification system: 1. `train_classifier(texts: List[str], labels: List[str]) -> ClassificationModel` 2. `predict(model: Classification…",
        "Text Classification",
        "understanding"
      ],
      "guid": "nlp_231acc48bc59f",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "text_classification",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Classification Pipeline<br>Implement: <code>extract_features()</code>",
        "<pre><code>def extract_features(texts: List[str], method: str = 'tfidf') -> Tuple[List[List[float]], List[str]]:\n    \"\"\"\n    Extract features from texts for classification.\n    \n    This is the MOST IMPORTANT step in text classification.\n    Feature quality determines model performance more than algorithm choice.\n    \"\"\"\n    \n    # STEP 1: Build vocabulary from all texts\n    # This creates our feature space - each unique word becomes a dimension\n    vocab = set()\n    for text in texts:\n        words = text.lower().split()  # Simple tokenization\n        vocab.update(words)\n    vocab = sorted(list(vocab))  # Sort for consistency\n    \n    if method == 'tfidf':\n        # STEP 2: Calculate document frequencies for IDF computation</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_1f6ec7f0b9697",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Classification Pipeline<br>Implement: <code>train_logistic_regression()</code>",
        "<pre><code>def train_logistic_regression(X: List[List[float]], y: List[int]) -> Dict:\n    \"\"\"\n    Train logistic regression classifier from scratch.\n    \n    Logistic regression is linear classifier with sigmoid activation.\n    Good baseline for text classification - simple but effective.\n    \"\"\"\n    # STEP 1: Convert to numpy arrays for easier math\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # STEP 2: Add bias term (intercept)\n    # This allows the decision boundary to not pass through origin\n    X_with_bias = np.column_stack([np.ones(len(X)), X_np])\n    \n    # STEP 3: Initialize weights randomly (small values)\n    # Small initialization prevents sigmoid saturation early in training\n    n_features = X_with_bias.shape[1]</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_7c4d632630706",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Classification Pipeline<br>Write the formula",
        "<pre>TF-IDF(t,d) = TF(t,d) × IDF(t); IDF(t)=log(N/df(t))</pre>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_5fa87c2ee13c1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Classification Pipeline<br>List the algorithm steps",
        "<pre>1. `train_classifier(texts: List[str], labels: List[str]) -> ClassificationModel`\n2. `predict(model: ClassificationModel, texts: List[str]) -> List[str]`\n3. `evaluate_classifier(y_true: List[str], y_pred: List[str]) -> Dict[str, float]`\n4. \"This movie is fantastic!\" -> \"positive\"\n5. \"Terrible experience, would not recommend\" -> \"negative\"\n6. \"It was okay, nothing special\" -> \"neutral\"</pre>",
        "Text Classification",
        "steps"
      ],
      "guid": "nlp_3552ac2e94802",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "text_classification",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>What is the key idea?",
        "**Time: 15 minutes** Implement a function that tokenizes text into words while handling edge cases. **Requirements:**",
        "Tokenization",
        "understanding"
      ],
      "guid": "nlp_c3a490ad2e10b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Tokenization<br>Implement: <code>tokenize()</code>",
        "<pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"\n    Tokenize text into words, preserving contractions and handling punctuation.\n    \n    This is ALWAYS asked in NLP interviews - seems simple but has many edge cases.\n    \n    Key challenges:\n    - Contractions: \"don't\" should stay as one token, not [\"don\", \"'\", \"t\"]\n    - Punctuation: \"Hello!\" should become [\"Hello\", \"!\"]\n    - Empty/None input: Handle gracefully\n    - Unicode characters: Different languages, emojis\n    \"\"\"\n    \n    # STEP 1: Handle edge cases first\n    # Always check for None/empty input in interviews\n    if not text:\n        return []\n    </code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_aaa3336dfa874",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Tokenization<br>Implement: <code>tokenize_simple()</code>",
        "<pre><code>def tokenize_simple(text: str) -> List[str]:\n    \"\"\"\n    Alternative approach: Replace-then-split method.\n    \n    Sometimes interviewers want to see multiple approaches.\n    This is simpler but less robust than regex.\n    \"\"\"\n    if not text:\n        return []\n    \n    # STEP 1: Replace punctuation with spaces (except apostrophes)\n    # This preserves contractions while isolating other punctuation\n    cleaned = re.sub(r\"[^\\w\\s']\", \" \", text)\n    \n    # STEP 2: Split on whitespace\n    # Simple but effective for basic cases\n    return cleaned.split()</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_97e59044d0075",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Tokenization<br>List the algorithm steps",
        "<pre>1. Split on whitespace and punctuation (except apostrophes in contractions)\n2. Handle empty/None input\n3. Preserve contractions like \"don't\", \"I'm\"\n4. Contractions: \"don't\" should stay as one token, not [\"don\", \"'\", \"t\"]\n5. Punctuation: \"Hello!\" should become [\"Hello\", \"!\"]\n6. Empty/None input: Handle gracefully</pre>",
        "Tokenization",
        "steps"
      ],
      "guid": "nlp_a91190b307377",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Tokenization<br>What's the complexity?",
        "<pre>Time complexity: O(n)\nSpace complexity: O(n)</pre>",
        "Tokenization",
        "complexity"
      ],
      "guid": "nlp_51985d74ae8b1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Text Tokenization<br>Common pitfalls?",
        "<pre>This is ALWAYS asked in NLP interviews - seems simple but has many edge cases.\n# STEP 1: Handle edge cases first\nThis demonstrates knowledge of English language patterns.\nComprehensive test cases that cover edge cases interviewers ask about.</pre>",
        "Tokenization",
        "pitfalls"
      ],
      "guid": "nlp_933b850bd3f78",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "pitfalls"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>What is the key idea?",
        "**Time: 30 minutes** Implement a simplified BPE tokenizer for subword segmentation. **Requirements:**",
        "Tokenization Advanced",
        "understanding"
      ],
      "guid": "nlp_eb31590b02eb8",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization_advanced",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Byte Pair Encoding (BPE) Tokenizer<br>Implement: <code>get_word_frequencies()</code>",
        "<pre><code>def get_word_frequencies(texts: List[str]) -> Dict[str, int]:\n    \"\"\"Get word frequencies with end-of-word marker.\"\"\"\n    word_freqs = Counter()\n    \n    for text in texts:\n        words = text.lower().split()\n        for word in words:\n            # Add end-of-word marker\n            word_with_marker = ' '.join(word) + ' </w>'\n            word_freqs[word_with_marker] += 1\n    \n    return dict(word_freqs)</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_0d1266d5050eb",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Byte Pair Encoding (BPE) Tokenizer<br>Implement: <code>get_pairs()</code>",
        "<pre><code>def get_pairs(word_freqs: Dict[str, int]) -> Counter:\n    \"\"\"Get all adjacent character pairs with their frequencies.\"\"\"\n    pairs = Counter()\n    \n    for word, freq in word_freqs.items():\n        chars = word.split()\n        for i in range(len(chars) - 1):\n            pair = (chars[i], chars[i + 1])\n            pairs[pair] += freq\n    \n    return pairs</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_092c25ac28d63",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Byte Pair Encoding (BPE) Tokenizer<br>List the algorithm steps",
        "<pre>1. Start with characters: ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd']\n2. Find most frequent pair: 'l' + 'l' -> 'll'\n3. Merge and repeat\n4. Start with character-level vocabulary\n5. Iteratively merge most frequent adjacent pairs\n6. Build final vocabulary with token IDs</pre>",
        "Tokenization Advanced",
        "steps"
      ],
      "guid": "nlp_150339e665e57",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization_advanced",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Topic Modeling with LSA and LDA</b><br>What is the key idea?",
        "Implement topic modeling algorithms: 1. `perform_lsa(documents: List[str], num_topics: int = 5) -> LSAModel` 2. `perform_lda(documents: List[str], num_topics: i…",
        "TopicModeling",
        "understanding"
      ],
      "guid": "nlp_2c7ce5e4b68b9",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "topicmodeling",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Topic Modeling with LSA and LDA<br>Implement: <code>perform_lsa()</code>",
        "<pre><code>def perform_lsa(documents: List[str], num_topics: int = 5) -> LSAModel:\n    \"\"\"Perform Latent Semantic Analysis.\"\"\"\n    model = LSAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "TopicModeling",
        "implementation"
      ],
      "guid": "nlp_3c9c49b8fb64f",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Topic Modeling with LSA and LDA<br>Implement: <code>perform_lda()</code>",
        "<pre><code>def perform_lda(documents: List[str], num_topics: int = 5) -> LDAModel:\n    \"\"\"Perform Latent Dirichlet Allocation.\"\"\"\n    model = LDAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "TopicModeling",
        "implementation"
      ],
      "guid": "nlp_061b6a3f47e5b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Topic Modeling with LSA and LDA<br>Write the formula",
        "<pre>TF-IDF(t,d) = TF(t,d) × IDF(t); IDF(t)=log(N/df(t))</pre>",
        "TopicModeling",
        "formula"
      ],
      "guid": "nlp_aa32a36a21dc8",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "topicmodeling",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Topic Modeling with LSA and LDA<br>List the algorithm steps",
        "<pre>1. `perform_lsa(documents: List[str], num_topics: int = 5) -> LSAModel`\n2. `perform_lda(documents: List[str], num_topics: int = 5) -> LDAModel`\n3. `extract_topics(model: Union[LSAModel, LDAModel], num_words: int = 10) -> List[List[Tuple[str, float]]]`\n4. `get_document_topics(model, document: str) -> List[Tuple[int, float]]`\n5. LSA using SVD decomposition\n6. LDA with Gibbs sampling or variational inference</pre>",
        "TopicModeling",
        "steps"
      ],
      "guid": "nlp_8278199a6596b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "topicmodeling",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Topic Modeling with LSA and LDA<br>Common pitfalls?",
        "<pre>\"Transfer learning reuses knowledge from one task to another\",</pre>",
        "TopicModeling",
        "pitfalls"
      ],
      "guid": "nlp_17898da162a95",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "topicmodeling",
        "pitfalls"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: BERT Fine-tuning for Sentiment Analysis</b><br>What is the key idea?",
        "Implement BERT fine-tuning for sentiment classification: 1. `load_pretrained_bert(model_name: str = 'bert-base-uncased') -> Model` 2. `fine_tune_bert(model, tex…",
        "Transformers",
        "understanding"
      ],
      "guid": "nlp_1549e500e2e84",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "transformers",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: BERT Fine-tuning for Sentiment Analysis<br>Implement: <code>load_pretrained_bert()</code>",
        "<pre><code>def load_pretrained_bert(model_name: str = 'bert-base-uncased', \n                        task: str = 'custom') -> Tuple[Union[nn.Module, any], any]:\n    \"\"\"Load pre-trained BERT model and tokenizer.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return None, None\n    \n    if task == 'custom':\n        # Custom model with our classification head\n        model = BERTSentimentClassifier(model_name)\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n    else:\n        # Hugging Face model for sequence classification\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_name, num_labels=2\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    return model, tokenizer</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_be6e247efadc5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: BERT Fine-tuning for Sentiment Analysis<br>Implement: <code>fine_tune_bert()</code>",
        "<pre><code>def fine_tune_bert(model: nn.Module, texts: List[str], labels: List[int],\n                  tokenizer, epochs: int = 3, batch_size: int = 16,\n                  learning_rate: float = 2e-5, warmup_steps: int = 0) -> nn.Module:\n    \"\"\"Fine-tune BERT for sentiment analysis.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return model\n    \n    # Create dataset and dataloader\n    dataset = SentimentDataset(texts, labels, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Setup optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n    \n    total_steps = len(dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_5efdfa30b8429",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: BERT Fine-tuning for Sentiment Analysis<br>List the algorithm steps",
        "<pre>1. `load_pretrained_bert(model_name: str = 'bert-base-uncased') -> Model`\n2. `fine_tune_bert(model, texts: List[str], labels: List[int], epochs: int = 3) -> Model`\n3. `predict_with_bert(model, texts: List[str]) -> List[Tuple[str, float, Dict]]`\n4. Use Hugging Face Transformers\n5. Handle tokenization with special tokens\n6. Implement proper fine-tuning strategy (freeze/unfreeze layers)</pre>",
        "Transformers",
        "steps"
      ],
      "guid": "nlp_8540abe669454",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "transformers",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Comprehensive Text Normalization Pipeline</b><br>What is the key idea?",
        "Build a complete text normalization system: 1. `normalize_text(text: str, options: Dict[str, bool]) -> str` 2. `clean_html(html: str) -> str`",
        "Utilities",
        "understanding"
      ],
      "guid": "nlp_ff64dcf408442",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "utilities",
        "understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Comprehensive Text Normalization Pipeline<br>Implement: <code>expand_contractions()</code>",
        "<pre><code>def expand_contractions(text: str) -> str:\n    \"\"\"Expand contractions in text.\"\"\"\n    # Convert to lowercase for matching\n    text_lower = text.lower()\n    \n    # Sort contractions by length (descending) to match longer ones first\n    sorted_contractions = sorted(CONTRACTIONS.items(), key=lambda x: len(x[0]), reverse=True)\n    \n    for contraction, expansion in sorted_contractions:\n        # Use word boundaries for accurate matching\n        pattern = r'\\b' + re.escape(contraction) + r'\\b'\n        text_lower = re.sub(pattern, expansion, text_lower, flags=re.IGNORECASE)\n    \n    # Preserve original capitalization pattern\n    result = []\n    for i, char in enumerate(text):\n        if i < len(text_lower):\n            if char.isupper():</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_66fdf7a88142e",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Comprehensive Text Normalization Pipeline<br>Implement: <code>expand_abbreviations()</code>",
        "<pre><code>def expand_abbreviations(text: str) -> str:\n    \"\"\"Expand common abbreviations.\"\"\"\n    for abbr, expansion in ABBREVIATIONS.items():\n        # Handle period after abbreviation\n        pattern1 = r'\\b' + re.escape(abbr) + r'\\.\\b'\n        pattern2 = r'\\b' + re.escape(abbr) + r'\\b'\n        \n        text = re.sub(pattern1, expansion, text, flags=re.IGNORECASE)\n        text = re.sub(pattern2, expansion, text, flags=re.IGNORECASE)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_744bb46336529",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Problem: Comprehensive Text Normalization Pipeline<br>List the algorithm steps",
        "<pre>1. `normalize_text(text: str, options: Dict[str, bool]) -> str`\n2. `clean_html(html: str) -> str`\n3. `expand_contractions(text: str) -> str`\n4. `normalize_unicode(text: str) -> str`\n5. Handle URLs, emails, phone numbers\n6. Expand contractions and abbreviations</pre>",
        "Utilities",
        "steps"
      ],
      "guid": "nlp_63a3fc79b886a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "utilities",
        "steps"
      ]
    }
  ]
}