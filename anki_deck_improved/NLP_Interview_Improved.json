{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-interview-deck-improved-2024",
  "deck_config_uuid": "nlp-deck-config-improved",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-deck-config-improved",
      "dyn": false,
      "name": "NLP Interview Improved",
      "new": {
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 20
      },
      "rev": {
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "perDay": 100
      }
    }
  ],
  "desc": "Complete NLP interview solutions optimized for memorization.",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "NLP Interview Prep (Improved)",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-model-improved",
      "sortf": 0,
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "latexPost": "\\end{document}",
      "tags": [],
      "vers": [],
      "css": "\n.card {\n    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;\n    font-size: 16px;\n    text-align: left;\n    color: #2d3748;\n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    padding: 20px;\n    min-height: 100vh;\n}\n.card-content {\n    background: white;\n    border-radius: 12px;\n    padding: 20px;\n    box-shadow: 0 10px 40px rgba(0,0,0,0.1);\n    max-width: 700px;\n    margin: 0 auto;\n}\npre {\n    background-color: #f7fafc;\n    color: #2d3748;\n    padding: 16px;\n    border-radius: 8px;\n    overflow-x: auto;\n    font-size: 14px;\n    line-height: 1.6;\n    font-family: 'SF Mono', Monaco, Consolas, monospace;\n    border-left: 4px solid #667eea;\n}\ncode {\n    background-color: #edf2f7;\n    padding: 2px 6px;\n    border-radius: 4px;\n    color: #d6336c;\n    font-size: 14px;\n}\nb {\n    color: #5a67d8;\n    font-weight: 600;\n    font-size: 18px;\n}\nhr {\n    border: none;\n    border-top: 2px solid #e2e8f0;\n    margin: 20px 0;\n}\n/* Mobile optimizations */\n@media (max-width: 600px) {\n    .card { padding: 15px; }\n    .card-content { padding: 15px; }\n    pre { font-size: 12px; padding: 12px; }\n}\n            ",
      "flds": [
        {
          "name": "Front",
          "ord": 0,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        },
        {
          "name": "Back",
          "ord": 1,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        },
        {
          "name": "Topic",
          "ord": 2,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        },
        {
          "name": "Type",
          "ord": 3,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20,
          "description": ""
        }
      ],
      "name": "NLP Interview Card (Improved)",
      "tmpls": [
        {
          "afmt": "<div class='card-content'>{{FrontSide}}<hr id=answer>{{Back}}<br><br><small style='color:#718096'>{{Topic}} • {{Type}}</small></div>",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class='card-content'>{{Front}}</div>"
        }
      ],
      "type": 0
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self</b><br>Write the complete implementation",
        "<pre><code>def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"\n    Numerically stable softmax implementation.\n    \n    Why stable? Subtracting max prevents overflow when exponentiating large numbers.\n    This is critical for attention weights which can have large values.\n    \"\"\"\n    x_max = np.max(x, axis=axis, keepdims=True)\n    exp_x = np.exp(x - x_max)\n    \n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)</code></pre>",
        "Attention Mechanisms",
        "full_implementation"
      ],
      "guid": "nlp_bf899785af07d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Self-Attention</b><br>Write the attention formula",
        "<pre>Attention(Q,K,V) = softmax(QK^T / √d_k)V\n\nSteps:\n1. Compute scores: QK^T\n2. Scale: divide by √d_k\n3. Apply softmax for weights\n4. Multiply by V for output</pre>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_39b82a34ded1c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self</b><br>List the algorithm steps",
        "<pre>1. Initialize weight matrices\n2. Create Query, Key, Value matrices\n3. Calculate attention scores\n4. Scale by sqrt(d_k)\n5. Apply softmax to get attention weights\n6. Apply attention to values</pre>",
        "Attention Mechanisms",
        "algorithm_steps"
      ],
      "guid": "nlp_fd477dbea0ab3",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self</b><br>What's the time and space complexity?",
        "<pre>Time complexity: O(n²d)\nSpace complexity: O(n²)\n\nWhere:\n- n = sequence length\n- d = dimension size</pre>",
        "Attention Mechanisms",
        "complexity"
      ],
      "guid": "nlp_28d5ad7933b6b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self</b><br>What are the key interview talking points?",
        "<pre>• Create Query, Key, Value matrices\n• Multiple attention heads can focus on different aspects\n• print(\"Self-Attention Mechanism - Interview Demo\")\n• Divide by √d_k to prevent gradient vanishing in softmax</pre>",
        "Attention Mechanisms",
        "insights"
      ],
      "guid": "nlp_064cf5f8eeb03",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Self</b><br>What are common implementation mistakes?",
        "<pre>❌ Forgetting to scale by √d_k\n❌ Wrong dimension in matrix multiplication\n❌ Not applying causal mask for autoregressive\n❌ Incorrect softmax axis</pre>",
        "Attention Mechanisms",
        "pitfalls"
      ],
      "guid": "nlp_93f71970f4308",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "attention_mechanisms",
        "mistakes"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>Write the complete implementation",
        "<pre><code>def analyze_vocabulary_distribution(documents: List[str]) -> Dict[str, int]:\n    \"\"\"\n    Analyze vocabulary characteristics - good for follow-up questions.\n    \n    INTERVIEW INSIGHTS:\n    - Most words appear very rarely (Zipf's law)\n    - Top 100 words account for ~50% of text\n    - Vocabulary size grows with more documents\n    \"\"\"\n    vocab, _ = create_bow_vector(documents)\n    \n    word_doc_counts = {}\n    for word in vocab:\n        count = sum(1 for doc in documents if word in doc.lower())\n        word_doc_counts[word] = count\n    \n    return word_doc_counts</code></pre>",
        "BoW Vectors",
        "full_implementation"
      ],
      "guid": "nlp_e2a29df5f362d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>List the algorithm steps",
        "<pre>1. Handle edge case\n2. Build vocabulary from all documents\n3. Create ordered vocabulary\n4. Convert each document to vector\n5. Input validation\n6. Calculate dot product (numerator)</pre>",
        "BoW Vectors",
        "algorithm_steps"
      ],
      "guid": "nlp_f0579c113f02e",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Bag of Words from Scratch</b><br>What's the time and space complexity?",
        "<pre>Time: O(d × n × v)\nSpace: O(d × v)</pre>",
        "BoW Vectors",
        "complexity"
      ],
      "guid": "nlp_45fe588623ed1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "bow_vectors",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text CNN for Classification</b><br>Write the complete implementation",
        "<pre><code>def text_to_sequence(text: str, vocab: Dict[str, int], max_len: int = 10) -> List[int]:\n    \"\"\"Convert text to padded integer sequence.\"\"\"\n    words = text.lower().split()\n    sequence = [vocab.get(word, 0) for word in words]  # 0 for unknown words\n    \n    if len(sequence) < max_len:\n        sequence += [0] * (max_len - len(sequence))\n    else:\n        sequence = sequence[:max_len]\n    \n    return sequence</code></pre>",
        "CNN Text",
        "full_implementation"
      ],
      "guid": "nlp_918c4f5a90af7",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "cnn_text",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip</b><br>Write the complete implementation",
        "<pre><code>def sigmoid(x: float) -> float:\n    \"\"\"\n    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n    \n    Used in Word2Vec to convert dot products to probabilities.\n    Clamp input to prevent numerical overflow/underflow.\n    \"\"\"\n    clamped_x = max(-500, min(500, x))\n    return 1 / (1 + math.exp(-clamped_x))</code></pre>",
        "Embeddings",
        "full_implementation"
      ],
      "guid": "nlp_40a79d927dc33",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip</b><br>List the algorithm steps",
        "<pre>1. Handle missing words\n2. Get current embeddings\n3. FORWARD PASS - Calculate current similarity\n4. BACKWARD PASS - Calculate gradients\n5. UPDATE EMBEDDINGS\n6. Handle missing words</pre>",
        "Embeddings",
        "algorithm_steps"
      ],
      "guid": "nlp_f30341fe0a98a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Word2Vec Skip</b><br>What are the key interview talking points?",
        "<pre>• print(\"WORD2VEC SKIP-GRAM - Interview Demo\")\n• \")</pre>",
        "Embeddings",
        "insights"
      ],
      "guid": "nlp_c1ce5e08317b5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "embeddings",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki</b><br>Write the complete implementation",
        "<pre><code>def spell_check_approach():\n    \"\"\"\n    KEY: Use edit distance + frequency ranking\n    STEPS: 1) Find close words 2) Rank by frequency\n    INSIGHT: Most typos are 1-2 edits away\n    \"\"\"\n    pass</code></pre>",
        "Example Anki Refactor",
        "full_implementation"
      ],
      "guid": "nlp_080d4d9ea6ad5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example: Anki</b><br>What are the key interview talking points?",
        "<pre>• Use edit distance + frequency ranking\n• Higher frequency, lower distance = better\n• Mention trade-offs:</pre>",
        "Example Anki Refactor",
        "insights"
      ],
      "guid": "nlp_21977d9d62488",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "example_anki_refactor",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Fine</b><br>Write the complete implementation",
        "<pre><code>def add_classification_head(pretrained_model_dim: int, num_classes: int) -> Dict:\n    \"\"\"Add classification head to pretrained LLM.\"\"\"\n    \n    std = np.sqrt(2.0 / (pretrained_model_dim + num_classes))\n    \n    return {\n        'W_cls': np.random.randn(pretrained_model_dim, num_classes) * std,\n        'b_cls': np.zeros(num_classes)\n    }</code></pre>",
        "Fine Tuning",
        "full_implementation"
      ],
      "guid": "nlp_172f14cab496d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "fine_tuning",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>Write the complete implementation",
        "<pre><code>def gelu(x: np.ndarray) -> np.ndarray:\n    \"\"\"GELU activation function used in GPT.\"\"\"\n    return 0.5 * x * (1 + np.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))</code></pre>",
        "GPT Implementation",
        "full_implementation"
      ],
      "guid": "nlp_35761a9632096",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "gpt_implementation",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: GPT Transformer Block</b><br>What are the key interview talking points?",
        "<pre>• \")</pre>",
        "GPT Implementation",
        "insights"
      ],
      "guid": "nlp_9430c74349c45",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "gpt_implementation",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Instruction Following Setup</b><br>Write the complete implementation",
        "<pre><code>def format_instruction_data(instruction: str, response: str) -> str:\n    \"\"\"Format instruction-response pair for training.\"\"\"\n    return f\"### Instruction:\\n{instruction}\\n### Response:\\n{response}\"</code></pre>",
        "Instruction Tuning",
        "full_implementation"
      ],
      "guid": "nlp_67630723fb9e4",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "instruction_tuning",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Instruction Following Setup</b><br>What are the key interview talking points?",
        "<pre>• \")</pre>",
        "Instruction Tuning",
        "insights"
      ],
      "guid": "nlp_9b13726ea24c6",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "instruction_tuning",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Generation with LLMs</b><br>Write the complete implementation",
        "<pre><code>def softmax(logits: List[float], temperature: float = 1.0) -> List[float]:\n    \"\"\"Convert logits to probabilities with temperature scaling.\"\"\"\n    if temperature != 1.0:\n        logits = [l / temperature for l in logits]\n    \n    max_logit = max(logits)\n    exp_logits = [math.exp(l - max_logit) for l in logits]\n    sum_exp = sum(exp_logits)\n    \n    return [exp_l / sum_exp for exp_l in exp_logits]</code></pre>",
        "LLM Fundamentals",
        "full_implementation"
      ],
      "guid": "nlp_f7e6a4a3364d5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "llm_fundamentals",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: LLM Evaluation Metrics</b><br>Write the complete implementation",
        "<pre><code>def calculate_perplexity(model_probs: List[List[float]], \n                        target_tokens: List[int]) -> float:\n    \"\"\"Calculate perplexity from model probabilities.\"\"\"\n    if not model_probs or not target_tokens:\n        return float('inf')\n    \n    if len(model_probs) != len(target_tokens):\n        return float('inf')\n    \n    log_likelihood = 0.0\n    num_tokens = 0\n    \n    for probs, target_token in zip(model_probs, target_tokens):\n        if target_token < len(probs):\n            prob = probs[target_token]\n            \n            log_likelihood += math.log(max(prob, 1e-10))\n            num_tokens += 1\n    \n    if num_tokens == 0:\n        return float('inf')\n    \n    avg_log_likelihood = log_likelihood / num_tokens\n    perplexity = math.exp(-avg_log_likelihood)\n    \n    return perplexity</code></pre>",
        "Model Evaluation",
        "full_implementation"
      ],
      "guid": "nlp_c8ecaad9a747b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "model_evaluation",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Named Entity Recognition with Custom Entities</b><br>Write the complete implementation",
        "<pre><code>def extract_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract named entities using spaCy.\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        entities[ent.label_].append(ent.text)\n    \n    for label in entities:\n        entities[label] = list(dict.fromkeys(entities[label]))\n    \n    return dict(entities)</code></pre>",
        "NER",
        "full_implementation"
      ],
      "guid": "nlp_8671636a4a805",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ner",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: N</b><br>Write the complete implementation",
        "<pre><code>def build_bigram_model(texts: List[str]) -> Dict[str, Dict[str, float]]:\n    \"\"\"Build bigram language model with probabilities.\"\"\"\n    if not texts:\n        return {}\n    \n    bigram_counts = defaultdict(Counter)\n    \n    for text in texts:\n        words = text.lower().split()\n        \n        words = ['<START>'] + words + ['<END>']\n        \n        for i in range(len(words) - 1):\n            w1, w2 = words[i], words[i + 1]\n            bigram_counts[w1][w2] += 1\n    \n    model = {}\n    for w1, w2_counts in bigram_counts.items():\n        total = sum(w2_counts.values())\n        model[w1] = {w2: count/total for w2, count in w2_counts.items()}\n    \n    return model</code></pre>",
        "NGrams",
        "full_implementation"
      ],
      "guid": "nlp_1b88d7051062c",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "ngrams",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Neural Network from Scratch</b><br>Write the complete implementation",
        "<pre><code>    def sigmoid(x: np.ndarray) -> np.ndarray:\n        \"\"\"Sigmoid activation function.\"\"\"\n        x = np.clip(x, -500, 500)\n        return 1 / (1 + np.exp(-x))</code></pre>",
        "Neural Fundamentals",
        "full_implementation"
      ],
      "guid": "nlp_3021ca3360a7d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "neural_fundamentals",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Part</b><br>Write the complete implementation",
        "<pre><code>def pos_tag_text(text: str) -> List[Tuple[str, str]]:\n    \"\"\"POS tag text using NLTK's default tagger (Penn Treebank tagset).\"\"\"\n    tokens = nltk.word_tokenize(text)\n    return nltk.pos_tag(tokens)</code></pre>",
        "POS Tagging",
        "full_implementation"
      ],
      "guid": "nlp_209506cb10266",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "pos_tagging",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Rule</b><br>Write the complete implementation",
        "<pre><code>def classify_sentiment(sentiment_scores: Dict[str, float]) -> str:\n    \"\"\"\n    Convert sentiment scores to simple classification.\n    \n    CLASSIFICATION THRESHOLDS (VADER standard):\n    - compound >= 0.05: positive\n    - compound <= -0.05: negative  \n    - -0.05 < compound < 0.05: neutral\n    \n    These thresholds are empirically determined from testing.\n    \"\"\"\n    compound = sentiment_scores['compound']\n    \n    if compound >= 0.05:\n        return 'positive'\n    elif compound <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'</code></pre>",
        "Sentiment Analysis",
        "full_implementation"
      ],
      "guid": "nlp_e708694e49b18",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Rule</b><br>List the algorithm steps",
        "<pre>1. Handle edge cases first\n2. Simple tokenization\n3. Get base sentiment scores for each word\n4. Handle punctuation emphasis\n5. Calculate final sentiment distribution\n6. Calculate compound score (overall sentiment)</pre>",
        "Sentiment Analysis",
        "algorithm_steps"
      ],
      "guid": "nlp_ef4b2fe28f804",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Rule</b><br>What are the key interview talking points?",
        "<pre>• print(\"RULE-BASED SENTIMENT ANALYSIS - Interview Demo\")\n• \")</pre>",
        "Sentiment Analysis",
        "insights"
      ],
      "guid": "nlp_e5d7633d8d41f",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sentiment_analysis",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Simple LSTM for Sentiment</b><br>Write the complete implementation",
        "<pre><code>def lstm_cell(x_t: List[float], h_prev: List[float], c_prev: List[float],\n              weights: Dict) -> Tuple[List[float], List[float]]:\n    \"\"\"Single LSTM cell forward pass.\"\"\"\n    hidden_size = len(h_prev)\n    combined = x_t + h_prev\n    \n    f_t = [sigmoid(dot_product(combined, weights['Wf'][i]) + weights['bf'][i]) for i in range(hidden_size)]\n    i_t = [sigmoid(dot_product(combined, weights['Wi'][i]) + weights['bi'][i]) for i in range(hidden_size)]\n    o_t = [sigmoid(dot_product(combined, weights['Wo'][i]) + weights['bo'][i]) for i in range(hidden_size)]\n    \n    c_candidate = [tanh(dot_product(combined, weights['Wc'][i]) + weights['bc'][i]) for i in range(hidden_size)]\n    \n    c_t = [f_t[i] * c_prev[i] + i_t[i] * c_candidate[i] for i in range(hidden_size)]\n    h_t = [o_t[i] * tanh(c_t[i]) for i in range(hidden_size)]\n    \n    return h_t, c_t</code></pre>",
        "Sequence Models",
        "full_implementation"
      ],
      "guid": "nlp_142ded0cf7e77",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "sequence_models",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Similarity Metrics</b><br>Write the complete implementation",
        "<pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"Simple tokenization.\"\"\"\n    return text.lower().split()</code></pre>",
        "Similarity",
        "full_implementation"
      ],
      "guid": "nlp_2392c06e7db06",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "similarity",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cosine Similarity</b><br>Write the formula and implementation",
        "<pre>cosine_sim(A,B) = A·B / (||A|| × ||B||)\n\ndef cosine_similarity(vec1, vec2):\n    dot_product = sum(a*b for a,b in zip(vec1, vec2))\n    norm1 = sqrt(sum(a**2 for a in vec1))\n    norm2 = sqrt(sum(b**2 for b in vec2))\n    return dot_product / (norm1 * norm2)</pre>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_aa6d1fef19e90",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Stemming vs Lemmatization</b><br>Write the complete implementation",
        "<pre><code>def get_wordnet_pos(treebank_tag):\n    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags.\"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN  # default</code></pre>",
        "Stemming Lemmatization",
        "full_implementation"
      ],
      "guid": "nlp_6b95545c5e6fa",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stemming_lemmatization",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Remove Stopwords (Configurable)</b><br>Write the complete implementation",
        "<pre><code>def remove_stopwords(tokens: Iterable[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]:\n    \"\"\"Remove stopwords, preserving order.\n\n    Case-insensitive membership check, preserves original casing in the output.\n    \"\"\"\n    stop_set = set(ENGLISH_STOPWORDS)\n    if extra_stopwords:\n        stop_set |= {w.lower() for w in extra_stopwords}\n\n    cleaned: List[str] = []\n    for token in tokens:\n        if token and token.lower() not in stop_set:\n            cleaned.append(token)\n    return cleaned</code></pre>",
        "Stop Word Removal",
        "full_implementation"
      ],
      "guid": "nlp_c53f1b799e4cd",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "stop_word_removal",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>TF-IDF</b><br>Write the formula and explain each part",
        "<pre>TF-IDF(t,d) = TF(t,d) × IDF(t)\n\nTF(t,d) = count(t in d) / total_terms(d)\nIDF(t) = log(N / df(t))\n\nWhere:\n- t = term\n- d = document\n- N = total documents\n- df(t) = documents containing t</pre>",
        "TFIDF",
        "formula"
      ],
      "guid": "nlp_4f90c5ed33b43",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF</b><br>List the algorithm steps",
        "<pre>1. Handle edge cases\n2. Tokenize all documents (simple whitespace splitting)\n3. Build vocabulary from all unique words\n4. Calculate Document Frequency (DF) for each term\n5. Calculate TF-IDF for each document\n6. Find common terms between the two vectors</pre>",
        "TFIDF",
        "algorithm_steps"
      ],
      "guid": "nlp_e907efc46cda1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF</b><br>What's the time and space complexity?",
        "<pre>Time complexity: O(d×v)\nSpace complexity: O(d×v)\n\nWhere:\n- d = number of documents\n- v = vocabulary size\n- n = average document length</pre>",
        "TFIDF",
        "complexity"
      ],
      "guid": "nlp_651ff4dbefa8a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF</b><br>What are the key interview talking points?",
        "<pre>• Show step-by-step execution\n• \")\n• TF-IDF balances term frequency with document rarity\n• Use log in IDF to dampen the effect of very rare terms</pre>",
        "TFIDF",
        "insights"
      ],
      "guid": "nlp_bc447dcd51970",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: TF</b><br>What are common implementation mistakes?",
        "<pre>❌ Forgetting to handle empty documents\n❌ Not normalizing TF by document length\n❌ Using linear scale instead of log for IDF\n❌ Division by zero in cosine similarity</pre>",
        "TFIDF",
        "pitfalls"
      ],
      "guid": "nlp_4f106f60ff8d8",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tfidf",
        "mistakes"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>List the algorithm steps",
        "<pre>1. Build vocabulary from all texts\n2. Calculate document frequencies for IDF computation\n3. Convert each text to TF-IDF vector\n4. Convert to numpy arrays for easier math\n5. Add bias term (intercept)\n6. Initialize weights randomly (small values)</pre>",
        "Text Classification",
        "algorithm_steps"
      ],
      "guid": "nlp_40a526be8c6f1",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "text_classification",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Classification Pipeline</b><br>What are the key interview talking points?",
        "<pre>• print(\"TEXT CLASSIFICATION - Interview Walkthrough\")\n• \")</pre>",
        "Text Classification",
        "insights"
      ],
      "guid": "nlp_2dad2bd0782a2",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "text_classification",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>Write the complete implementation",
        "<pre><code>def tokenize_simple(text: str) -> List[str]:\n    \"\"\"\n    Alternative approach: Replace-then-split method.\n    \n    Sometimes interviewers want to see multiple approaches.\n    This is simpler but less robust than regex.\n    \"\"\"\n    if not text:\n        return []\n    \n    cleaned = re.sub(r\"[^\\w\\s']\", \" \", text)\n    \n    return cleaned.split()</code></pre>",
        "Tokenization",
        "full_implementation"
      ],
      "guid": "nlp_abd694be465d5",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>List the algorithm steps",
        "<pre>1. Handle edge cases first\n2. Use regex pattern for tokenization\n3. Replace punctuation with spaces (except apostrophes)\n4. Split on whitespace\n5. Handle special entities before general tokenization\n6. Case handling</pre>",
        "Tokenization",
        "algorithm_steps"
      ],
      "guid": "nlp_f9b8b596c025b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "steps"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>What's the time and space complexity?",
        "<pre>Time complexity: O(n)\nSpace complexity: O(n)</pre>",
        "Tokenization",
        "complexity"
      ],
      "guid": "nlp_06ddfdaebc5ab",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Text Tokenization</b><br>What are the key interview talking points?",
        "<pre>• - Contractions: \"don't\" should stay as one token, not [\"don\", \"'\", \"t\"]\n• \"\"\"\n• Show your thinking process\n• Handle punctuation, contractions, and special characters</pre>",
        "Tokenization",
        "insights"
      ],
      "guid": "nlp_67bf47e3df85b",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>Write the complete implementation",
        "<pre><code>def get_word_frequencies(texts: List[str]) -> Dict[str, int]:\n    \"\"\"Get word frequencies with end-of-word marker.\"\"\"\n    word_freqs = Counter()\n    \n    for text in texts:\n        words = text.lower().split()\n        for word in words:\n            word_with_marker = ' '.join(word) + ' </w>'\n            word_freqs[word_with_marker] += 1\n    \n    return dict(word_freqs)</code></pre>",
        "Tokenization Advanced",
        "full_implementation"
      ],
      "guid": "nlp_b4f4d1cc86b9a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization_advanced",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>What are the key interview talking points?",
        "<pre>• \")\n• Handle punctuation, contractions, and special characters\n• Consider subword tokenization (BPE) for OOV handling</pre>",
        "Tokenization Advanced",
        "insights"
      ],
      "guid": "nlp_c56bb9f0ac098",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "tokenization_advanced",
        "interview_tips"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Topic Modeling with LSA and LDA</b><br>Write the complete implementation",
        "<pre><code>    def fit(self, documents: List[str]):\n        \"\"\"Fit LSA model to documents.\"\"\"\n        if self.use_tfidf:\n            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n        else:\n            self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n        \n        doc_term_matrix = self.vectorizer.fit_transform(documents)\n        self.vocabulary = self.vectorizer.get_feature_names_out()\n        \n        self.svd = TruncatedSVD(n_components=self.num_topics, random_state=42)\n        self.document_topic_matrix = self.svd.fit_transform(doc_term_matrix)\n        \n        self.topic_word_matrix = self.svd.components_\n        \n        self.document_topic_matrix = normalize(self.document_topic_matrix, axis=1)</code></pre>",
        "TopicModeling",
        "full_implementation"
      ],
      "guid": "nlp_98f28297c1078",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "topicmodeling",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: BERT Fine</b><br>Write the complete implementation",
        "<pre><code>def load_pretrained_bert(model_name: str = 'bert-base-uncased', \n                        task: str = 'custom') -> Tuple[Union[nn.Module, any], any]:\n    \"\"\"Load pre-trained BERT model and tokenizer.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return None, None\n    \n    if task == 'custom':\n        model = BERTSentimentClassifier(model_name)\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n    else:\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_name, num_labels=2\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    return model, tokenizer</code></pre>",
        "Transformers",
        "full_implementation"
      ],
      "guid": "nlp_88e31cbd13b7a",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "transformers",
        "implementation",
        "core"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Comprehensive Text Normalization Pipeline</b><br>Write the complete implementation",
        "<pre><code>def expand_contractions(text: str) -> str:\n    \"\"\"Expand contractions in text.\"\"\"\n    text_lower = text.lower()\n    \n    sorted_contractions = sorted(CONTRACTIONS.items(), key=lambda x: len(x[0]), reverse=True)\n    \n    for contraction, expansion in sorted_contractions:\n        pattern = r'\\b' + re.escape(contraction) + r'\\b'\n        text_lower = re.sub(pattern, expansion, text_lower, flags=re.IGNORECASE)\n    \n    result = []\n    for i, char in enumerate(text):\n        if i < len(text_lower):\n            if char.isupper():\n                result.append(text_lower[i].upper())\n            else:\n                result.append(text_lower[i])\n        else:\n            result.append(char)\n    \n    return ''.join(result)</code></pre>",
        "Utilities",
        "full_implementation"
      ],
      "guid": "nlp_8bfe78ced795d",
      "note_model_uuid": "nlp-model-improved",
      "tags": [
        "utilities",
        "implementation",
        "core"
      ]
    }
  ]
}