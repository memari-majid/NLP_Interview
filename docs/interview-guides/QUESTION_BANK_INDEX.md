# NLP Interview Question Bank - Master Index

A comprehensive, searchable index of all NLP interview questions organized by multiple dimensions.

## üìä Questions by Difficulty

### üü¢ Easy (15-20 min)
| Question | Topic | Companies | Link |
|----------|-------|-----------|------|
| Basic Tokenization | Text Processing | All | [Solution](NLP/Tokenization/tokenization_solution.py) |
| Stop Word Removal | Text Processing | Amazon, Microsoft | [Solution](NLP/Stop_Word_Removal/stopword_removal_solution.py) |
| Stemming vs Lemmatization | Text Processing | Google | [Solution](NLP/Stemming_Lemmatization/stemming_lemmatization_solution.py) |
| Bag of Words | Vectorization | Meta, Apple | [Solution](NLP/BoW_Vectors/bag_of_words_solution.py) |
| Cosine Similarity | Similarity | All | [Solution](NLP/Similarity/cosine_similarity_solution.py) |
| N-gram Generation | Feature Engineering | Amazon | [Solution](NLP/NGrams/ngrams_solution.py) |
| Regex Entity Extraction | Information Extraction | Startups | [Solution](NLP/Regex_NLP/regex_patterns_solution.py) |

### üü° Medium (25-30 min)
| Question | Topic | Companies | Link |
|----------|-------|-----------|------|
| TF-IDF Implementation | Vectorization | Google, Amazon | [Solution](NLP/TFIDF/tfidf_solution.py) |
| Word2Vec Training | Embeddings | Meta, Microsoft | [Solution](NLP/Embeddings/word2vec_solution.py) |
| Text Classification Pipeline | ML Applications | All | [Solution](NLP/Text_Classification/text_classification_solution.py) |
| POS Tagging | Sequence Labeling | Google | [Solution](NLP/POS_Tagging/pos_tagging_solution.py) |
| Named Entity Recognition | Information Extraction | Amazon, Apple | [Solution](NLP/NER/ner_solution.py) |
| Sentiment Analysis (VADER) | ML Applications | Startups | [Solution](NLP/Sentiment_Analysis/vader_sentiment_solution.py) |
| Simple Perceptron | Neural Networks | Meta | [Solution](NLP/Neural_Fundamentals/perceptron_neural_net_solution.py) |

### üî¥ Hard (30-45 min)
| Question | Topic | Companies | Link |
|----------|-------|-----------|------|
| Self-Attention Mechanism | Transformers | OpenAI, Google | [Solution](NLP/Attention_Mechanisms/self_attention_solution.py) |
| BERT Fine-tuning | Transfer Learning | Meta, Microsoft | [Solution](NLP/Transformers/bert_sentiment_solution.py) |
| BPE Tokenization | Advanced Tokenization | OpenAI, Anthropic | [Solution](NLP/Tokenization_Advanced/bpe_tokenization_solution.py) |
| GPT Block Implementation | LLMs | OpenAI | [Solution](NLP/GPT_Implementation/gpt_block_solution.py) |
| LSTM from Scratch | Sequence Models | Google, Amazon | [Solution](NLP/Sequence_Models/lstm_sentiment_solution.py) |
| Topic Modeling (LDA) | Unsupervised Learning | Research Labs | [Solution](NLP/TopicModeling/lsa_lda_solution.py) |
| CNN for Text | Neural Networks | Meta | [Solution](NLP/CNN_Text/cnn_text_classification_solution.py) |
| Text Generation | LLMs | OpenAI, Anthropic | [Solution](NLP/LLM_Fundamentals/text_generation_solution.py) |
| Instruction Tuning | LLMs | OpenAI, Google | [Solution](NLP/Instruction_Tuning/instruction_following_solution.py) |

## üè¢ Questions by Company

### OpenAI / Anthropic
- üî¥ [Self-Attention Implementation](NLP/Attention_Mechanisms/)
- üî¥ [GPT Block](NLP/GPT_Implementation/)
- üî¥ [BPE Tokenization](NLP/Tokenization_Advanced/)
- üî¥ [Text Generation Strategies](NLP/LLM_Fundamentals/)
- üü° [Model Evaluation Metrics](NLP/Model_Evaluation/)

### Google
- üü° [TF-IDF for Search](NLP/TFIDF/)
- üî¥ [BERT Fine-tuning](NLP/Transformers/)
- üü¢ [Tokenization Edge Cases](NLP/Tokenization/)
- üü° [Word Embeddings](NLP/Embeddings/)
- üî¥ [Sequence Models](NLP/Sequence_Models/)

### Meta (Facebook)
- üü° [Text Classification](NLP/Text_Classification/)
- üî¥ [CNN for Text](NLP/CNN_Text/)
- üü° [Word2Vec Implementation](NLP/Embeddings/)
- üü¢ [Similarity Metrics](NLP/Similarity/)

### Amazon
- üü¢ [Stop Words for Search](NLP/Stop_Word_Removal/)
- üü° [Sentiment Analysis](NLP/Sentiment_Analysis/)
- üü° [Named Entity Recognition](NLP/NER/)
- üü¢ [N-gram Features](NLP/NGrams/)

### Microsoft
- üü° [Text Classification Pipeline](NLP/Text_Classification/)
- üî¥ [BERT Applications](NLP/Transformers/)
- üü¢ [Text Normalization](NLP/Utilities/)
- üü° [Fine-tuning Strategies](NLP/Fine_Tuning/)

## üéØ Questions by Topic Category

### Text Preprocessing (Foundation)
1. [Tokenization](NLP/Tokenization/) - Split text into tokens
2. [Stop Words](NLP/Stop_Word_Removal/) - Remove common words
3. [Stemming/Lemmatization](NLP/Stemming_Lemmatization/) - Normalize words
4. [Text Normalization](NLP/Utilities/) - Clean and standardize

### Feature Engineering (Classical NLP)
1. [Bag of Words](NLP/BoW_Vectors/) - Count vectorization
2. [TF-IDF](NLP/TFIDF/) - Term weighting
3. [N-grams](NLP/NGrams/) - Sequential features
4. [POS Tagging](NLP/POS_Tagging/) - Grammatical analysis

### Embeddings & Similarity
1. [Word2Vec](NLP/Embeddings/) - Dense word vectors
2. [Cosine Similarity](NLP/Similarity/) - Vector comparison
3. [Semantic Search](NLP/TFIDF/) - Document retrieval

### Machine Learning Applications
1. [Text Classification](NLP/Text_Classification/) - Categorization
2. [Sentiment Analysis](NLP/Sentiment_Analysis/) - Opinion mining
3. [Named Entity Recognition](NLP/NER/) - Entity extraction
4. [Topic Modeling](NLP/TopicModeling/) - Theme discovery

### Deep Learning
1. [Perceptron](NLP/Neural_Fundamentals/) - Basic neural unit
2. [CNN for Text](NLP/CNN_Text/) - Convolutional approach
3. [LSTM/RNN](NLP/Sequence_Models/) - Sequential models
4. [Attention Mechanism](NLP/Attention_Mechanisms/) - Focus mechanism

### Transformers & LLMs
1. [Self-Attention](NLP/Attention_Mechanisms/) - Core mechanism
2. [BERT Fine-tuning](NLP/Transformers/) - Transfer learning
3. [GPT Architecture](NLP/GPT_Implementation/) - Generative models
4. [BPE Tokenization](NLP/Tokenization_Advanced/) - Subword units
5. [Text Generation](NLP/LLM_Fundamentals/) - Decoding strategies
6. [Instruction Tuning](NLP/Instruction_Tuning/) - Task adaptation
7. [Model Evaluation](NLP/Model_Evaluation/) - Performance metrics

## üîó Related Problems Matrix

| If you solved... | Try these next... | Why? |
|-----------------|-------------------|------|
| Tokenization | BPE, Stop Words | Build on tokenization |
| TF-IDF | Word2Vec, Text Classification | From sparse to dense vectors |
| Word2Vec | Self-Attention, Similarity | Embedding applications |
| Text Classification | CNN Text, BERT Fine-tuning | Neural approaches |
| Self-Attention | GPT Block, Transformers | Transformer architecture |
| LSTM | Self-Attention, CNN Text | Compare architectures |

## ‚è±Ô∏è Time-Based Practice Plans

### 30-Minute Session
- 2 Easy problems (15 min each)
- OR 1 Medium problem with review

### 1-Hour Session  
- 1 Easy (15 min) + 1 Medium (30 min) + Review (15 min)
- OR 2 Medium problems

### 2-Hour Session
- 1 Easy + 2 Medium + 1 Hard
- OR Mock interview format (3-4 problems)

## üè∑Ô∏è Problem Tags

### By Concept
`#vectorization` `#embeddings` `#classification` `#generation` `#attention` `#tokenization` `#similarity` `#sequence-models` `#transformers`

### By Technique
`#dynamic-programming` `#sliding-window` `#matrix-operations` `#gradient-descent` `#beam-search` `#greedy` `#probabilistic`

### By Data Structure
`#arrays` `#hashmaps` `#tries` `#matrices` `#graphs` `#queues` `#heaps`

## üìà Learning Paths

### Path 1: Classical NLP ‚Üí Modern NLP
1. Tokenization ‚Üí Stop Words ‚Üí Stemming
2. BoW ‚Üí TF-IDF ‚Üí Word2Vec
3. Text Classification ‚Üí Sentiment Analysis
4. Self-Attention ‚Üí BERT ‚Üí GPT

### Path 2: ML Engineer Focus
1. Text Classification ‚Üí Feature Engineering
2. TF-IDF ‚Üí Similarity ‚Üí Search
3. Word2Vec ‚Üí Fine-tuning
4. Model Evaluation ‚Üí System Design

### Path 3: Research/LLM Focus
1. Self-Attention ‚Üí Transformers
2. GPT Implementation ‚Üí Text Generation
3. BPE ‚Üí Instruction Tuning
4. Model Evaluation ‚Üí Advanced Topics

## üé≤ Random Problem Generator

```python
# Quick script to get random problems by criteria
import random

easy = ["tokenization", "stop_words", "stemming", "bow", "similarity", "ngrams", "regex"]
medium = ["tfidf", "word2vec", "classification", "pos", "ner", "sentiment", "perceptron"]
hard = ["attention", "bert", "bpe", "gpt", "lstm", "lda", "cnn", "generation", "instruction"]

def get_problem(difficulty=None, time_limit=30):
    if difficulty == "easy" or time_limit <= 20:
        return random.choice(easy)
    elif difficulty == "medium" or time_limit <= 30:
        return random.choice(medium)
    elif difficulty == "hard" or time_limit > 30:
        return random.choice(hard)
    else:
        all_problems = easy + medium + hard
        return random.choice(all_problems)
```

## üì± Quick Access Links

- [Interview Quick Reference](INTERVIEW_QUICK_REFERENCE.md)
- [Phone Interview Plan](PHONE_INTERVIEW_STUDY_PLAN.md)
- [Memory Palace](NLP_MEMORY_PALACE.md)
- [Anki Decks](ANKI_SETUP.md)
