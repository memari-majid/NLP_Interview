# ðŸ§  Attention & Transformers Flashcards

## ðŸ“š Overview
Modern neural architectures powering current NLP. **5 essential cards** covering attention mechanisms and transformer architecture.

## ðŸ“Š Deck Contents
- **Self-attention** concept and formula
- **Attention formula** with scaling explanation
- **Attention vs RNNs** advantages
- **Why transformers work** so effectively
- **Transformer components** architecture overview

## ðŸš€ Import to Anki
1. **Copy this entire folder** to your computer
2. **Open Anki** â†’ `File` â†’ `CrowdAnki: Import from disk`
3. **Select this folder** (`NLP_AttentionTransformers`)
4. **Import** - deck will appear as "Attention & Transformers"

## ðŸ“± Study Settings
- **New cards**: 10-15 per day
- **Review time**: ~20 seconds per card
- **Total study**: 8-12 minutes daily

## ðŸŽ¯ Learning Focus
Master the attention mechanism and transformer architecture that powers BERT, GPT, and all modern LLMs. Critical for any current NLP interview.
