{
  "meta": {
    "title": "Comprehensive NLP Interview Knowledge Base",
    "version": "2.0",
    "sources": [
      "NLP Interview Problems",
      "Theory Flashcards",
      "NLP Cheat Sheet",
      "Practical Examples"
    ],
    "total_topics": 8,
    "total_patterns": 4
  },
  "topics": {
    "fundamentals": {
      "concepts": [
        "Tokenization",
        "Stemming",
        "Lemmatization",
        "Stop words",
        "N-grams"
      ],
      "implementations": [
        "tokenization",
        "stemming_lemmatization",
        "stop_word_removal",
        "ngrams"
      ],
      "interview_focus": "Basic preprocessing, understanding text representation"
    },
    "embeddings": {
      "concepts": [
        "Word2Vec",
        "GloVe",
        "FastText",
        "Contextualized embeddings"
      ],
      "implementations": [
        "word2vec",
        "embeddings"
      ],
      "interview_focus": "Static vs contextual, training methods, similarity"
    },
    "classical_ml": {
      "concepts": [
        "TF-IDF",
        "Bag of Words",
        "Naive Bayes",
        "SVM"
      ],
      "implementations": [
        "tfidf",
        "bow_vectors",
        "text_classification"
      ],
      "interview_focus": "Feature extraction, classification pipelines"
    },
    "deep_learning": {
      "concepts": [
        "RNN",
        "LSTM",
        "GRU",
        "CNN for text"
      ],
      "implementations": [
        "lstm_sentiment",
        "cnn_text_classification"
      ],
      "interview_focus": "Sequence modeling, gradient problems, architectures"
    },
    "transformers": {
      "concepts": [
        "Attention",
        "BERT",
        "GPT",
        "T5"
      ],
      "implementations": [
        "self_attention",
        "bert_sentiment",
        "gpt_block"
      ],
      "interview_focus": "Attention mechanism, pre-training, fine-tuning"
    },
    "applications": {
      "concepts": [
        "NER",
        "POS",
        "QA",
        "Summarization",
        "Translation"
      ],
      "implementations": [
        "ner",
        "pos_tagging",
        "sentiment_analysis"
      ],
      "interview_focus": "Task-specific approaches, evaluation metrics"
    },
    "llms": {
      "concepts": [
        "Prompting",
        "Few-shot",
        "RLHF",
        "Instruction tuning"
      ],
      "implementations": [
        "text_generation",
        "instruction_following",
        "fine_tuning"
      ],
      "interview_focus": "Scaling laws, emergent abilities, alignment"
    },
    "evaluation": {
      "concepts": [
        "Metrics",
        "Benchmarks",
        "Human evaluation"
      ],
      "implementations": [
        "model_evaluation"
      ],
      "interview_focus": "Choosing right metrics, interpreting results"
    }
  },
  "libraries": {
    "spacy": {
      "description": "Industrial-strength NLP with pre-trained models",
      "features": [
        "NER",
        "POS tagging",
        "Dependency parsing",
        "Tokenization"
      ],
      "install": "pip install spacy",
      "models": [
        "en_core_web_sm",
        "en_core_web_lg"
      ],
      "example": "import spacy\nnlp = spacy.load('en_core_web_sm')"
    },
    "nltk": {
      "description": "Natural Language Toolkit for education and research",
      "features": [
        "Corpora",
        "Tokenization",
        "Stemming",
        "Classification"
      ],
      "install": "pip install nltk",
      "download": "nltk.download('punkt')",
      "example": "import nltk\nfrom nltk.tokenize import word_tokenize"
    },
    "transformers": {
      "description": "State-of-the-art transformer models by HuggingFace",
      "features": [
        "BERT",
        "GPT",
        "T5",
        "Fine-tuning",
        "Pipelines"
      ],
      "install": "pip install transformers",
      "example": "from transformers import pipeline\nclassifier = pipeline('sentiment-analysis')"
    },
    "gensim": {
      "description": "Topic modeling and document similarity",
      "features": [
        "Word2Vec",
        "Doc2Vec",
        "LDA",
        "LSA",
        "FastText"
      ],
      "install": "pip install gensim",
      "example": "from gensim.models import Word2Vec, LdaModel"
    },
    "sentence-transformers": {
      "description": "Sentence embeddings for semantic similarity",
      "features": [
        "Semantic search",
        "Clustering",
        "Cross-encoders"
      ],
      "install": "pip install sentence-transformers",
      "example": "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')"
    },
    "flair": {
      "description": "Framework for state-of-the-art NLP",
      "features": [
        "NER",
        "POS",
        "Text classification",
        "Embeddings"
      ],
      "install": "pip install flair",
      "example": "from flair.models import SequenceTagger\ntagger = SequenceTagger.load('ner')"
    }
  },
  "models": {
    "bert": {
      "type": "Encoder",
      "use_cases": [
        "Classification",
        "NER",
        "QA",
        "Understanding"
      ],
      "variants": [
        "BERT-base",
        "BERT-large",
        "DistilBERT",
        "RoBERTa",
        "ALBERT"
      ],
      "pretraining": "MLM + NSP",
      "context": "Bidirectional"
    },
    "gpt": {
      "type": "Decoder",
      "use_cases": [
        "Generation",
        "Completion",
        "Few-shot learning"
      ],
      "variants": [
        "GPT-2",
        "GPT-3",
        "GPT-Neo",
        "GPT-J"
      ],
      "pretraining": "Next token prediction",
      "context": "Autoregressive (left-to-right)"
    },
    "t5": {
      "type": "Encoder-Decoder",
      "use_cases": [
        "Translation",
        "Summarization",
        "QA",
        "Any text-to-text"
      ],
      "variants": [
        "T5-small",
        "T5-base",
        "T5-large",
        "Flan-T5"
      ],
      "pretraining": "Span corruption",
      "context": "Encoder: bidirectional, Decoder: autoregressive"
    },
    "word2vec": {
      "type": "Static embeddings",
      "architectures": [
        "CBOW",
        "Skip-gram"
      ],
      "training": "Negative sampling or hierarchical softmax",
      "dimensions": "50-300 typically"
    },
    "glove": {
      "type": "Static embeddings",
      "method": "Global matrix factorization + local context",
      "advantages": "Captures global statistics"
    }
  },
  "datasets": [
    {
      "name": "SQuAD",
      "task": "Question Answering",
      "size": "100K+ questions",
      "format": "Context-question-answer triples"
    },
    {
      "name": "GLUE",
      "task": "Multiple NLP tasks benchmark",
      "subtasks": [
        "Sentiment",
        "Similarity",
        "Entailment"
      ],
      "evaluation": "Average score across tasks"
    },
    {
      "name": "CoNLL-2003",
      "task": "Named Entity Recognition",
      "entities": [
        "PER",
        "LOC",
        "ORG",
        "MISC"
      ],
      "languages": [
        "English",
        "German"
      ]
    },
    {
      "name": "IMDB Reviews",
      "task": "Sentiment Analysis",
      "size": "50K reviews",
      "classes": [
        "Positive",
        "Negative"
      ]
    },
    {
      "name": "AG News",
      "task": "Text Classification",
      "size": "120K articles",
      "classes": [
        "World",
        "Sports",
        "Business",
        "Sci/Tech"
      ]
    },
    {
      "name": "WikiText",
      "task": "Language Modeling",
      "versions": [
        "WikiText-2",
        "WikiText-103"
      ],
      "metric": "Perplexity"
    }
  ],
  "formulas": {
    "tfidf": {
      "formula": "TF-IDF(t,d) = TF(t,d) \u00d7 log(N/DF(t))",
      "explanation": "Term frequency \u00d7 Inverse document frequency",
      "components": {
        "TF": "Term frequency in document",
        "N": "Total number of documents",
        "DF": "Document frequency (docs containing term)"
      }
    },
    "cosine_similarity": {
      "formula": "cos(\u03b8) = (A\u00b7B) / (||A|| \u00d7 ||B||)",
      "explanation": "Dot product divided by product of magnitudes",
      "range": "[-1, 1] where 1 = identical direction"
    },
    "attention": {
      "formula": "Attention(Q,K,V) = softmax(QK^T/\u221ad_k)V",
      "explanation": "Scaled dot-product attention",
      "components": {
        "Q": "Query matrix",
        "K": "Key matrix",
        "V": "Value matrix",
        "d_k": "Dimension of keys"
      }
    },
    "perplexity": {
      "formula": "PPL = exp(-1/N \u03a3 log P(w_i|context))",
      "explanation": "Exponentiated average negative log-likelihood",
      "interpretation": "Lower is better"
    },
    "f1_score": {
      "formula": "F1 = 2 \u00d7 (precision \u00d7 recall) / (precision + recall)",
      "explanation": "Harmonic mean of precision and recall",
      "components": {
        "precision": "TP / (TP + FP)",
        "recall": "TP / (TP + FN)"
      }
    },
    "bleu": {
      "formula": "BLEU = BP \u00d7 exp(\u03a3 w_n log p_n)",
      "explanation": "Brevity penalty \u00d7 weighted n-gram precision",
      "components": {
        "BP": "Brevity penalty",
        "p_n": "n-gram precision",
        "w_n": "weights (usually uniform)"
      }
    }
  },
  "metrics": {
    "classification": {
      "accuracy": "Correct predictions / Total predictions",
      "precision": "True Positives / (True Positives + False Positives)",
      "recall": "True Positives / (True Positives + False Negatives)",
      "f1": "Harmonic mean of precision and recall",
      "auc_roc": "Area under ROC curve"
    },
    "generation": {
      "bleu": "N-gram overlap with reference",
      "rouge": "Recall-oriented for summarization",
      "meteor": "Considers synonyms and paraphrases",
      "bertscore": "Semantic similarity using BERT"
    },
    "language_modeling": {
      "perplexity": "How well model predicts next token",
      "cross_entropy": "Average negative log probability"
    },
    "similarity": {
      "cosine": "Angle between vectors",
      "euclidean": "Straight-line distance",
      "jaccard": "Intersection over union",
      "levenshtein": "Edit distance"
    }
  },
  "interview_patterns": {
    "preprocessing_pipeline": {
      "pattern": "Design a text preprocessing pipeline",
      "solution": "\ndef preprocess_pipeline(text):\n    # 1. Lowercase\n    text = text.lower()\n    # 2. Remove special chars\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    # 3. Tokenize\n    tokens = word_tokenize(text)\n    # 4. Remove stopwords\n    tokens = [t for t in tokens if t not in stop_words]\n    # 5. Lemmatize\n    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n    return tokens\n",
      "variations": [
        "Add spell correction",
        "Handle URLs/emails",
        "Preserve entities"
      ]
    },
    "similarity_search": {
      "pattern": "Implement semantic search",
      "solution": "\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n# Index documents\ndoc_embeddings = model.encode(documents)\n# Search\nquery_embedding = model.encode([query])\nsimilarities = cosine_similarity(query_embedding, doc_embeddings)[0]\ntop_k = similarities.argsort()[-k:][::-1]\n",
      "variations": [
        "Use FAISS for scale",
        "Implement reranking",
        "Add filters"
      ]
    },
    "classification_pipeline": {
      "pattern": "Build text classifier",
      "solution": "\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=10000)),\n    ('clf', LinearSVC())\n])\npipeline.fit(X_train, y_train)\n",
      "variations": [
        "Use BERT",
        "Add cross-validation",
        "Handle imbalanced data"
      ]
    },
    "ner_extraction": {
      "pattern": "Extract named entities",
      "solution": "\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef extract_entities(text):\n    doc = nlp(text)\n    entities = [(ent.text, ent.label_) for ent in doc.ents]\n    return entities\n",
      "variations": [
        "Custom NER with BERT",
        "Rule-based for domain",
        "Combine approaches"
      ]
    }
  },
  "code_snippets": [
    {
      "id": "snippet_0",
      "code": "pip install -U sentence-transformers",
      "category": "transformers"
    },
    {
      "id": "snippet_1",
      "code": "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Sentences to encode\nsentences = [\n  \"This framework generates embeddings for each input sentence.\",\n  \"Sentences are passed as a list of string.\",\n  \"The quick brown fox jumps over the lazy dog.\"\n]\n\nembeddings = model.encode(sentences)\n\nfor sentence, embedding in zip(sentences, embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding[:15])\n    print(\"Embedding dimension\", embedding.shape)\n    print(\"\")",
      "category": "embeddings"
    },
    {
      "id": "snippet_2",
      "code": "from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\n# download from https://nlp.stanford.edu/projects/glove\nf = open('./glove.6B.100d.txt')\n\nembeddings_index = {}\n\nfor line in f:\n  values = line.split()\n  word = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\n\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\nembedding_dim = 100\nmaxlen = 100\ntraining_samples = 200\nvalidation_samples = 10000\nmax_words = 10000\n\nembedding_matrix = np.zeros((max_words, embedding_dim))\n\nfor word, i in word_index.items():\n  if i < max_words:\n    embedding_vector = embeddings_index.get(word)\n      if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n# option A)\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\n# option B) LSTM\n#model.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))\n\nmodel.save_weights('pre_trained_glove_model.h5')",
      "category": "embeddings"
    },
    {
      "id": "snippet_3",
      "code": "import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\n\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nmodel = hub.load(module_url)\n\ndef embed(input):\n  return model(input)\n\nword = \"Elephant\"\nsentence = \"I am a sentence for which I would like to get its embedding.\"\nparagraph = (\n    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n    \"the more 'diluted' the embedding will be.\")\n\nmessages = [word, sentence, paragraph]\n\nmessage_embeddings = embed(messages)\n\nfor i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n  print(\"Message: {}\".format(messages[i]))\n  print(\"Embedding size: {}\".format(len(message_embedding)))\n  message_embedding_snippet = \", \".join(\n      (str(x) for x in message_embedding[:3]))\n  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))",
      "category": "embeddings"
    },
    {
      "id": "snippet_4",
      "code": "import gensim\nword2vev_model = gensim.models.word2vec.Word2Vec(sentence_list)",
      "category": "embeddings"
    },
    {
      "id": "snippet_5",
      "code": "from sklearn.feature_extraction.text import TfidfVectorizer\n\ndocument_corpus = [\n  \"Dog bites man\", \n  \"Man bites dog\", \n  \"Dog eats meat\", \n  \"Man eats food\"\n]\n\ntfidf = TfidfVectorizer()\nbow_rep_tfidf = tfidf.fit_transform(document_corpus)\n\nprint(\"IDF for all words in the vocabulary\")\nprint(tfidf.idf_)\nprint(\"\\nAll words in the vocabulary.\")\nprint(tfidf.get_feature_names_out())\n\ntemp = tfidf.transform([\"Dog bites man\"])\n\nprint(\"\\nTF-IDF representation for 'Dog bites man':\\n\", temp.toarray())",
      "category": "tfidf"
    },
    {
      "id": "snippet_6",
      "code": "from sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\ndocument_corpus = [\n  \"Dog bites man\", \n  \"Man bites dog\", \n  \"Dog eats meat\", \n  \"Man eats food\"\n]\n\n# Write a function for cleaning strings and returning an array of ngrams\ndef ngrams_analyzer(string):\n    string = re.sub(r'[,-./]', r'', string)\n    ngrams = zip(*[string[i:] for i in range(5)])  # N-Gram length is 5\n    return [''.join(ngram) for ngram in ngrams]\n\n# Construct your vectorizer for building the TF-IDF matrix\ntfidf = TfidfVectorizer(analyzer=ngrams_analyzer)\n\nbow_rep_tfidf = tfidf.fit_transform(document_corpus)\n\nprint(\"IDF for all words in the vocabulary\")\nprint(tfidf.idf_)\nprint(\"\\nAll words in the vocabulary.\")\nprint(tfidf.get_feature_names_out())\n\ntemp = tfidf.transform([\"Dog bites man\"])\n\nprint(\"\\nTF-IDF representation for 'Dog bites man':\\n\", temp.toarray())\n\n# Credits: https://towardsdatascience.com/group-thousands-of-similar-spreadsheet-text-cells-in-seconds-2493b3ce6d8d",
      "category": "embeddings"
    },
    {
      "id": "snippet_7",
      "code": "# John likes to watch movies. Mary likes movies too.\nBoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};",
      "category": "general"
    },
    {
      "id": "snippet_8",
      "code": "import spacy",
      "category": "general"
    },
    {
      "id": "snippet_9",
      "code": "# Import dataset\nnlp = spacy.load(\"en\")\n# Import large dataset. Needs to be downloaded first.\n# nlp = spacy.load(\"en_core_web_lg\")",
      "category": "general"
    },
    {
      "id": "snippet_10",
      "code": "# spacy: Removing stop words\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\nprint('spacy: Number of stop words: %d' % len(spacy_stopwords))",
      "category": "preprocessing"
    },
    {
      "id": "snippet_11",
      "code": "# nltk: Removing stop words \nfrom nltk.corpus import stopwords\nenglish_stop_words = stopwords.words('english')\n\nprint('ntlk: Number of stop words: %d' % len(english_stop_words))",
      "category": "preprocessing"
    },
    {
      "id": "snippet_12",
      "code": "text = 'Larry Page founded Google in early 1990.'\ndoc = nlp(text)\ntokens = [token.text for token in doc if not token.is_stop]\nprint('Original text: %s' % (text))\nprint()\nprint(tokens)",
      "category": "preprocessing"
    },
    {
      "id": "snippet_13",
      "code": "doc = nlp(\"Larry Page founded Google in early 1990.\")\nspan = doc[2:4]\nspan.text",
      "category": "general"
    },
    {
      "id": "snippet_14",
      "code": "[(spans) for spans in doc]",
      "category": "general"
    },
    {
      "id": "snippet_15",
      "code": "doc = nlp(\"Larry Page founded Google in early 1990.\")\n[token.text for token in doc]",
      "category": "general"
    },
    {
      "id": "snippet_16",
      "code": "# Load OpenAI GPT-2 using PyTorch Transformers\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\n# https://huggingface.co/pytorch-transformers/serialization.html",
      "category": "tokenization"
    },
    {
      "id": "snippet_17",
      "code": "text = \"Asian shares skidded on Tuesday after a rout in tech stocks put Wall Street to the sword\"\ndoc = nlp(text)\n[(x.orth_, x.pos_, spacy.explain(x.pos_)) for x in [token for token in doc]]",
      "category": "pos"
    },
    {
      "id": "snippet_18",
      "code": "[(x.orth_, x.tag_, spacy.explain(x.tag_)) for x in [token for token in doc]]",
      "category": "pos"
    },
    {
      "id": "snippet_19",
      "code": "# using nltk\nimport nltk\n\ntokens = nltk.word_tokenize(text)\npos_tags = nltk.pos_tag(tokens)\npos_tags",
      "category": "tokenization"
    },
    {
      "id": "snippet_20",
      "code": "[(token, token.ent_iob_, token.ent_type_) for token in doc]",
      "category": "general"
    },
    {
      "id": "snippet_21",
      "code": "import nltk\nfrom nltk.stem.porter import *\n\nstemmer = PorterStemmer()\ntokens = ['compute', 'computer', 'computed', 'computing']\nfor token in tokens:\n    print(token + ' --> ' + stemmer.stem(token))",
      "category": "preprocessing"
    },
    {
      "id": "snippet_22",
      "code": "doc = nlp(\"Was Google founded in early 1990?\")\n[(x.orth_, x.lemma_) for x in [token for token in doc]]",
      "category": "preprocessing"
    },
    {
      "id": "snippet_23",
      "code": "doc = nlp(\"Larry Page founded Google in early 1990. Sergey Brin joined.\")\n[sent.text for sent in doc.sents]",
      "category": "ner"
    },
    {
      "id": "snippet_24",
      "code": "doc = nlp(\"We are reading a text.\")\n# Dependency labels\n[(x.orth_, x.dep_, spacy.explain(x.dep_)) for x in [token for token in doc]]",
      "category": "general"
    },
    {
      "id": "snippet_25",
      "code": "# Syntactic head token (governor)\n[token.head.text for token in doc]",
      "category": "general"
    },
    {
      "id": "snippet_26",
      "code": "doc = nlp(\"I have a red car\")\n[chunk.text for chunk in doc.noun_chunks]",
      "category": "general"
    },
    {
      "id": "snippet_27",
      "code": "doc = nlp(\"Larry Page founded Google in the US in early 1990.\")\n# Text and label of named entity span\n[(ent.text, ent.label_) for ent in doc.ents]",
      "category": "ner"
    },
    {
      "id": "snippet_28",
      "code": "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n[(X.text, X.label_) for X in doc.ents]",
      "category": "ner"
    },
    {
      "id": "snippet_29",
      "code": "from collections import Counter\n\nlabels = [x.label_ for x in doc.ents]\nCounter(labels)",
      "category": "ner"
    }
  ]
}