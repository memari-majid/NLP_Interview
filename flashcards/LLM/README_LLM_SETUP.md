# LLM Flashcard Setup Guide

## ğŸ¯ **Ready for ML:LLM:<chapters> Creation**

This directory contains the complete infrastructure for creating Large Language Model (LLM) flashcards with hierarchical organization in Anki.

## ğŸ“‚ **Directory Structure**

```
flashcards/LLM/
â”œâ”€â”€ _TEMPLATE.json              # Complete CrowdAnki template for LLM cards
â”œâ”€â”€ 01_llm_foundations/         # What are LLMs, history, basic concepts
â”œâ”€â”€ 02_transformer_architecture/# Architecture details, encoder-decoder
â”œâ”€â”€ 03_attention_mechanisms/    # Self-attention, multi-head, positional encoding
â”œâ”€â”€ 04_training_scaling/        # Pre-training, scaling laws, compute requirements
â”œâ”€â”€ 05_fine_tuning_adaptation/  # SFT, RLHF, LoRA, adapters
â”œâ”€â”€ 06_prompting_techniques/    # Few-shot, chain-of-thought, prompt engineering
â”œâ”€â”€ 07_evaluation_benchmarks/   # BLEU, ROUGE, human eval, safety benchmarks
â”œâ”€â”€ 08_deployment_inference/    # Model serving, optimization, quantization
â”œâ”€â”€ 09_safety_alignment/        # Constitutional AI, red teaming, bias mitigation
â””â”€â”€ 10_multimodal_llms/         # Vision-language, multimodal capabilities
```

## ğŸ—ï¸ **Anki Hierarchy**

When imported, these flashcards will create:

```
ğŸ“‚ ML
  â””â”€â”€ ğŸ“‚ LLM  
      â”œâ”€â”€ ğŸ“š 01 LLM Foundations
      â”œâ”€â”€ ğŸ“š 02 Transformer Architecture
      â”œâ”€â”€ ğŸ“š 03 Attention Mechanisms  
      â”œâ”€â”€ ğŸ“š 04 Training & Scaling
      â”œâ”€â”€ ğŸ“š 05 Fine-tuning & Adaptation
      â”œâ”€â”€ ğŸ“š 06 Prompting Techniques
      â”œâ”€â”€ ğŸ“š 07 Evaluation & Benchmarks
      â”œâ”€â”€ ğŸ“š 08 Deployment & Inference
      â”œâ”€â”€ ğŸ“š 09 Safety & Alignment
      â””â”€â”€ ğŸ“š 10 Multimodal LLMs
```

## ğŸš€ **Workflow**

### **1. Generate Content**
Use `Custom Instructions.md` with your AI assistant:
- Provide LLM chapter, paper, or documentation
- Specify: "Generate flashcards for ML:LLM:XX [Chapter Name]"
- Get complete CrowdAnki JSON output

### **2. Save & Convert**
```bash
# Save JSON file in appropriate directory
# Example: flashcards/LLM/01_llm_foundations/01_llm_foundations.json

# Convert to APKG
python convert_json_to_apkg.py

# Result: APKG_Output/01-llm-foundations.apkg
```

### **3. Import & Study**
- Double-click any `.apkg` file
- Anki imports with perfect ML:LLM hierarchy
- Study with spaced repetition

## ğŸ“‹ **Content Guidelines**

### **Question Types**
- **Foundations**: "What is...", "How does..."
- **Technical**: "Compare X vs Y", "When to use..."  
- **Practical**: "How would you...", "What are the trade-offs..."

### **Answer Structure** (6 sections required)
1. **Concept**: Core definition
2. **Intuition**: Why it works
3. **Mechanics**: How it works  
4. **Trade-offs**: Limitations
5. **Applications**: Real uses
6. **Memory Hook**: Memorable phrase

### **Difficulty Distribution**
- **20% Easy**: Basic definitions and concepts
- **60% Medium**: Core mechanics and applications
- **20% Hard**: Advanced topics and edge cases

## ğŸ“š **Recommended Sources**

### **Foundational Papers**
- "Attention Is All You Need" (Transformers)
- "Language Models are Few-Shot Learners" (GPT-3)
- "Training language models to follow instructions with human feedback" (InstructGPT)
- "Constitutional AI" (Claude/Anthropic)

### **Survey Papers**
- "A Survey of Large Language Models"
- "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods"
- "Multimodal Machine Learning: A Survey and Taxonomy"

### **Books & Courses**
- "Speech and Language Processing" by Jurafsky & Martin
- Stanford CS224N: Natural Language Processing with Deep Learning
- Hugging Face Transformers documentation

## ğŸ¯ **Ready to Start**

Everything is set up for you to begin creating comprehensive LLM flashcards. Just pick a topic, use the updated Custom Instructions, and start generating!
