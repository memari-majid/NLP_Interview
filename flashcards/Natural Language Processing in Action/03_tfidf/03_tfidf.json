{
  "crowdanki_uuid": "deck-8186",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0
    }
  ],
  "desc": "Comprehensive flashcards for 3 Math with words: Term frequency–inverse document frequency vectors",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "3 Math with words: Term frequency–inverse document frequency vectors",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n\n.front {\n font-weight: bold;\n color: #2c3e50;\n}\n\n.back {\n text-align: left;\n padding: 20px;\n}\n\n.concept {\n font-weight: bold;\n color: #e74c3c;\n margin-bottom: 10px;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 10px;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 10px;\n}\n\n.tradeoffs {\n color: #f39c12;\n margin-bottom: 10px;\n}\n\n.applications {\n color: #9b59b6;\n margin-bottom: 10px;\n}\n\n.memory-hook {\n background-color: #ecf0f1;\n padding: 10px;\n border-left: 4px solid #34495e;\n font-style: italic;\n color: #34495e;\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>"
        }
      ],
      "type": 0
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note--2645794198076619719-8186",
      "fields": [
        "What is a bag-of-words (BOW) vector?",
        "<div class=\"concept\"><strong>Concept:</strong> A bag-of-words vector is a numerical representation of text where each dimension corresponds to a unique word in the vocabulary, and the value is the count or frequency of that word in the document.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> It treats text as an unordered collection (bag) of words, ignoring grammar and order, focusing on word occurrences to capture basic content.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Tokenize the text, build a vocabulary of unique words, then create a vector where each position holds the count of the corresponding word.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Simple and efficient but loses word order and context, leading to potential loss of meaning.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Used in document classification, spam filtering, and basic search engines.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Imagine dumping all words from a document into a bag and counting how many of each type you pull out.</div>",
        "Bag-of-Words Vector Representation Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--2645794198076619719-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Bag-of-Words",
        "Vector Representation",
        "Easy"
      ]
    },
    {
      "crowdanki_uuid": "note-6182551501977049345-8186",
      "fields": [
        "Explain the intuition behind term frequency (TF).",
        "<div class=\"concept\"><strong>Concept:</strong> Term frequency is the number of times a word appears in a document, often normalized by the total number of words in the document.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Words that appear more frequently in a document are likely more important to its meaning, like 'bias' in an article about algorithmic bias.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> TF(t, d) = count of t in d / total words in d.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Overemphasizes common words like 'the' or 'and', which may not carry much meaning.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Basic feature in text analysis, combined with IDF for better relevance.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Think of TF as a spotlight that brightens words repeated often in a story.</div>",
        "Term Frequency Intuition Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-6182551501977049345-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Term Frequency",
        "Intuition",
        "Easy"
      ]
    },
    {
      "crowdanki_uuid": "note-929929532141739201-8186",
      "fields": [
        "How do you compute inverse document frequency (IDF)?",
        "<div class=\"concept\"><strong>Concept:</strong> IDF measures how rare a word is across the corpus, giving higher weight to unique words.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Common words across documents add little unique information; rare ones distinguish documents.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> IDF(t) = log(total documents / documents containing t), often with smoothing like adding 1 to avoid division by zero.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Sensitive to corpus size; very rare words might be overemphasized if noisy.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Weights terms in search engines to prioritize distinctive keywords.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> IDF is like a rarity score in a game—common items are cheap, rare ones valuable.</div>",
        "Inverse Document Frequency Math Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-929929532141739201-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Inverse Document Frequency",
        "Math",
        "Medium"
      ]
    },
    {
      "crowdanki_uuid": "note--8134078055732042294-8186",
      "fields": [
        "What is TF-IDF and how is it calculated?",
        "<div class=\"concept\"><strong>Concept:</strong> TF-IDF is a score reflecting a word's importance in a document relative to the corpus.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Balances how often a word appears in one document against its commonality elsewhere.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> TF-IDF(t, d) = TF(t, d) * IDF(t), often using log for IDF to follow Zipf's law.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Ignores word order and semantics; performs poorly on short texts or small corpora.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Document similarity, keyword extraction, search ranking.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> TF-IDF is a word's 'VIP pass'—frequent locally but rare globally gets high access.</div>",
        "TF-IDF Theory Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--8134078055732042294-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Theory",
        "Medium"
      ]
    },
    {
      "crowdanki_uuid": "note-5965998036540456521-8186",
      "fields": [
        "Compare Euclidean distance and cosine similarity for text vectors.",
        "<div class=\"concept\"><strong>Concept:</strong> Euclidean distance measures straight-line distance between points; cosine similarity measures angle between vectors.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Euclidean cares about magnitude (length); cosine focuses on direction (overlap in content).</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Euclidean: sqrt(sum((x_i - y_i)^2)); Cosine: dot(x,y) / (norm(x) * norm(y)).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Euclidean suffers in high dimensions (curse of dimensionality); cosine is robust for varying document lengths.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Cosine for text similarity in search; Euclidean less common for sparse text vectors.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Euclidean is walking distance on a map; cosine is how aligned two arrows point.</div>",
        "Vector Similarity Comparisons Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-5965998036540456521-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vector Similarity",
        "Comparisons",
        "Hard"
      ]
    },
    {
      "crowdanki_uuid": "note-6419516913633319396-8186",
      "fields": [
        "What is Zipf's law and its implications for NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> Zipf's law states word frequency is inversely proportional to its rank in the frequency table.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> A few words are very common; most are rare, creating a power-law distribution.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Frequency of rank k word ≈ 1/k * frequency of rank 1 word; plot log(freq) vs log(rank) is linear.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Explains why stop words dominate; requires logging in TF-IDF to balance frequencies.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Predicts word distributions; justifies stop word removal and log scaling in models.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like city populations: New York (the) is huge, smaller ones drop off quickly.</div>",
        "Zipf's Law Theory Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-6419516913633319396-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Zipf's Law",
        "Theory",
        "Medium"
      ]
    },
    {
      "crowdanki_uuid": "note-2144943578505276795-8186",
      "fields": [
        "How do n-grams improve upon basic bag-of-words?",
        "<div class=\"concept\"><strong>Concept:</strong> N-grams are sequences of n consecutive tokens, capturing local context.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> BOW ignores order; n-grams preserve some phrasing, like 'New York' as a bigram.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> For n=2, split text into overlapping pairs; count frequencies like words.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Increases dimensionality exponentially; sparse for higher n.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Language modeling, spelling correction, authorship attribution.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> N-grams are word buddies—alone they're lost, together they tell a mini-story.</div>",
        "N-grams Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-2144943578505276795-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "N-grams",
        "Application",
        "Medium"
      ]
    },
    {
      "crowdanki_uuid": "note-1644131795236545922-8186",
      "fields": [
        "Describe the mechanics of using scikit-learn's CountVectorizer.",
        "<div class=\"concept\"><strong>Concept:</strong> CountVectorizer converts text to a matrix of token counts.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Automates building BOW vectors from a corpus.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Initialize with params like ngram_range; fit() builds vocab; transform() creates sparse matrix of counts.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Defaults to word tokens; customizable but may need preprocessing for punctuation.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Feature extraction for ML models on text data.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> It's a word counter robot: feeds on text, spits out numbered bags.</div>",
        "CountVectorizer Mechanics Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-1644131795236545922-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "CountVectorizer",
        "Mechanics",
        "Medium"
      ]
    },
    {
      "crowdanki_uuid": "note-6122957643168558787-8186",
      "fields": [
        "What are the trade-offs of using TF-IDF in a very small corpus?",
        "<div class=\"concept\"><strong>Concept:</strong> TF-IDF relies on document frequencies across a corpus.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> In small corpora, rare words might be overvalued due to limited samples.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> IDF becomes unstable; log( small N / df ) can exaggerate uniqueness.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Poor generalization; overfitting to noise; better for large, diverse corpora.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Still useful for tiny FAQ bots, but add smoothing.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Small pond, big fish—rare words seem too important without enough water (data).</div>",
        "TF-IDF Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-6122957643168558787-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Trade-offs",
        "Hard"
      ]
    },
    {
      "crowdanki_uuid": "note-9048109414618986474-8186",
      "fields": [
        "How does cosine similarity connect to dot products in vector spaces?",
        "<div class=\"concept\"><strong>Concept:</strong> Cosine similarity is the normalized dot product of two vectors.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Measures how much vectors 'agree' in direction, ignoring size.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Cos_sim = dot(A,B) / (||A|| * ||B||), where || || is L2 norm.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Efficient for sparse vectors; but assumes Euclidean space.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Document matching, recommendation systems.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Dot product is raw overlap; cosine adjusts for vector 'volume'.</div>",
        "Cosine Similarity Math Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-9048109414618986474-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Cosine Similarity",
        "Math",
        "Hard"
      ]
    },
    {
      "crowdanki_uuid": "note-793104567053585018-8186",
      "fields": [
        "Explain an application of character n-grams.",
        "<div class=\"concept\"><strong>Concept:</strong> Character n-grams break text into sequences of characters.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Captures subword patterns, robust to spelling variations.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> For n=3, 'hello' -> 'hel', 'ell', 'llo'; count frequencies.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Higher dimensionality than words; but handles OOV better.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Language detection, typo-tolerant search, authorship analysis.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like DNA sequencing—short snippets reveal the organism (language).</div>",
        "N-grams Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-793104567053585018-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "N-grams",
        "Application",
        "Medium"
      ]
    },
    {
      "crowdanki_uuid": "note--1706555586041496425-8186",
      "fields": [
        "What is the curse of dimensionality in text vectors?",
        "<div class=\"concept\"><strong>Concept:</strong> High-dimensional spaces make distances less meaningful.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> As dimensions grow, points spread out, similarities dilute.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> In high-D, Euclidean distances concentrate; cosine helps but not fully.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Sparsity increases computation; requires dimensionality reduction.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Affects large vocab TF-IDF; mitigated in embeddings.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Crowded party in a huge hall—everyone feels far apart.</div>",
        "Vector Spaces Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--1706555586041496425-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vector Spaces",
        "Trade-offs",
        "Hard"
      ]
    },
    {
      "crowdanki_uuid": "note--3410993029050119354-8186",
      "fields": [
        "How can TF-IDF be used to build a simple FAQ bot?",
        "<div class=\"concept\"><strong>Concept:</strong> Vectorize questions, match user query to closest FAQ via similarity.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Treat query as document, find most relevant stored question, return its answer.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Fit TfidfVectorizer on FAQs; transform query; compute cosine sim; pick max.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Literal matching; fails on synonyms/typos without enhancements.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Customer support bots, knowledge bases.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> FAQ bot as a librarian: matches your question to the closest book title.</div>",
        "TF-IDF Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--3410993029050119354-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Application",
        "Medium"
      ]
    },
    {
      "crowdanki_uuid": "note--465106690625677162-8186",
      "fields": [
        "Compare TF-IDF with Okapi BM25 for ranking.",
        "<div class=\"concept\"><strong>Concept:</strong> BM25 is a probabilistic ranking function improving on TF-IDF.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> BM25 saturates TF growth and adjusts for document length nonlinearly.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> BM25 adds parameters for TF saturation and length normalization.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> BM25 better for relevance in search; more params to tune.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Modern search engines prefer BM25 over plain TF-IDF.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> TF-IDF is basic scale; BM25 adds smart dampers for balance.</div>",
        "TF-IDF Comparisons Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--465106690625677162-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Comparisons",
        "Hard"
      ]
    },
    {
      "crowdanki_uuid": "note-6107642476117540781-8186",
      "fields": [
        "What connections exist between Zipf's law and IDF?",
        "<div class=\"concept\"><strong>Concept:</strong> Both deal with frequency distributions in language.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Zipf explains why common words have low info; IDF logs to compress scale.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> IDF uses log to linearize Zipf's power law in frequencies.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Without log, rare words dominate; with it, balances importance.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Justifies log in TF-IDF for uniform weighting.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Zipf's steep hill; log IDF flattens it for fair climbing.</div>",
        "Zipf's Law Connections Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-6107642476117540781-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Zipf's Law",
        "Connections",
        "Hard"
      ]
    },
    {
      "crowdanki_uuid": "note--7279676607282247588-8186",
      "fields": [
        "Describe an edge case where TF-IDF fails.",
        "<div class=\"concept\"><strong>Concept:</strong> TF-IDF assumes independence and frequency-based importance.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Synonyms or context changes meaning without frequency shift.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Two docs with same words different orders (e.g., 'man bites dog' vs 'dog bites man').</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> No semantic understanding; needs embeddings for better handling.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Poor for sentiment with negation or irony.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> TF-IDF blind to word puzzles—same pieces, different picture.</div>",
        "TF-IDF Edge Cases Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--7279676607282247588-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Edge Cases",
        "Hard"
      ]
    }
  ]
}