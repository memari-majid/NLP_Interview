{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "ml-optimization--training-2024",
  "deck_config_uuid": "ml-interview-config",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "ml-interview-config",
      "dyn": false,
      "name": "Optimization & Training",
      "new": {
        "bury": true,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 20,
        "separate": true
      },
      "rev": {
        "bury": true,
        "ease4": 1.3,
        "fuzz": 0.05,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "minSpace": 1,
        "perDay": 150
      }
    }
  ],
  "desc": "Gradient descent, optimizers, and training techniques",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "Optimization & Training",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "ml-interview-model",
      "css": "\n.card {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-size: 16px;\n    line-height: 1.5;\n    color: #333;\n    background-color: #fafafa;\n    padding: 20px;\n    max-width: 600px;\n    margin: 0 auto;\n}\n\n.front { \n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    color: white;\n    padding: 25px;\n    border-radius: 10px;\n}\n\n.back { \n    background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n    color: white;\n    padding: 25px;\n    border-radius: 10px;\n}\n\nb { color: #ffeb3b; font-weight: 600; }\n.formula { \n    font-family: 'Courier New', monospace; \n    background: rgba(255,255,255,0.2); \n    padding: 3px 6px; \n    border-radius: 4px;\n    font-size: 14px;\n}\n.example { font-style: italic; opacity: 0.9; }\n                    ",
      "flds": [
        {
          "__type__": "NoteModelField",
          "name": "Front",
          "ord": 0,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Back",
          "ord": 1,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Topic",
          "ord": 2,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Type",
          "ord": 3,
          "sticky": false
        }
      ],
      "name": "ML Interview Card",
      "req": [
        [
          0,
          "any",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "__type__": "CardTemplate",
          "afmt": "<div class=\"card back\">{{Front}}<hr>{{Back}}<br><br><small><i>{{Topic}} • {{Type}}</i></small></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card",
          "ord": 0,
          "qfmt": "<div class=\"card front\">{{Front}}</div>"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "What is the difference between batch, mini-batch, and stochastic gradient descent?",
        "<b>Batch GD:</b> Use entire dataset for each update - stable but slow<br>\n<b>Stochastic GD:</b> Use one sample - noisy but fast convergence<br>\n<b>Mini-batch GD:</b> Use small batches - balance of stability and speed<br>\n<b>Typical batch sizes:</b> 32, 64, 128, 256 depending on memory and data size",
        "Optimization & Training",
        "interview_concept"
      ],
      "guid": "ml_b9a3e1e4",
      "note_model_uuid": "ml-interview-model",
      "tags": [
        "optimization_&_training",
        "interview_concept",
        "ml_interview"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "How do Adam, RMSprop, and momentum optimizers work?",
        "<b>Momentum:</b> Accumulates gradients for smoother updates<br>\n<b>RMSprop:</b> Adapts learning rate per parameter using squared gradient history<br>\n<b>Adam:</b> Combines momentum + RMSprop with bias correction<br>\n<b>Usage:</b> Adam is default choice, RMSprop for RNNs, momentum for simple cases",
        "Optimization & Training",
        "interview_concept"
      ],
      "guid": "ml_3f10470b",
      "note_model_uuid": "ml-interview-model",
      "tags": [
        "optimization_&_training",
        "interview_concept",
        "ml_interview"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is batch normalization and why is it effective?",
        "<b>Method:</b> Normalize inputs to each layer (mean=0, std=1)<br>\n<b>Benefits:</b> Faster training, less sensitive to initialization, acts as regularization<br>\n<b>Training:</b> Use batch statistics; <b>Inference:</b> Use running averages<br>\n<b>Formula:</b> y = γ × (x-μ)/σ + β (γ,β are learnable parameters)",
        "Optimization & Training",
        "interview_concept"
      ],
      "guid": "ml_014900b1",
      "note_model_uuid": "ml-interview-model",
      "tags": [
        "optimization_&_training",
        "interview_concept",
        "ml_interview"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is learning rate scheduling and when would you use it?",
        "<b>Purpose:</b> Adjust learning rate during training for better convergence<br>\n<b>Step decay:</b> Reduce by factor every few epochs<br>\n<b>Exponential decay:</b> Gradual continuous reduction<br>\n<b>Cosine annealing:</b> Follows cosine curve for smooth reduction",
        "Optimization & Training",
        "interview_concept"
      ],
      "guid": "ml_c74dc16c",
      "note_model_uuid": "ml-interview-model",
      "tags": [
        "optimization_&_training",
        "interview_concept",
        "ml_interview"
      ]
    }
  ]
}