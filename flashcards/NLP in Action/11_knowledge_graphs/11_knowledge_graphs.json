{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-11-info-extraction-kg",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0
    }
  ],
  "desc": "Comprehensive flashcards for 11 Knowledge Graphs",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:11 Knowledge Graphs",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "ml-nlp-interview-model",
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n\n.front {\n font-weight: bold;\n color: #2c3e50;\n}\n\n.back {\n text-align: left;\n padding: 20px;\n}\n\n.concept {\n font-weight: bold;\n color: #e74c3c;\n margin-bottom: 10px;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 10px;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 10px;\n}\n\n.tradeoffs {\n color: #f39c12;\n margin-bottom: 10px;\n}\n\n.applications {\n color: #9b59b6;\n margin-bottom: 10px;\n}\n\n.memory-hook {\n background-color: #ecf0f1;\n padding: 10px;\n border-left: 4px solid #34495e;\n font-style: italic;\n color: #34495e;\n}",
      "flds": [
        {
          "__type__": "NoteModelField",
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "ML/NLP Interview",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "__type__": "CardTemplate",
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>"
        }
      ],
      "type": 0
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-1",
      "fields": [
        "What is a knowledge graph in the context of NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> A knowledge graph is a database that stores knowledge as relationships between concepts, often using nodes for entities and edges for relations.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> It's like a web of facts connecting things in the world, allowing machines to reason about relationships.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Built from extracted entities and relations from text, stored as triples (subject, relation, object).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Flexible for complex queries but can be computationally intensive to build and query compared to relational databases.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Fact-checking, grounding LLMs, question answering.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Imagine a spider web where each intersection is an entity and threads are relations – pulling one reveals connected facts.</div>",
        "NLP Intuition Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-11-1",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Intuition",
        "Easy"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-2",
      "fields": [
        "Explain the process of grounding LLMs using knowledge graphs.",
        "<div class=\"concept\"><strong>Concept:</strong> Grounding anchors LLM responses in real-world knowledge via facts from a knowledge graph.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Prevents hallucinations by fact-checking generated text against structured knowledge.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Extract facts from LLM output, query KG for verification, use symbolic reasoning.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Adds reliability but increases complexity and computation; separates reasoning from generation.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Reliable chatbots, explainable AI decisions.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like checking a map before telling a story about a journey – ensures the path is real.</div>",
        "NLP Theory Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-11-2",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Theory",
        "Medium"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-3",
      "fields": [
        "How does sentence segmentation work, and why is it important for IE?",
        "<div class=\"concept\"><strong>Concept:</strong> Breaking text into sentences for processing in IE pipelines.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Sentences are self-contained units of meaning, ideal for extracting facts.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use regex for simple cases or spaCy models for accuracy, handling punctuation ambiguities.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Regex is fast but inaccurate; neural models are accurate but slower.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Preprocessing for NER, relation extraction in documents.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like cutting a rope into usable lengths – too long, it's unwieldy; sentences are the right chunks.</div>",
        "NLP Application Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-11-3",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Application",
        "Easy"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-4",
      "fields": [
        "Describe named entity recognition (NER) and its challenges.",
        "<div class=\"concept\"><strong>Concept:</strong> Identifying and categorizing named entities like persons, organizations in text.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Pinpointing specific 'things' in text to build knowledge.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use regex for patterns (e.g., GPS) or neural models like spaCy with BiLSTM+CRF.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Pattern-based is precise for known formats but inflexible; neural is general but requires training data.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Extracting entities for KG nodes.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like highlighting names in a phonebook – NER tags the important 'whos' and 'wheres'.</div>",
        "NLP Theory Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-11-4",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Theory",
        "Medium"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-5",
      "fields": [
        "What is coreference resolution, and why is it necessary?",
        "<div class=\"concept\"><strong>Concept:</strong> Identifying mentions referring to the same entity (e.g., pronouns to names).</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Connects 'she' back to 'Gebru' to avoid redundant KG nodes.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use transformer-based models like Coreferee in spaCy.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Handles ambiguity but computationally heavy; errors lead to incorrect relations.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Consolidating entity mentions in long texts.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like following aliases in a spy novel – corefs link the disguises to the real identity.</div>",
        "NLP Intuition Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-11-5",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Intuition",
        "Medium"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-6",
      "fields": [
        "Explain dependency parsing and its role in relation extraction.",
        "<div class=\"concept\"><strong>Concept:</strong> Building a tree of grammatical dependencies between words.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Shows how words rely on each other to form meaning, like sentence diagrams.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> SpaCy uses neural models to tag deps like nsubj, dobj; root is main verb.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Captures logic but can miss nuanced constituency; vs. constituency parsing which is more detailed but slower.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Extracting subject-verb-object triples for KG edges.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like a family tree for words – dependencies show who's the boss (root) and kids (children).</div>",
        "NLP Theory Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-11-6",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Theory",
        "Hard"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-7",
      "fields": [
        "How does pattern-based relation extraction work?",
        "<div class=\"concept\"><strong>Concept:</strong> Using rules or patterns to find relations between entities.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Matches linguistic patterns like 'X met Y' for 'has-met' relation.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> SpaCy Matcher with POS patterns; seed sentences expand to similar ones.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Precise for specific domains but suffers semantic drift; less general than neural.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Extracting meetings from historical texts.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like fishing with a specific lure – patterns catch exact relation 'fish' but miss others.</div>",
        "NLP Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-11-7",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Application",
        "Medium"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-8",
      "fields": [
        "What are neural methods for relation extraction?",
        "<div class=\"concept\"><strong>Concept:</strong> Using NNs to identify relations, closed (fixed labels) or open (generated labels).</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Models learn relation patterns from data, beyond rules.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Models like LUKE use entity-aware attention; train on datasets like DocRED.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Handles variety but needs large labeled data; open risks bizarre labels.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Drug interactions from pharma texts.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like a detective learning clues – neural RE infers hidden relations from evidence.</div>",
        "NLP Theory Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-11-8",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Theory",
        "Hard"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-9",
      "fields": [
        "Describe building a knowledge base from extracted relations.",
        "<div class=\"concept\"><strong>Concept:</strong> Storing triples as graph with nodes (entities) and edges (relations).</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Organizes facts for inference and multi-hop queries.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use RDF triples; normalize entities; add to graphs like NELL or Wikidata.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Enables complex reasoning vs. tables but queries can be slower without optimization.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Commonsense KB, fact-checking.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like building a Lego city – entities are blocks, relations connect them into a functional world.</div>",
        "NLP Application Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-11-9",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Application",
        "Hard"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-10",
      "fields": [
        "How do you query a knowledge graph, e.g., with SPARQL on Wikidata?",
        "<div class=\"concept\"><strong>Concept:</strong> Using query languages to retrieve facts from KGs.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Translates questions into graph traversals for answers.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> SPARQL with SELECT, WHERE; use Q-ids, P-ids; nest for complex queries.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Powerful for inference but requires knowing IDs; vs. SQL for relational DBs.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Finding coauthors from notable works.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like asking a librarian for books – SPARQL navigates the KG library shelves.</div>",
        "NLP Connections Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-11-10",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Connections",
        "Medium"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-11",
      "fields": [
        "Compare knowledge graphs to relational databases for QA.",
        "<div class=\"concept\"><strong>Concept:</strong> KGs store relations as graphs, enabling multi-hop inference.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Graphs handle interconnected facts better than tables for reasoning.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Query with SPARQL vs. SQL; graphs allow recursive relations easily.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> KGs better for unstructured inference; RDBs for structured, fast joins.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Answering 'military rank' via inference.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Tables are flat maps; graphs are 3D globes – better for navigating relations.</div>",
        "NLP Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-11-11",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Trade-offs",
        "Hard"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-12",
      "fields": [
        "What is the role of GOFAI in modern NLP pipelines?",
        "<div class=\"concept\"><strong>Concept:</strong> Good Old-Fashioned AI: symbolic reasoning for logic-based systems.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Combines with neural for reliable, explainable AI.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use for inference in KGs, grounding probabilistic models.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Deterministic but limited without data; hybrid with DL overcomes this.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Fact-checking LLM outputs.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like blending old wisdom (GOFAI) with new tech (DL) for balanced AI.</div>",
        "NLP Connections Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-11-12",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Connections",
        "Easy"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-13",
      "fields": [
        "How can semantic heatmaps aid in understanding text structure?",
        "<div class=\"concept\"><strong>Concept:</strong> Visualizing similarity of sentence embeddings.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Shows redundant or unique parts in documents.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use BERT/MiniLM embeddings, cosine similarity matrix, seaborn heatmap.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Reveals structure but compute-heavy for long docs.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Editing books, analyzing redundancy.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like a heat signature map – hot spots show similar ideas clustering.</div>",
        "NLP Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-11-13",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Application",
        "Medium"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-14",
      "fields": [
        "Discuss entity name normalization in KGs.",
        "<div class=\"concept\"><strong>Concept:</strong> Standardizing entity representations (e.g., dates to ISO).</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Ensures unique, consistent entity IDs to avoid duplicates.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Resolve synonyms, correct spellings, link to unique IDs.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Prevents pollution but requires migration on changes.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Connecting events on same date.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like standardizing addresses – normalization ensures mail (queries) reaches the right place.</div>",
        "NLP Trade-offs Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-11-14",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Trade-offs",
        "Medium"
      ]
    },
    {
      "__type__": "Note",
      "crowdanki_uuid": "note-11-15",
      "fields": [
        "What datasets are used for benchmarking relation extraction?",
        "<div class=\"concept\"><strong>Concept:</strong> Labeled datasets for training/evaluating RE models.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Provide ground truth for learning relations.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> TACRED (41 types), DocRED (document-level).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Large for generality but labeling is expensive.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Fine-tuning models like LUKE.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like recipe books for chefs – datasets guide models in cooking up relations.</div>",
        "NLP Connections Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-11-15",
      "note_model_uuid": "ml-nlp-interview-model",
      "tags": [
        "NLP",
        "Connections",
        "Hard"
      ]
    }
  ]
}