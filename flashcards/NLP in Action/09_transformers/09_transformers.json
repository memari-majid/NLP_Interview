{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-5964",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 09 Transformers",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:09 Transformers",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n\n.front {\n font-weight: bold;\n color: #2c3e50;\n}\n\n.back {\n text-align: left;\n padding: 20px;\n}\n\n.concept {\n font-weight: bold;\n color: #e74c3c;\n margin-bottom: 10px;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 10px;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 10px;\n}\n\n.tradeoffs {\n color: #f39c12;\n margin-bottom: 10px;\n}\n\n.applications {\n color: #9b59b6;\n margin-bottom: 10px;\n}\n\n.memory-hook {\n background-color: #ecf0f1;\n padding: 10px;\n border-left: 4px solid #34495e;\n font-style: italic;\n color: #34495e;\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note--6645811740357786220-5964",
      "fields": [
        "What is the primary advantage of transformers over recurrent neural networks like LSTMs in terms of processing sequences?",
        "<div class=\"concept\"><strong>Concept:</strong> Transformers vs. RNNs</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Transformers process entire sequences at once using attention, avoiding sequential computation.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Unlike RNNs, which process tokens sequentially and suffer from vanishing gradients, transformers use self-attention to connect all tokens in parallel.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Transformers require more memory for long sequences due to quadratic complexity in attention, but enable faster training on GPUs.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Used in translation, summarization, and text generation tasks like GPT models.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Think of transformers as a 'global view' party where everyone talks at once, unlike RNNs' 'conga line' where info passes one by one.</div>",
        "Transformers RNNs Attention Intuition",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--6645811740357786220-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Transformers",
        "RNNs",
        "Attention",
        "Intuition"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--6871731277012639437-5964",
      "fields": [
        "Explain how byte pair encoding (BPE) contributes to the efficiency of transformers.",
        "<div class=\"concept\"><strong>Concept:</strong> Byte Pair Encoding (BPE)</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> BPE builds a compact vocabulary by merging frequent character pairs, reducing out-of-vocabulary issues.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Start with characters, iteratively merge the most frequent pairs into new tokens until vocabulary size is reached, e.g., 5,000 tokens.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Reduces vocabulary size for efficiency but may split rare words into subwords, increasing sequence length slightly.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Tokenization in models like BERT and GPT for handling diverse languages and compressing internet-scale data.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> BPE is like zip-compressing text by gluing common letter pairs, making big dictionaries tiny like a travel suitcase.</div>",
        "BPE Tokenization Theory",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--6871731277012639437-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "BPE",
        "Tokenization",
        "Theory"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8087592423536449987-5964",
      "fields": [
        "How does positional encoding enable transformers to handle sequence order without recurrence?",
        "<div class=\"concept\"><strong>Concept:</strong> Positional Encoding</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> It adds a unique 'position signal' to each token's embedding, like numbering seats in a theater.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Uses sine and cosine functions: PE(pos,2i) = sin(pos / 10000^{2i/d}), PE(pos,2i+1) = cos(pos / 10000^{2i/d}), added to embeddings.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Fixed and doesn't require learning, but limited to fixed max sequence length; learned alternatives exist but add parameters.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Essential in BERT for bidirectional context and GPT for generation order.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Imagine words on a wavy sine wave conveyor belt—each position has a unique ripple pattern.</div>",
        "Positional Encoding Math Transformers",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--8087592423536449987-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Positional Encoding",
        "Math",
        "Transformers"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4584784221639618230-5964",
      "fields": [
        "Describe the mechanics of multi-head self-attention in transformers and its benefits.",
        "<div class=\"concept\"><strong>Concept:</strong> Multi-Head Self-Attention</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Multiple 'heads' focus on different aspects of relationships between words, like multiple spotlights on a stage.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Project input to Q, K, V matrices per head; compute scaled dot-product attention; concatenate and project outputs.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Increases model capacity without much computation cost, but more heads require more memory.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Core to BERT's bidirectional encoding and GPT's generation.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like a hydra with multiple heads, each attending to a different word connection in the text.</div>",
        "Attention Multi-Head Mechanics",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--4584784221639618230-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Attention",
        "Multi-Head",
        "Mechanics"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--1906994981567990761-5964",
      "fields": [
        "Compare the recursion in transformers to recurrence in RNNs, and discuss implications for scalability.",
        "<div class=\"concept\"><strong>Concept:</strong> Recursion vs. Recurrence</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Transformers recycle outputs at the network level, not per neuron, allowing parallel processing.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Transformers run the full network per token prediction, recycling output tokens as input; RNNs loop internally per step.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> No unrolling needed in transformers, enabling stacking and parallelism, but higher per-token compute.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Enables massive models like GPT-3 for text generation.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNNs are like a single juggler passing balls hand-to-hand; transformers are a team juggling in sync.</div>",
        "Recursion Recurrence Comparisons",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--1906994981567990761-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Recursion",
        "Recurrence",
        "Comparisons"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2790658604882004647-5964",
      "fields": [
        "How can transformers be applied to extractive and abstractive summarization of long documents?",
        "<div class=\"concept\"><strong>Concept:</strong> Summarization with Transformers</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Extractive picks key sentences; abstractive generates new ones, mimicking human paraphrasing.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use encoder-decoder for abstractive (e.g., BART); attention scores for extractive to rank sentences.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Extractive is factual but less fluent; abstractive is creative but risks hallucinations.</div><br><br><div class=\"applications\"><strong>Applications:</strong> News summarization, document compression in legal or medical fields.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Extractive is copying highlights; abstractive is rewriting notes in your own words.</div>",
        "Summarization Applications Transformers",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-2790658604882004647-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Summarization",
        "Applications",
        "Transformers"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4080939832949066047-5964",
      "fields": [
        "What are the trade-offs of fine-tuning a pretrained transformer like BERT for a specific application?",
        "<div class=\"concept\"><strong>Concept:</strong> Fine-Tuning Transformers</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Adapt a general expert to your niche by teaching it specifics.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Load pretrained weights, add task-specific layers, train on labeled data with lower learning rate.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Quick and data-efficient but risks overfitting if data is small; may forget general knowledge (catastrophic forgetting).</div><br><br><div class=\"applications\"><strong>Applications:</strong> Toxic comment classification, sentiment analysis.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like retraining a polymath chef to specialize in vegan dishes without losing basic skills.</div>",
        "Fine-Tuning Trade-offs BERT",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--4080939832949066047-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Fine-Tuning",
        "Trade-offs",
        "BERT"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7828878304916274345-5964",
      "fields": [
        "Explain how to estimate the information capacity of a transformer model.",
        "<div class=\"concept\"><strong>Concept:</strong> Information Capacity of Transformers</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Capacity is like storage space for patterns; more layers/heads mean more room.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Roughly, parameters count: layers * (hidden_size^2 * heads + feedforward).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Higher capacity improves performance on large data but increases compute and overfitting risk.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Scaling for LLMs like GPT-3 to handle diverse tasks.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Think of it as bookshelf size—more shelves (layers) hold more books (patterns).</div>",
        "Capacity Math Scaling",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-7828878304916274345-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Capacity",
        "Math",
        "Scaling"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8698406415299322282-5964",
      "fields": [
        "How does BERT's bidirectional training differ from unidirectional models like GPT, and what are the connections to other architectures?",
        "<div class=\"concept\"><strong>Concept:</strong> BERT's Bidirectionality</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> BERT reads both ways for full context, like scanning a page before filling blanks.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Pretrained with masked language modeling (MLM) and next sentence prediction (NSP).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Better for understanding but not autoregressive, so less suited for pure generation.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Question answering, semantic search.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> BERT is a detective looking left and right for clues; GPT is a storyteller predicting forward.</div>",
        "BERT Bidirectional Connections",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-8698406415299322282-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "BERT",
        "Bidirectional",
        "Connections"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--6654826601932398582-5964",
      "fields": [
        "Discuss the applications and limitations of using transformers for generating grammatically correct text.",
        "<div class=\"concept\"><strong>Concept:</strong> Text Generation with Transformers</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Autoregressively predict next tokens based on patterns learned from data.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Decoder stacks generate tokens recursively, using beam search for better outputs.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Produces fluent text but can hallucinate facts or lack reasoning.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Chatbots, story generation, code completion.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like an improv actor continuing a scene plausibly, but sometimes forgetting the plot.</div>",
        "Generation Applications Limitations",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--6654826601932398582-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Generation",
        "Applications",
        "Limitations"
      ],
      "__type__": "Note"
    }
  ]
}