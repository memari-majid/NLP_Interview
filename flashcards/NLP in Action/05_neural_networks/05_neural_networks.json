{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-3314",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 05 Neural Networks",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:05 Neural Networks",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n\n.front {\n font-weight: bold;\n color: #2c3e50;\n}\n\n.back {\n text-align: left;\n padding: 20px;\n}\n\n.concept {\n font-weight: bold;\n color: #e74c3c;\n margin-bottom: 10px;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 10px;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 10px;\n}\n\n.tradeoffs {\n color: #f39c12;\n margin-bottom: 10px;\n}\n\n.applications {\n color: #9b59b6;\n margin-bottom: 10px;\n}\n\n.memory-hook {\n background-color: #ecf0f1;\n padding: 10px;\n border-left: 4px solid #34495e;\n font-style: italic;\n color: #34495e;\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note--5347775305428609894-3314",
      "fields": [
        "What is a neural network in the context of machine learning?",
        "<div class=\"concept\"><strong>Concept:</strong> A neural network is a machine learning model inspired by biological brains, consisting of interconnected nodes (neurons) that process data in layers.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Think of it as a mini-brain that learns patterns from data by adjusting connections, much like how your brain forms memories.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> It processes inputs through weighted connections, applies activation functions, and outputs predictions; trained via backpropagation to minimize error.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Powerful for complex patterns but computationally intensive and prone to overfitting without proper regularization.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Used in NLP for tasks like question answering, summarization, and natural language inference.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Neural nets: Brains in code, firing signals to learn words.</div>",
        "Neural Networks Definition Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--5347775305428609894-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neural Networks",
        "Definition",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-5664908433389600125-3314",
      "fields": [
        "Why are neural networks particularly useful for NLP tasks?",
        "<div class=\"concept\"><strong>Concept:</strong> Neural networks excel in NLP because they automatically engineer features from raw text data.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> They act like a 'word brain' that understands meaning from statistics, without needing manual preprocessing like stemming.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> They learn representations based on word relationships and targets, optimizing features via backpropagation.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Avoid over-reliance on them for explainable models; they can be black boxes compared to traditional ML.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Question answering, reading comprehension, summarization, and even code generation for NLP.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> NNs for NLP: Auto-feature magic, ditching stems for stats.</div>",
        "NLP Neural Networks Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-5664908433389600125-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Neural Networks",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8491639523810441160-3314",
      "fields": [
        "Explain the intuition behind why neural networks generalize better from few examples compared to traditional ML.",
        "<div class=\"concept\"><strong>Concept:</strong> Neural networks generalize by learning hierarchical representations and features automatically.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Like a brain connecting dots with limited info, NNs build deep patterns that apply broadly.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Through layered transformations, they capture abstract features optimized for the task.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Require more data overall; with too few, they overfit unless regularized.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In NLP, predicting meanings of rare words or names based on character patterns.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Generalization: NNs as pattern detectives with a few clues.</div>",
        "Neural Networks Intuition Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-8491639523810441160-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neural Networks",
        "Intuition",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4006200999827126877-3314",
      "fields": [
        "What mathematical theory underpins how neural networks approximate functions, and what are its limitations?",
        "<div class=\"concept\"><strong>Concept:</strong> The Universal Approximation Theorem states that neural networks can approximate any continuous function.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> NNs are flexible shape-shifters for data curves, but not magic—they need depth and data.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> With sufficient neurons and layers, they model complex mappings via weights and activations.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Doesn't guarantee learning the function; prone to local minima and requires large compute.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Approximating nonlinear relationships in NLP like word embeddings.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> UAT: NNs as universal function mimics, but training's the trick.</div>",
        "Neural Networks Math Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--4006200999827126877-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neural Networks",
        "Math",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6824438697138052999-3314",
      "fields": [
        "Compare neural networks to traditional feature engineering methods in NLP.",
        "<div class=\"concept\"><strong>Concept:</strong> Neural networks automate feature engineering, unlike manual methods like TF-IDF or stemming.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Manual is guessing tools; NNs learn the best tools from data itself.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> NNs optimize representations through gradients; traditional uses fixed transformations like PCA.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> NNs are data-hungry and less interpretable; traditional is faster but suboptimal.</div><br><br><div class=\"applications\"><strong>Applications:</strong> NNs for end-to-end NLP pipelines; traditional for quick IR tasks.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> NN vs. old school: Auto vs. handcraft, depth over guesswork.</div>",
        "Neural Networks Comparison Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-6824438697138052999-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neural Networks",
        "Comparison",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4311672126738063265-3314",
      "fields": [
        "What is backpropagation and why is it essential for training neural networks?",
        "<div class=\"concept\"><strong>Concept:</strong> Backpropagation is an algorithm to compute gradients of the loss with respect to weights.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> It's like error feedback rippling back, telling each neuron how to adjust.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Uses chain rule to propagate derivatives backward through layers.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Efficient but can suffer from vanishing/exploding gradients in deep nets.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Training NNs for NLP tasks like name sex prediction.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Backprop: Error echoes fixing weights layer by layer.</div>",
        "Backpropagation Mechanics Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--4311672126738063265-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Backpropagation",
        "Mechanics",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--6027783099025974660-3314",
      "fields": [
        "Describe the biological inspiration behind artificial neurons.",
        "<div class=\"concept\"><strong>Concept:</strong> Artificial neurons mimic biological ones with inputs (dendrites), processing (nucleus), and outputs (axon).</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Like brain cells firing on signals, artificial ones activate on weighted sums.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Weights as sensitivities; threshold via activation functions.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Simplified model; doesn't capture full brain complexity like plasticity.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Basis for perceptrons in NLP classification.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Bio neurons: Dendrites in, axon out, weights as sensitivity knobs.</div>",
        "Neurons Intuition Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--6027783099025974660-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neurons",
        "Intuition",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-5464118806663367377-3314",
      "fields": [
        "How does a perceptron work mathematically?",
        "<div class=\"concept\"><strong>Concept:</strong> A perceptron is a single artificial neuron with weighted sum and threshold activation.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Sums inputs like votes, fires if over threshold.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Output = step(sum(w_i * x_i) + b), where b is bias.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Linearly separable only; can't handle XOR without layers.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Basic binary classification, e.g., spam detection.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Perceptron: Weighted vote, step to decide.</div>",
        "Perceptron Math Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-5464118806663367377-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Perceptron",
        "Math",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6513642701222972157-3314",
      "fields": [
        "What is the role of the bias term in a neuron?",
        "<div class=\"concept\"><strong>Concept:</strong> The bias term shifts the activation threshold, like an intercept in regression.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> It's the 'always on' input ensuring output even with zero features.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Added as w0 * 1 in the weighted sum.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Essential for flexibility; without it, zero inputs always output zero.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In logistic neurons for shifting decision boundaries.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Bias: The constant nudge in neuron decisions.</div>",
        "Neurons Mechanics Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-6513642701222972157-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neurons",
        "Mechanics",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-1076240157498574763-3314",
      "fields": [
        "Explain how a single neuron relates to logistic regression.",
        "<div class=\"concept\"><strong>Concept:</strong> A neuron with sigmoid activation is equivalent to logistic regression.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Both model probabilities with a soft threshold.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Output = sigmoid(w · x + b); trained similarly but via backprop in nets.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Neuron scalable in networks; logistic simpler for single-layer.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Sex prediction from names using character n-grams.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Neuron = logistic: Sigmoid slopes for binary bets.</div>",
        "Neurons Comparison Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-1076240157498574763-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neurons",
        "Comparison",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7495109560889856173-3314",
      "fields": [
        "What are the trade-offs of using stemming or lemmatization in deep learning NLP pipelines?",
        "<div class=\"concept\"><strong>Concept:</strong> Stemming/lemmatization reduces words to roots, but in DL, it's often unnecessary.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> They 'rob' raw info; NNs learn better from full forms.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Traditional preprocessing; NNs auto-learn from chars/words.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> May help small data but hurts generalization; test without them.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Avoid in modern NLP for tasks like sentiment analysis.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Stemmers: Old hats stealing NN's learning feast.</div>",
        "NLP Trade-offs Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-7495109560889856173-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Trade-offs",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-5345542093427100113-3314",
      "fields": [
        "Describe the mechanics of gradient descent in neural network training.",
        "<div class=\"concept\"><strong>Concept:</strong> Gradient descent minimizes loss by updating weights opposite to gradients.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Skiing down error slopes to the lowest point.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> w_new = w_old - lr * ∇loss; iterate over epochs.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Can stuck in local minima; slow for large data.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Optimizing weights in NLP models.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> GD: Downhill ski on error hills.</div>",
        "Optimization Mechanics Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-5345542093427100113-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Optimization",
        "Mechanics",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--7285359424451064111-3314",
      "fields": [
        "What is stochastic gradient descent (SGD) and how does it differ from standard GD?",
        "<div class=\"concept\"><strong>Concept:</strong> SGD uses single or mini-batch samples for gradient estimates.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Shaky skiing to escape pits, exploring more terrain.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Updates per sample/batch; adds noise to avoid local minima.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Faster, escapes locals but noisier convergence.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Standard for large NLP datasets in PyTorch.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> SGD: Jittery descent shaking off traps.</div>",
        "Optimization Comparison Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--7285359424451064111-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Optimization",
        "Comparison",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-4442220625448602797-3314",
      "fields": [
        "Why use mini-batch learning in neural networks?",
        "<div class=\"concept\"><strong>Concept:</strong> Mini-batch balances full batch and stochastic learning.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Group ski: Steady progress with some shake.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Batch size 16-64; averages gradients for stability.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Optimal for GPU; too small noisy, too large slow.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Training transformers on text corpora.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Mini-batch: Sweet spot squad for slope surfing.</div>",
        "Optimization Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-4442220625448602797-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Optimization",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2155739231972823591-3314",
      "fields": [
        "What is the polynomial feature explosion and how do neural networks address it?",
        "<div class=\"concept\"><strong>Concept:</strong> Explosion: Exponential growth of features in polynomial expansions.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Too many combos blow up dims; NNs smartly select.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> NNs learn nonlinear combos via layers, avoiding explicit polys.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> NNs implicit but black-box; polys explicit but infeasible.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In NLP, handling word interactions without manual polys.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Poly boom: NNs defuse with layered learning.</div>",
        "Feature Engineering Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-2155739231972823591-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Feature Engineering",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-582345569941861349-3314",
      "fields": [
        "How do you implement a basic perceptron in Python?",
        "<div class=\"concept\"><strong>Concept:</strong> A simple function computing weighted sum and step activation.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Dot product decides: Fire or not.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> def perceptron(x, w): return sum(xi*wi for xi,wi in zip(x,w)) > 0</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Basic; add sigmoid for probabilities.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Toy binary classifiers.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Python perc: Zip, sum, threshold—done.</div>",
        "Implementation Mechanics Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-582345569941861349-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Implementation",
        "Mechanics",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-692402801267722579-3314",
      "fields": [
        "Explain the sigmoid activation function mathematically and its use.",
        "<div class=\"concept\"><strong>Concept:</strong> Sigmoid: σ(x) = 1 / (1 + e^{-x}), maps to (0,1).</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Soft switch for probabilities.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Compresses linear combo to prob; derivative for backprop.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Vanishing gradients; prefer ReLU for deep nets.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Binary classification outputs.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Sigmoid: S-curve squishing to probs.</div>",
        "Activation Math Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-692402801267722579-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Activation",
        "Math",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-1634856270673245043-3314",
      "fields": [
        "What is binary cross-entropy loss and when to use it?",
        "<div class=\"concept\"><strong>Concept:</strong> BCE: - (y log(p) + (1-y) log(1-p)), measures binary prediction error.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Penalizes confident wrongs heavily.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Averages over samples; weights for imbalance.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> For binary; multi-class use categorical CE.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Sex classification in names.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> BCE: Log penalty for prob mismatches.</div>",
        "Loss Functions Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-1634856270673245043-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Loss Functions",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7682160349256587849-3314",
      "fields": [
        "How does PyTorch facilitate building a logistic regression neuron?",
        "<div class=\"concept\"><strong>Concept:</strong> PyTorch uses nn.Module for models, with Linear and sigmoid.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Lego-like: Build layers, forward prop.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> class LogisticNN(nn.Module): linear = nn.Linear(in, out); forward: sigmoid(linear(X))</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Flexible but manual; vs. scikit-learn's simplicity.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Scalable NLP models.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> PyTorch neuron: Linear + sigmoid in Module.</div>",
        "PyTorch Implementation Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-7682160349256587849-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "PyTorch",
        "Implementation",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-4907503354037111896-3314",
      "fields": [
        "Discuss connections between neural networks and biological brains in NLP.",
        "<div class=\"concept\"><strong>Concept:</strong> NNs inspired by neurons but abstract; process language statistically.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Brains predict words; NNs do too via patterns.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Weights as synapses; backprop as learning.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> NNs efficient but lack true understanding.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Word prediction, AGI paths.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> NN-brain link: Synaptic stats for word wisdom.</div>",
        "Neural Networks Connections Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-4907503354037111896-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neural Networks",
        "Connections",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--7193089003509740270-3314",
      "fields": [
        "What are local minima in optimization and how to mitigate them?",
        "<div class=\"concept\"><strong>Concept:</strong> Local minima: Suboptimal low points in error surface.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Stuck in divots missing the valley bottom.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Nonconvex surfaces; SGD noise helps escape.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Momentum accelerates; but no guarantee.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Deep NLP training.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Local mins: Divot traps, SGD shakes free.</div>",
        "Optimization Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--7193089003509740270-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Optimization",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4933841532059020127-3314",
      "fields": [
        "How to prepare name data for sex prediction in NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> Use TF-IDF on character n-grams as features.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Break names into char patterns for generalization.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> TfidfVectorizer(analyzer='char', ngram_range=(1,3)); aggregate counts.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Handles OOV but binary labels oversimplify.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Gender inference in text.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Names to vecs: Char n-grams TF-IDF sex guess.</div>",
        "NLP Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--4933841532059020127-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--1797895239973757015-3314",
      "fields": [
        "What is the Universal Approximation Theorem's relevance to NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> UAT: NNs can approximate any function with enough neurons.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> NNs flexible for language's complexity.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Multi-layer for nonlinearities in word relations.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Practical limits: Data, compute, overfitting.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Modeling ambiguous word meanings.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> UAT in NLP: NNs bend to language twists.</div>",
        "Neural Networks Theory Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--1797895239973757015-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Neural Networks",
        "Theory",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-354921508147267587-3314",
      "fields": [
        "Explain momentum in SGD optimizers.",
        "<div class=\"concept\"><strong>Concept:</strong> Momentum accelerates updates in consistent directions.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Build speed downhill, slow on turns.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> v = μ v + ∇; w -= lr v.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Helps escape saddles but can overshoot.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Faster convergence in NLP training.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Momentum: Optimizer's speedup snowball.</div>",
        "Optimization Mechanics Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-354921508147267587-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Optimization",
        "Mechanics",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-4498475118243905738-3314",
      "fields": [
        "Why avoid polynomial features in favor of neural networks?",
        "<div class=\"concept\"><strong>Concept:</strong> Polys explode dims; NNs learn equivalents implicitly.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Manual boom vs. auto nonlinear magic.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Layers create interactions without explicit multiplication.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> NNs less interpretable but scalable.</div><br><br><div class=\"applications\"><strong>Applications:</strong> High-dim NLP without feature explosion.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Polys: Boom avoided by NN layers.</div>",
        "Feature Engineering Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-4498475118243905738-3314",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Feature Engineering",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    }
  ]
}