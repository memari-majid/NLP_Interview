{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-2848",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 02 Tokenization",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:02 Tokenization",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n\n.front {\n font-weight: bold;\n color: #2c3e50;\n}\n\n.back {\n text-align: left;\n padding: 20px;\n}\n\n.concept {\n font-weight: bold;\n color: #e74c3c;\n margin-bottom: 10px;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 10px;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 10px;\n}\n\n.tradeoffs {\n color: #f39c12;\n margin-bottom: 10px;\n}\n\n.applications {\n color: #9b59b6;\n margin-bottom: 10px;\n}\n\n.memory-hook {\n background-color: #ecf0f1;\n padding: 10px;\n border-left: 4px solid #34495e;\n font-style: italic;\n color: #34495e;\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note-2345456388112084668-2848",
      "fields": [
        "What is tokenization in NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> Tokenization is the process of breaking unstructured natural language text into smaller units called tokens, which can be words, punctuation, or other meaningful elements.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Imagine chopping a sentence into bite-sized pieces like words or symbols, making it easier for a computer to process language like counting ingredients in a recipe.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> A tokenizer segments text based on rules, such as splitting on whitespace or using regular expressions to handle punctuation.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Simple tokenizers are fast but may mishandle punctuation or complex cases; advanced ones are accurate but slower.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Used in search engines, chatbots, and machine learning pipelines to convert text into numerical data.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Tokenization is like 'token'izing a ticket—breaking text into tickets (tokens) for the NLP ride.</div>",
        "Tokenization Basics Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-2345456388112084668-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Tokenization",
        "Basics",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--2010130301652350521-2848",
      "fields": [
        "Why is tokenization the foundation of NLP pipelines?",
        "<div class=\"concept\"><strong>Concept:</strong> Tokenization converts raw text into discrete units (tokens) that can be numerically represented for further processing.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Without breaking text into tokens, computers can't 'count' or analyze language, like trying to bake without measuring ingredients.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> It segments text into words, n-grams, or subwords, enabling vectorization and statistical analysis.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Retains structure but loses some raw information like exact whitespace; essential for efficiency in large models.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Powers LLMs like ChatGPT by predicting next tokens; used in sentiment analysis and search.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Tokenization is the 'first cut'—like slicing bread before making a sandwich in NLP.</div>",
        "Tokenization Intuition Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--2010130301652350521-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Tokenization",
        "Intuition",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--3572727522048097349-2848",
      "fields": [
        "Compare simple whitespace tokenization with rule-based tokenization using regular expressions.",
        "<div class=\"concept\"><strong>Concept:</strong> Whitespace tokenization splits on spaces; rule-based uses regex to handle punctuation and contractions.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Whitespace is quick but clumsy with commas; regex is smarter, keeping 'There's' intact like a precise surgeon.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Whitespace: text.split(); Regex: re.findall(r'\\w+(?:\\'\\w+)?|[^\\w\\s]', text) to manage apostrophes.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Whitespace is faster but error-prone; regex is accurate but requires crafting patterns, potentially missing edge cases.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Whitespace for quick prototypes; regex for handling English contractions in sentiment analysis.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Whitespace is a blunt axe; regex a sharp scalpel for token surgery.</div>",
        "Tokenization Comparison Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--3572727522048097349-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Tokenization",
        "Comparison",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--2381062065712765712-2848",
      "fields": [
        "What are the advantages of using SpaCy for tokenization?",
        "<div class=\"concept\"><strong>Concept:</strong> SpaCy provides fast, production-ready tokenization with additional linguistic features like POS tagging and lemmatization.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> It's like a Swiss Army knife—tokens plus grammar insights, speeding up pipeline development.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Load model: nlp = spacy.load('en_core_web_sm'); Process: doc = nlp(text); Tokens: [tok.text for tok in doc].</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Accurate and feature-rich but slower than regex; requires model downloads.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In chatbots for intent recognition or NER in legal texts.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> SpaCy is 'spacey'—expansive features orbiting around tokens.</div>",
        "Tokenization SpaCy Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--2381062065712765712-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Tokenization",
        "SpaCy",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-9126635698890621045-2848",
      "fields": [
        "Explain Byte Pair Encoding (BPE) and its use in subword tokenization.",
        "<div class=\"concept\"><strong>Concept:</strong> BPE is a subword tokenization method that merges frequent character pairs into tokens, building a vocabulary from data.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Like matchmaking letters that hang out together, creating efficient 'word pieces' for unknown words.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Start with characters; iteratively merge most frequent pairs until vocabulary size limit; uses counts from corpus.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Handles OOV words well but requires training on corpus; larger vocab than characters but smaller than words.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In transformers like GPT for multilingual robustness and handling misspellings.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> BPE: 'Byte Pairs Engage'—characters pair up like dancers at a byte ball.</div>",
        "Subword Tokenization BPE Theory Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-9126635698890621045-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Subword Tokenization",
        "BPE",
        "Theory",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--3553826004294695929-2848",
      "fields": [
        "How do n-grams improve upon single-word tokens?",
        "<div class=\"concept\"><strong>Concept:</strong> n-grams are sequences of n tokens, capturing context and order lost in bag-of-words.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Single words are isolated notes; n-grams are chords, preserving phrases like 'ice cream'.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Generate sequences: 2-grams from 'I scream for ice cream' include 'I scream', 'scream for'.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Retains order but explodes vocabulary size; useful for negation like 'not good'.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In search for phrase matching or sentiment to handle 'not bad'.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> n-grams: 'n' neighbors grouping words like friends in a chain.</div>",
        "n-grams Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--3553826004294695929-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "n-grams",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--2931142686645909444-2848",
      "fields": [
        "What are stop words and when should they be removed?",
        "<div class=\"concept\"><strong>Concept:</strong> Stop words are common words like 'the', 'and' with low information value.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> They're the glue in sentences but often noise; remove to focus on content words.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Filter using lists from NLTK or SpaCy: [word for word in tokens if word not in stop_words].</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Reduces dimensionality but loses relational info like in 'reported to the CEO'.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In keyword search to speed up; avoid in n-grams for context.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Stop words: 'Stop' the filler traffic in your word highway.</div>",
        "Stop Words Trade-offs Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--2931142686645909444-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Stop Words",
        "Trade-offs",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--1315166507552900421-2848",
      "fields": [
        "Describe case folding and its impact on NLP pipelines.",
        "<div class=\"concept\"><strong>Concept:</strong> Case folding normalizes text by converting to lowercase, consolidating 'House' and 'house'.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Ignores shouting (caps) to treat words equally, like ignoring accents in speech.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> text.lower() or selective on sentence starts to preserve proper nouns.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Reduces vocabulary but loses proper noun info; improves recall, reduces precision.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In search engines for broader matches; avoid in NER.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Case folding: Folding paper to make capitals small, like origami for text.</div>",
        "Normalization Case Folding Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--1315166507552900421-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Normalization",
        "Case Folding",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-4501976400282047235-2848",
      "fields": [
        "Compare stemming and lemmatization in terms of accuracy and use cases.",
        "<div class=\"concept\"><strong>Concept:</strong> Stemming chops suffixes (e.g., Porter stemmer); lemmatization uses dictionaries for root forms.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Stemming is rough axe; lemmatization precise knife, considering meaning.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Stem: stemmer.stem('better') -> 'bett'; Lemma: lemmatizer.lemmatize('better', pos='a') -> 'good'.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Stemming faster, more aggressive (errors like 'betting' to 'bett'); Lemmatization accurate but needs POS.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Stemming for search recall; Lemmatization for chatbots needing precision.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Stemming stems from rules; Lemmatization lemmas from knowledge—like stem vs. full fruit.</div>",
        "Stemming Lemmatization Comparison Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-4501976400282047235-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Stemming",
        "Lemmatization",
        "Comparison",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-1225588349399029214-2848",
      "fields": [
        "How does tokenization differ for logographic languages like Chinese?",
        "<div class=\"concept\"><strong>Concept:</strong> No spaces between words; uses segmentation tools like Jieba to split into meaningful units.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Chinese text is a continuous stream; tokenization carves words from the flow.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Jieba modes: accurate (minimal splits), full (all possible), search (for engines).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> No stemming easy; radicals exist but splitting changes meaning drastically.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In translation or search for Chinese; Jieba for accurate segmentation.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Chinese tokens: Carving jade—precise cuts for beautiful words.</div>",
        "Logographic Languages Chinese Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-1225588349399029214-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Logographic Languages",
        "Chinese",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2219697511616317270-2848",
      "fields": [
        "What is a one-hot vector in NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> A binary vector where one position is 1 (hot) indicating a token's presence, others 0.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Like a light switch: only one on for each word in vocabulary.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> For vocab size V, vector length V; index i=1 if token i present.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Lossless but sparse and high-dimensional; retains order in sequences.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Input to neural nets; sequence models like RNNs.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> One-hot: One spotlight in a dark room highlighting a word.</div>",
        "Vectors One-hot Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-2219697511616317270-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vectors",
        "One-hot",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--2879108278126717736-2848",
      "fields": [
        "Explain bag-of-words (BOW) vectors and their advantages.",
        "<div class=\"concept\"><strong>Concept:</strong> A vector counting token occurrences in a document, ignoring order.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Jumble words in a bag and count them—summarizes content without sequence.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use Counter on tokens; vector per document with vocab dimensions.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Compresses info, fast for search; loses order and grammar.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Search indexes, similarity via distance; basis for TF-IDF.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> BOW: Bag of popcorn words—count kernels, ignore plot.</div>",
        "Vectors BOW Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--2879108278126717736-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vectors",
        "BOW",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--3692864881778815117-2848",
      "fields": [
        "How does VADER perform rule-based sentiment analysis?",
        "<div class=\"concept\"><strong>Concept:</strong> VADER uses a lexicon of words/emojis with sentiment scores, adjusted for rules like negation.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Scores words like a mood ring; boosts for emphasis, handles emoticons.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Lexicon dict; polarity_scores computes neg/neu/pos/compound.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Fast, no training; limited to known 7500 tokens, English-focused.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Social media analysis; quick sentiment on short texts.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> VADER: Darth Vader judging text's dark (negative) side.</div>",
        "Sentiment VADER Mechanics Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--3692864881778815117-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Sentiment",
        "VADER",
        "Mechanics",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--3482573859813434659-2848",
      "fields": [
        "Describe how Naive Bayes is used for sentiment analysis.",
        "<div class=\"concept\"><strong>Concept:</strong> Naive Bayes classifies text based on word probabilities given sentiment labels.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Learns which words predict positive/negative from labeled data, like a word detective.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Train on BOW; predict_proba for scores; assumes independence.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Simple, interpretable; poor on negation without n-grams, domain-specific.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Movie/product reviews; spam detection.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Naive Bayes: 'Naive' assumes words independent, like kids playing alone.</div>",
        "Sentiment Naive Bayes Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--3482573859813434659-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Sentiment",
        "Naive Bayes",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--3752076634109946451-2848",
      "fields": [
        "What are the trade-offs of using synonym substitution in NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> Replaces words with synonyms to normalize vocabulary, like typo correction or expansion.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Swaps similar words to broaden understanding, but risks changing nuance.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Use dictionaries or models like WordNet; apply post-tokenization.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Reduces vocab, aids data augmentation; can invert meaning (e.g., 'heart broken').</div><br><br><div class=\"applications\"><strong>Applications:</strong> Search recall, adversarial testing; avoid in precise fields like medicine.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Synonyms: Swapping twins—looks same, but one might be evil.</div>",
        "Normalization Synonym Substitution Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--3752076634109946451-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Normalization",
        "Synonym Substitution",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--5821555077926676649-2848",
      "fields": [
        "How do radicals in Chinese relate to stemming in English?",
        "<div class=\"concept\"><strong>Concept:</strong> Radicals are building blocks of Chinese characters; stemming reduces English words to roots.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Radicals hint at meaning like prefixes; but splitting Chinese often alters semantics drastically.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Chinese: No direct stemming; radicals in phono-semantic compounds. English: Porter strips suffixes.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Chinese harder to stem without meaning loss; English stemming aggressive but useful.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Chinese segmentation with Jieba; English for IR recall.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Radicals: Roots of Chinese trees; stemming prunes English branches.</div>",
        "Logographic Languages Stemming Comparison Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--5821555077926676649-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Logographic Languages",
        "Stemming",
        "Comparison",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6831502505305338127-2848",
      "fields": [
        "Why might BOW vectors be preferred over one-hot sequences for search engines?",
        "<div class=\"concept\"><strong>Concept:</strong> BOW compresses documents to count vectors; one-hot retains sequence but is sparse.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> BOW is a summary snapshot; one-hot a full movie—search needs quick overviews.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> BOW: Sum occurrences; one-hot: Matrix per document with rows as words.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> BOW faster, constant time similarity; loses order, less for generation.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Indexing in Elasticsearch; similarity with cosine distance.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> BOW: Bag vs. piano roll—one compact, one unwound.</div>",
        "Vectors BOW One-hot Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-6831502505305338127-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vectors",
        "BOW",
        "One-hot",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--6889868466118391619-2848",
      "fields": [
        "In what scenarios is lemmatization preferred over stemming for improving search precision?",
        "<div class=\"concept\"><strong>Concept:</strong> Lemmatization returns valid words considering context; stemming may produce non-words.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Lemmatization keeps meaning intact; stemming roughly groups, risking irrelevant matches.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Lemmatization uses POS; stemming rules-based like Snowball.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Lemmatization slower, needs resources; improves precision by avoiding errors.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Chatbots for accurate responses; search where false positives costly.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Lemmatization: Lemonade from lemmas—refined; stemming: Stems left after juicing.</div>",
        "Lemmatization Stemming Precision Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--6889868466118391619-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Lemmatization",
        "Stemming",
        "Precision",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8360174733859265133-2848",
      "fields": [
        "How can n-grams help in handling negation in sentiment analysis?",
        "<div class=\"concept\"><strong>Concept:</strong> n-grams capture phrases like 'not good', preserving negation context.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Single words miss 'not'; pairs glue negation to adjectives for true sentiment.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Tokenize into 2-grams; include in BOW or model features.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Increases vocab size; better accuracy for nuances like sarcasm.</div><br><br><div class=\"applications\"><strong>Applications:</strong> In VADER or Naive Bayes to differentiate 'good' vs. 'not good'.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> n-grams: 'Not' sticking to 'good' like Velcro in sentiment.</div>",
        "n-grams Sentiment Negation Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-8360174733859265133-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "n-grams",
        "Sentiment",
        "Negation",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--7397195572626962683-2848",
      "fields": [
        "What mathematical issues arise from large vocabularies in one-hot encoding?",
        "<div class=\"concept\"><strong>Concept:</strong> One-hot creates high-dimensional sparse vectors, leading to curse of dimensionality.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Huge space with mostly zeros; hard to compute distances meaningfully.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Vector length = vocab size; similarity via dot product or cosine.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Exact representation but memory-intensive; prone to overfitting.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Neural inputs; mitigated by embeddings in deep learning.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> One-hot: Vast empty stadium with one fan cheering per word.</div>",
        "Vectors One-hot Math Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--7397195572626962683-2848",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vectors",
        "One-hot",
        "Math",
        "Hard"
      ],
      "__type__": "Note"
    }
  ]
}