{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-8778",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 06 Word Embeddings",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:06 Word Embeddings",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n\n.front {\n font-weight: bold;\n color: #2c3e50;\n}\n\n.back {\n text-align: left;\n padding: 20px;\n}\n\n.concept {\n font-weight: bold;\n color: #e74c3c;\n margin-bottom: 10px;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 10px;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 10px;\n}\n\n.tradeoffs {\n color: #f39c12;\n margin-bottom: 10px;\n}\n\n.applications {\n color: #9b59b6;\n margin-bottom: 10px;\n}\n\n.memory-hook {\n background-color: #ecf0f1;\n padding: 10px;\n border-left: 4px solid #34495e;\n font-style: italic;\n color: #34495e;\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note-8606863243572557377-8778",
      "fields": [
        "What are word embeddings, and why are they useful in NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> Word embeddings are dense vector representations of words that capture their semantic meanings based on context.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Think of words as characters in a game with attributes; embeddings are like unlabeled stat sheets where similar words have similar stats.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> They are learned from large text corpora using algorithms that position words in a vector space based on co-occurrences.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> They capture nuances but can embed biases from training data and struggle with polysemy in static forms.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Used in semantic search, sentiment analysis, and machine translation.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Word embeddings are like a character's 'stat sheet' in D&D – unlabeled but aligned for easy comparison and math.</div>",
        "NLP Word Embeddings Basics",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-8606863243572557377-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Word Embeddings",
        "Basics"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--5195154768661601641-8778",
      "fields": [
        "Explain the analogy between word embeddings and brain neurons.",
        "<div class=\"concept\"><strong>Concept:</strong> Word embeddings represent patterns of neuron firings in the brain when processing words.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Words trigger connected neurons like ripples in a pond; embeddings capture these connections numerically.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Algorithms learn from word co-occurrences, mimicking neural associations without explicit labels.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Simplifies complex brain processes, missing dynamic aspects like real-time learning.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Helps in understanding language models' mimicry of human thought in AI.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Embeddings are 'brain ripples' – waves of neuron activity frozen into vectors.</div>",
        "NLP Word Embeddings Intuition",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--5195154768661601641-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Word Embeddings",
        "Intuition"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-3274284362411875023-8778",
      "fields": [
        "How do word embeddings enable semantic search?",
        "<div class=\"concept\"><strong>Concept:</strong> Semantic search uses vector similarity to match query meaning to documents, beyond keywords.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Instead of exact matches, it finds words with similar 'meanings' via vector proximity.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Compute query vector, find nearest document vectors using cosine similarity.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> More accurate than TF-IDF but requires large corpora and can be computationally intensive.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Job title searches, recommendation systems like finding similar products.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Semantic search is like a fuzzy thesaurus – closer vectors mean closer meanings.</div>",
        "NLP Applications Search",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-3274284362411875023-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Applications",
        "Search"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8051609057817796684-8778",
      "fields": [
        "Describe how to perform analogy reasoning with word vectors.",
        "<div class=\"concept\"><strong>Concept:</strong> Analogy reasoning involves vector arithmetic like king - man + woman ≈ queen.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Adding/subtracting vectors shifts meaning in semantic space.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Compute resultant vector, find nearest word in vocabulary using cosine distance.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Works well for common analogies but fails on rare words or cultural biases.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Solving SAT analogies, tip-of-tongue word finders.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Vector math is like word algebra: solve for X in 'A is to B as C is to X'.</div>",
        "NLP Word Embeddings Reasoning",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--8051609057817796684-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Word Embeddings",
        "Reasoning"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--7544474656780087790-8778",
      "fields": [
        "What is the difference between CBOW and Skip-gram in Word2Vec?",
        "<div class=\"concept\"><strong>Concept:</strong> CBOW predicts target from context; Skip-gram predicts context from target.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> CBOW averages context for word; Skip-gram uses word to guess neighbors.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Both use neural nets with hidden layer as embeddings; Skip-gram better for rare words.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> CBOW faster but less accurate for infrequent words; Skip-gram opposite.</div><br><br><div class=\"applications\"><strong>Applications:</strong> CBOW for syntax, Skip-gram for semantics in large corpora.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> CBOW is 'bag of contexts' predicting word; Skip-gram skips to predict bag from word.</div>",
        "NLP Word2Vec Models",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--7544474656780087790-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Word2Vec",
        "Models"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8072362085700530399-8778",
      "fields": [
        "Compare Word2Vec, GloVe, and fastText.",
        "<div class=\"concept\"><strong>Concept:</strong> All create embeddings; Word2Vec neural, GloVe matrix factorization, fastText subword.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Word2Vec learns locally, GloVe globally, fastText handles OOV via parts.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Word2Vec: backprop; GloVe: SVD on co-occurrence; fastText: n-grams in Skip-gram.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Word2Vec slower; GloVe efficient; fastText robust to new words but larger models.</div><br><br><div class=\"applications\"><strong>Applications:</strong> fastText for multilingual, GloVe for speed, Word2Vec for baselines.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Word2Vec: neural learner; GloVe: global optimizer; fastText: subword fixer.</div>",
        "NLP Embeddings Comparisons",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--8072362085700530399-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Embeddings",
        "Comparisons"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4092725957311383293-8778",
      "fields": [
        "How do static and contextualized embeddings differ?",
        "<div class=\"concept\"><strong>Concept:</strong> Static: fixed vectors; Contextualized: vary by sentence context.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Static packs all senses; Contextualized adapts like human understanding.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Static: Word2Vec; Contextualized: BERT uses transformers for bidirectional context.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Static simpler/faster; Contextualized more accurate but resource-heavy.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Static for quick search; Contextualized for nuanced tasks like QA.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Static: frozen photo; Contextualized: live video of word meaning.</div>",
        "NLP Embeddings Advanced",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--4092725957311383293-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Embeddings",
        "Advanced"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6747530789425406401-8778",
      "fields": [
        "Explain biases in word embeddings and how to mitigate them.",
        "<div class=\"concept\"><strong>Concept:</strong> Biases reflect training data stereotypes, e.g., gender in professions.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Garbage in, garbage out – corpora biases embed into vectors.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Measure via distances; Debias by subtracting bias subspace.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Debiasing reduces bias but may lose useful information.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Fair AI in hiring, sentiment analysis.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Embeddings mirror society's mirror – debias to straighten reflections.</div>",
        "NLP Ethics Biases",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-6747530789425406401-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Ethics",
        "Biases"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6150364395815407491-8778",
      "fields": [
        "How to visualize word vectors using PCA?",
        "<div class=\"concept\"><strong>Concept:</strong> PCA reduces dimensions while preserving variance for 2D plots.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Projects high-D shadows to see clusters of similar words.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Fit PCA on vector matrix, transform to 2D, plot with labels.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Linear, may distort nonlinear relations; Use t-SNE for better clusters.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Mapping city vectors to see semantic geography.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> PCA is a photographer finding the best angle for word vector group pics.</div>",
        "NLP Visualization Dimensionality",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-6150364395815407491-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Visualization",
        "Dimensionality"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2872155356441743481-8778",
      "fields": [
        "Describe creating a graph from sentence embeddings.",
        "<div class=\"concept\"><strong>Concept:</strong> Graph nodes as sentences, edges as similarity above threshold.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Connects similar ideas like a mind map.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Average word vectors for sentences, compute similarity matrix, use NetworkX.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Reveals clusters but threshold sensitive; Computationally heavy for large texts.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Visualizing chapter concepts, document summarization.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Sentence graph: web of thoughts where close vectors tangle together.</div>",
        "NLP Graphs Applications",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-2872155356441743481-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Graphs",
        "Applications"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7272661731755234754-8778",
      "fields": [
        "How can word embeddings handle unnatural words or IDs?",
        "<div class=\"concept\"><strong>Concept:</strong> Treat sequences as tokens if proximity implies meaning.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Even codes like zip codes embed if contexts are similar.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Tokenize IDs, train on co-occurrences in texts.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Works for structured IDs but needs large data; fastText subwords help.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Course recommendations from IDs, deciphering ciphers.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Embeddings decode 'secret languages' like kids' Pig Latin or Caesar ciphers.</div>",
        "NLP Embeddings Unnatural",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-7272661731755234754-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Embeddings",
        "Unnatural"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--56637955056677583-8778",
      "fields": [
        "What math underpins Word2Vec training?",
        "<div class=\"concept\"><strong>Concept:</strong> Neural net minimizes loss in predicting contexts.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Adjusts vectors to make similar contexts close.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Softmax output, cross-entropy loss, backprop updates embeddings.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Local optima possible; GloVe uses global SVD for better convergence.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Basis for all embedding models.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Word2Vec math: neural tug-of-war pulling words by context ropes.</div>",
        "NLP Math Word2Vec",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--56637955056677583-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Math",
        "Word2Vec"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4596880287227997144-8778",
      "fields": [
        "Connect word embeddings to AI intelligence elements per Hofstadter.",
        "<div class=\"concept\"><strong>Concept:</strong> Embeddings enable flexibility, ambiguity handling, analogies.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Gives machines 'fuzzy' word understanding like humans.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Vector space allows nuanced operations beyond rigid rules.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Mimics but not true comprehension; Risks over-attribution of intelligence.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Chatbots with flexible responses.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Embeddings: AI's 'intuition pump' for Hofstadter's elements.</div>",
        "NLP AI Connections",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--4596880287227997144-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "AI",
        "Connections"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8255168209681897949-8778",
      "fields": [
        "How to train custom domain-specific embeddings?",
        "<div class=\"concept\"><strong>Concept:</strong> Use gensim on tokenized domain corpus.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Tailor vectors to specific jargon and usages.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Set params like size, window, min_count; Train Word2Vec model.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Better accuracy in domain but less general; Needs large data.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Medical texts for doctor embeddings.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Custom embeddings: fine-tuning a universal dictionary to your niche novel.</div>",
        "NLP Training Custom",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-8255168209681897949-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Training",
        "Custom"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--5661405331533085114-8778",
      "fields": [
        "Compare LSA and Word2Vec embeddings.",
        "<div class=\"concept\"><strong>Concept:</strong> LSA: SVD on TF-IDF; Word2Vec: neural on local contexts.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> LSA global docs; Word2Vec sentence neighborhoods.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> LSA faster for long docs; Word2Vec better analogies.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> LSA efficient but less nuanced; Word2Vec data-hungry.</div><br><br><div class=\"applications\"><strong>Applications:</strong> LSA for clustering; Word2Vec for reasoning.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> LSA: broad brush; Word2Vec: fine pen for word portraits.</div>",
        "NLP Comparisons LSA",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--5661405331533085114-8778",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "NLP",
        "Comparisons",
        "LSA"
      ],
      "__type__": "Note"
    }
  ]
}