{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-2040",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 08 RNNs & LSTMs",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:08 RNNs & LSTMs",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n\n.front {\n font-weight: bold;\n color: #2c3e50;\n}\n\n.back {\n text-align: left;\n padding: 20px;\n}\n\n.concept {\n font-weight: bold;\n color: #e74c3c;\n margin-bottom: 10px;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 10px;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 10px;\n}\n\n.tradeoffs {\n color: #f39c12;\n margin-bottom: 10px;\n}\n\n.applications {\n color: #9b59b6;\n margin-bottom: 10px;\n}\n\n.memory-hook {\n background-color: #ecf0f1;\n padding: 10px;\n border-left: 4px solid #34495e;\n font-style: italic;\n color: #34495e;\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note-4704564189098261864-2040",
      "fields": [
        "What is a Recurrent Neural Network (RNN) in the context of NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> A Recurrent Neural Network (RNN) is a type of neural network designed to handle sequential data by maintaining a hidden state that captures information from previous inputs.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> RNNs 'recycle' previous computations, like reusing understanding from prior words to process the next, allowing them to remember context over sequences.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> RNNs process inputs one token at a time, updating a hidden state h_t = tanh(W_{xh} x_t + W_{hh} h_{t-1} + b), and output y_t based on h_t.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Efficient for variable-length sequences but suffers from vanishing/exploding gradients over long sequences.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Text generation, translation, sentiment analysis.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Think of RNNs as a paint roller smearing words together, building a compact 'smudge' of meaning that evolves over time.</div>",
        "RNN Definition Intuition Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-4704564189098261864-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Definition",
        "Intuition",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--9090143777972863611-2040",
      "fields": [
        "Explain the intuition behind using RNNs for NLP tasks involving sequences.",
        "<div class=\"concept\"><strong>Concept:</strong> RNNs handle sequences by reusing outputs as inputs, enabling memory of prior tokens.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Like reading a book, where understanding each word builds on previous ones, RNNs accumulate context to predict or classify.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> The hidden state acts as memory, updated recurrently: h_t depends on h_{t-1} and x_t.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Better than CNNs for unbounded sequences but computationally serial, slower for long texts.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Predicting next word in a sentence or classifying long documents.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNNs are like a storyteller who remembers the plot so far to continue the tale coherently.</div>",
        "RNN Intuition Sequences Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--9090143777972863611-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Intuition",
        "Sequences",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2489115396705077536-2040",
      "fields": [
        "What are the mathematical steps to compute the forward pass in a basic RNN?",
        "<div class=\"concept\"><strong>Concept:</strong> Forward pass in RNN involves updating hidden state and producing output for each time step.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Each step blends new input with past memory to refine understanding.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> For input x_t: combined = cat(x_t, h_{t-1}); h_t = tanh(W_{c2h} * combined); y_t = softmax(W_{c2y} * combined). Repeat for each t.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Simple but gradients vanish over time, limiting long-range dependencies.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Language modeling: predict y_t as next token probabilities.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Unroll the loop: like a chain where each link (time step) passes strengthened memory forward.</div>",
        "RNN Math Forward Pass Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-2489115396705077536-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Math",
        "Forward Pass",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-9039832761841735444-2040",
      "fields": [
        "Compare RNNs to CNNs for processing text sequences: when to use each?",
        "<div class=\"concept\"><strong>Concept:</strong> RNNs process sequences recurrently; CNNs use fixed windows (kernels).</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> RNNs remember everything (in theory); CNNs scan local patterns efficiently.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> RNN: serial updates over time; CNN: parallel convolutions over n-grams.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> RNNs handle variable lengths better but slower; CNNs faster but fixed context window.</div><br><br><div class=\"applications\"><strong>Applications:</strong> RNN for generation/translation; CNN for classification on short texts.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNNs are marathon runners building endurance; CNNs are sprinters spotting quick patterns.</div>",
        "RNN CNN Comparison Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-9039832761841735444-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "CNN",
        "Comparison",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--677323456048151694-2040",
      "fields": [
        "What is backpropagation through time (BPTT) in RNNs?",
        "<div class=\"concept\"><strong>Concept:</strong> BPTT is the algorithm to compute gradients in RNNs by unrolling the network over time steps.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Error flows backward through the sequence, adjusting weights for each past decision.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Unroll RNN into a deep feedforward net; apply chain rule: δh_t / δW = sum over paths from output to t.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Captures temporal dependencies but prone to vanishing gradients; truncated BPTT limits computation.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Training RNNs for sequence prediction.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like rewinding a movie to fix plot holes, frame by frame.</div>",
        "RNN BPTT Math Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--677323456048151694-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "BPTT",
        "Math",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8794677524380584028-2040",
      "fields": [
        "How do RNNs handle one-to-many, many-to-one, and many-to-many mappings in NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> RNNs adapt to input/output sequence lengths: one input to sequence output, sequence to one, or sequence to sequence.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Flexible for varying data shapes, like generating text from a seed or tagging each word.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> One-to-many: repeat input hidden; many-to-one: pool final hidden; many-to-many: output per step.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Versatile but requires careful handling of variable lengths (padding).</div><br><br><div class=\"applications\"><strong>Applications:</strong> One-to-many: image captioning; many-to-one: sentiment; many-to-many: translation.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNNs as transformers: shrink/expand sequences like an accordion.</div>",
        "RNN Applications Mappings Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-8794677524380584028-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Applications",
        "Mappings",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--1540675312389236635-2040",
      "fields": [
        "Describe the hidden state in RNNs and its role in memory.",
        "<div class=\"concept\"><strong>Concept:</strong> Hidden state is a vector that encodes cumulative information from the sequence so far.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> It's the RNN's 'memory bank,' updating with each new token.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> h_t = activation(W_xh x_t + W_hh h_{t-1} + b_h).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Compact representation but forgets distant info due to vanishing gradients.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Used for final classification or intermediate predictions.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Like a snowball rolling, gathering mass (meaning) but melting old layers.</div>",
        "RNN Hidden State Intuition Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--1540675312389236635-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Hidden State",
        "Intuition",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-4624114796697230412-2040",
      "fields": [
        "How would you implement a basic RNN in PyTorch from scratch?",
        "<div class=\"concept\"><strong>Concept:</strong> Custom RNN module using linear layers for recurrence.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Build a loop that reuses hidden state without built-in RNN class.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> class RNN(nn.Module): init Linear for input+hidden to hidden/output; forward: cat(input, hidden), compute new hidden/output.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Educational but less efficient than torch.nn.RNN.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Character-level prediction, like surname nationality.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> DIY RNN: like building a bicycle chain link by link.</div>",
        "RNN PyTorch Implementation Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-4624114796697230412-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "PyTorch",
        "Implementation",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-3425413144370656834-2040",
      "fields": [
        "In predicting nationality from surnames, what challenges arise and how does an RNN address them?",
        "<div class=\"concept\"><strong>Concept:</strong> Character-level RNN classifies sequences based on patterns.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Learns letter patterns unique to cultures, despite overlaps.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Process name char-by-char, final hidden to softmax for nationality.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Handles ambiguity but accuracy limited by dataset overlaps/duplicates.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Anonymization, name generation.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNN as a cultural detective, piecing clues from letters.</div>",
        "RNN Application Surname Prediction Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-3425413144370656834-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Application",
        "Surname Prediction",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-4713377810924885076-2040",
      "fields": [
        "Explain vanishing/exploding gradients in RNNs and connections to long sequences.",
        "<div class=\"concept\"><strong>Concept:</strong> Gradients diminish (vanish) or amplify (explode) during BPTT over long sequences.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Memory fades or overwhelms, like whispers in a long tunnel.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Repeated multiplication by |W_hh| <1 (vanish) or >1 (explode).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Limits long-term dependencies; mitigated by LSTMs/GRUs.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Affects training on paragraphs vs. sentences.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Gradients as echoes: too quiet or too loud in a cave.</div>",
        "RNN Gradients Theory Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-4713377810924885076-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Gradients",
        "Theory",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--6297545636295500089-2040",
      "fields": [
        "What are Gated Recurrent Units (GRUs) and how do they improve RNNs?",
        "<div class=\"concept\"><strong>Concept:</strong> GRUs are RNN variants with reset/update gates to manage memory.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Gates decide what to forget/update, preserving relevant info longer.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> r = sig(Wxr x + Whr h); z = sig(Wxz x + Whz h); n = tanh(Wxn x + r*(Whn h)); h' = (1-z)*n + z*h.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Simpler than LSTMs, fewer params, but may underperform on very complex tasks.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Efficient language modeling, sequence prediction.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> GRUs as smart filters: reset junk, update essentials.</div>",
        "GRU Gates Improvement Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--6297545636295500089-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "GRU",
        "Gates",
        "Improvement",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8494021760874386989-2040",
      "fields": [
        "Derive the update equations for an LSTM cell.",
        "<div class=\"concept\"><strong>Concept:</strong> LSTM uses forget/input/output gates and cell state for long-term memory.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Separate short (hidden) and long (cell) memory, with gates controlling flow.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> f = sig(Wf x + Uf h); i = sig(Wi x + Ui h); o = sig(Wo x + Uo h); c~ = tanh(Wc x + Uc h); c = f*c + i*c~; h = o*tanh(c).</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> More params than GRUs, slower but better at long dependencies.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Advanced translation, summarization.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> LSTMs as vaults: forget old, input new, output wisely.</div>",
        "LSTM Math Equations Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-8494021760874386989-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "LSTM",
        "Math",
        "Equations",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8184987535805595560-2040",
      "fields": [
        "How do LSTMs and GRUs connect to vanilla RNNs in terms of architecture?",
        "<div class=\"concept\"><strong>Concept:</strong> LSTMs/GRUs extend RNNs with gates to handle long-term dependencies.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Add 'smarts' to memory management, preventing fade-out.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Both use hidden state recurrence; gates modulate updates.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Increased complexity/capacity vs. vanilla RNN simplicity.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Use LSTMs for deep reasoning; RNNs for short sequences.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Vanilla RNN as basic phone; LSTMs as smartphone with apps (gates).</div>",
        "LSTM GRU RNN Connections Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-8184987535805595560-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "LSTM",
        "GRU",
        "RNN",
        "Connections",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-1116751510755474689-2040",
      "fields": [
        "What trade-offs exist between RNN types (vanilla, GRU, LSTM) for NLP tasks?",
        "<div class=\"concept\"><strong>Concept:</strong> Vanilla RNNs are basic; GRUs/LSTMs add gates for better memory.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Balance simplicity/speed with memory depth.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Vanilla: ~n params; GRU: ~3n; LSTM: ~4n per unit.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Vanilla fastest but forgets; LSTM best memory but slowest.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Vanilla for chars; LSTM for paragraphs.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Cars: vanilla as bicycle (simple), GRU as sedan, LSTM as SUV (more capacity).</div>",
        "RNN Variants Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-1116751510755474689-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN Variants",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4415043790276039319-2040",
      "fields": [
        "How can RNNs be used for text generation?",
        "<div class=\"concept\"><strong>Concept:</strong> Sample from predicted probabilities to generate sequences.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Predict next token repeatedly, building text from a prompt.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Start with seed; feed output as next input; use temperature for randomness.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Creative but may loop or nonsensical without tuning.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Chatbots, story writing.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNN as improv actor: continues the scene based on cues.</div>",
        "RNN Generation Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--4415043790276039319-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Generation",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--3403996807622304140-2040",
      "fields": [
        "What is the difference between multiclass classification and multi-label tagging in RNNs?",
        "<div class=\"concept\"><strong>Concept:</strong> Multiclass: one exclusive label; multi-label: multiple independent labels.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> One answer vs. multiple tags, like single vs. multiple checkboxes.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Multiclass: softmax; multi-label: sigmoid per label.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Multi-label handles ambiguity but needs threshold tuning.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Multiclass: sentiment; multi-label: topic tagging.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Multiclass as radio buttons; multi-label as checkboxes.</div>",
        "Classification Tagging RNN Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--3403996807622304140-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Classification",
        "Tagging",
        "RNN",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8873177999457929605-2040",
      "fields": [
        "How does hyperparameter tuning affect RNN performance in language modeling?",
        "<div class=\"concept\"><strong>Concept:</strong> Tune layers, hidden size, LR, dropout for optimal loss.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Balance capacity and regularization to avoid over/underfitting.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Grid search: e.g., more layers increase depth but time; dropout reduces overfitting.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Deeper models better but slower; high LR speeds but unstable.</div><br><br><div class=\"applications\"><strong>Applications:</strong> WikiText-2 benchmarking.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Tuning as cooking: right spices (params) for perfect flavor (accuracy).</div>",
        "Hyperparameters Tuning RNN Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--8873177999457929605-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Hyperparameters",
        "Tuning",
        "RNN",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-5845195329254527885-2040",
      "fields": [
        "Explain batchification for training word-level RNNs.",
        "<div class=\"concept\"><strong>Concept:</strong> Split long sequence into parallel batches for efficient training.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Process multiple subsequences simultaneously, like reading columns.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Divide corpus into batch_size columns; train on slices.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Speeds GPU use but assumes independent batches.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Large corpora like WikiText-2.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> Batchify as slicing a long rope into parallel strands.</div>",
        "RNN Batchification Mechanics Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-5845195329254527885-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Batchification",
        "Mechanics",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2178765048029081342-2040",
      "fields": [
        "What ethical considerations arise from using RNNs for name nationality prediction?",
        "<div class=\"concept\"><strong>Concept:</strong> Models can perpetuate biases or enable discrimination.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Names carry cultural signals; misusing predicts harm.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Train on diverse data; anonymize outputs.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Useful for anonymization but risky for profiling.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Ethical: dataset debiasing; unethical: targeted ads/discrimination.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNN as a mirror: reflects dataset biases, so clean the glass.</div>",
        "Ethics RNN Application Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-2178765048029081342-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Ethics",
        "RNN",
        "Application",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--2376545962876058830-2040",
      "fields": [
        "How do RNNs connect to neuromorphic computing in NLP?",
        "<div class=\"concept\"><strong>Concept:</strong> RNNs mimic brain-like recurrence for processing sequences.</div><br><br><div class=\"intuition\"><strong>Intuition:</strong> Brains recycle thoughts; RNNs recycle states for text understanding.</div><br><br><div class=\"mechanics\"><strong>Mechanics:</strong> Hidden state as neural memory, updated over time.</div><br><br><div class=\"tradeoffs\"><strong>Trade-offs:</strong> Brain-inspired efficiency but not fully biological.</div><br><br><div class=\"applications\"><strong>Applications:</strong> Real-time transcription, inspired by human reading.</div><br><br><div class=\"memory-hook\"><strong>Memory Hook:</strong> RNNs as mini-brains: recurrent loops like thought cycles.</div>",
        "RNN Neuromorphic Connections Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--2376545962876058830-2040",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "RNN",
        "Neuromorphic",
        "Connections",
        "Medium"
      ],
      "__type__": "Note"
    }
  ]
}