{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-evaluation-metrics-2024",
  "deck_config_uuid": "nlp-comprehensive-config",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-comprehensive-config",
      "dyn": false,
      "name": "Evaluation Metrics",
      "new": {
        "bury": true,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 15,
        "separate": true
      },
      "rev": {
        "bury": true,
        "ease4": 1.3,
        "fuzz": 0.05,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "minSpace": 1,
        "perDay": 100
      }
    }
  ],
  "desc": "Measuring and comparing NLP system performance",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "Evaluation Metrics",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-comprehensive-model",
      "css": "\n.card {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-size: 16px;\n    line-height: 1.4;\n    color: #333;\n    background-color: #fafafa;\n    padding: 20px;\n    max-width: 500px;\n    margin: 0 auto;\n}\n\n.front { background-color: #e3f2fd; }\n.back { background-color: #f3e5f5; }\n\nb { color: #1976d2; font-weight: 600; }\n.formula { font-family: 'Courier New', monospace; background: #f5f5f5; padding: 2px 4px; }\n.example { font-style: italic; color: #555; }\n                    ",
      "flds": [
        {
          "__type__": "NoteModelField",
          "name": "Front",
          "ord": 0,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Back",
          "ord": 1,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Topic",
          "ord": 2,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Type",
          "ord": 3,
          "sticky": false
        }
      ],
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "any",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "__type__": "CardTemplate",
          "afmt": "<div class=\"card back\">{{Front}}<hr>{{Back}}<br><br><small><i>{{Topic}} • {{Type}}</i></small></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card",
          "ord": 0,
          "qfmt": "<div class=\"card front\">{{Front}}</div>"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "What are precision, recall, and F1 score?",
        "<b>Precision:</b> TP/(TP+FP) - fraction of retrieved items that are relevant<br>\n<b>Recall:</b> TP/(TP+FN) - fraction of relevant items that are retrieved<br>\n<b>F1:</b> 2×(P×R)/(P+R) - harmonic mean of precision and recall<br>\n<b>Trade-off:</b> Precision vs recall, F1 balances both",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_ceccecba",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is BLEU score and what does it measure?",
        "<b>Task:</b> Evaluate machine translation quality<br>\n<b>Method:</b> N-gram overlap between generated and reference translations<br>\n<b>Components:</b> Modified precision + brevity penalty<br>\n<b>Limitations:</b> Only lexical overlap, ignores semantic similarity",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_b07db0ee",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is ROUGE and when is it used?",
        "<b>Task:</b> Evaluate automatic summarization systems<br>\n<b>Types:</b> ROUGE-N (n-gram overlap), ROUGE-L (longest common subsequence)<br>\n<b>Comparison:</b> Generated summary vs reference summaries<br>\n<b>Advantage:</b> ROUGE-L captures word order, not just word overlap",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_f25a1f5c",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "How do you evaluate language models?",
        "<b>Perplexity:</b> PP = P(test_set)^(-1/N) - lower is better<br>\n<b>Cross-entropy:</b> Average negative log probability<br>\n<b>Bits per character:</b> log₂(perplexity) for character-level models<br>\n<b>Downstream:</b> Performance on specific tasks (classification, generation)",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_40f7e3b5",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is accuracy vs macro/micro-averaged F1?",
        "<b>Accuracy:</b> Correct predictions / total predictions<br>\n<b>Macro-F1:</b> Average F1 across classes (equal weight per class)<br>\n<b>Micro-F1:</b> F1 computed globally (equal weight per instance)<br>\n<b>Use:</b> Macro for balanced view, micro when class sizes vary",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_5f0cd127",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is inter-annotator agreement and how is it measured?",
        "<b>Purpose:</b> Measure consistency between human annotators<br>\n<b>Cohen's kappa:</b> Agreement beyond chance for two annotators<br>\n<b>Fleiss' kappa:</b> Extension to multiple annotators<br>\n<b>Interpretation:</b> κ > 0.8 excellent, 0.6-0.8 good, < 0.6 questionable",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_c674e167",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the difference between automatic and human evaluation?",
        "<b>Automatic:</b> Fast, consistent, scalable but limited to surface features<br>\n<b>Human:</b> Captures semantic quality but expensive and subjective<br>\n<b>Best practice:</b> Use automatic for development, human for final evaluation<br>\n<b>Modern:</b> Learned metrics (BERTScore) bridge automatic and human evaluation",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_47dc1cee",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is statistical significance testing in NLP evaluation?",
        "<b>Purpose:</b> Determine if performance difference is statistically meaningful<br>\n<b>Methods:</b> Bootstrap resampling, permutation test, t-test<br>\n<b>Multiple comparisons:</b> Bonferroni correction when testing many systems<br>\n<b>Reporting:</b> Always report confidence intervals and significance tests",
        "Evaluation Metrics",
        "concept"
      ],
      "guid": "nlp_5100c8b8",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "evaluation_metrics",
        "concept"
      ]
    }
  ]
}