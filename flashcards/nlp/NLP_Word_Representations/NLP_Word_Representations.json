{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-word-representations-2024",
  "deck_config_uuid": "nlp-comprehensive-config",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-comprehensive-config",
      "dyn": false,
      "name": "Word Representations",
      "new": {
        "bury": true,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 15,
        "separate": true
      },
      "rev": {
        "bury": true,
        "ease4": 1.3,
        "fuzz": 0.05,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "minSpace": 1,
        "perDay": 100
      }
    }
  ],
  "desc": "Converting words to numerical vectors for computation",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "Word Representations",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-comprehensive-model",
      "css": "\n.card {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-size: 16px;\n    line-height: 1.4;\n    color: #333;\n    background-color: #fafafa;\n    padding: 20px;\n    max-width: 500px;\n    margin: 0 auto;\n}\n\n.front { background-color: #e3f2fd; }\n.back { background-color: #f3e5f5; }\n\nb { color: #1976d2; font-weight: 600; }\n.formula { font-family: 'Courier New', monospace; background: #f5f5f5; padding: 2px 4px; }\n.example { font-style: italic; color: #555; }\n                    ",
      "flds": [
        {
          "__type__": "NoteModelField",
          "name": "Front",
          "ord": 0,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Back",
          "ord": 1,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Topic",
          "ord": 2,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Type",
          "ord": 3,
          "sticky": false
        }
      ],
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "any",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "__type__": "CardTemplate",
          "afmt": "<div class=\"card back\">{{Front}}<hr>{{Back}}<br><br><small><i>{{Topic}} • {{Type}}</i></small></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card",
          "ord": 0,
          "qfmt": "<div class=\"card front\">{{Front}}</div>"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "What is TF-IDF and how is it calculated?",
        "<b>Formula:</b> TF-IDF = TF(t,d) × IDF(t)<br>\n<b>TF:</b> Term frequency in document<br>\n<b>IDF:</b> log(N/df) where N=total docs, df=docs containing term<br>\n<b>Intuition:</b> Important words appear often in few documents",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_4ab59467",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the intuition behind Word2Vec?",
        "<b>Hypothesis:</b> Words in similar contexts have similar meanings<br>\n<b>Methods:</b> Skip-gram (predict context from word), CBOW (predict word from context)<br>\n<b>Training:</b> Neural network with negative sampling<br>\n<b>Result:</b> Dense vectors where similar words are close in space",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_c25a8b1f",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the difference between Word2Vec and GloVe?",
        "<b>Word2Vec:</b> Local context window, neural network training<br>\n<b>GloVe:</b> Global co-occurrence statistics, matrix factorization approach<br>\n<b>Advantage:</b> GloVe uses all corpus statistics, Word2Vec is more scalable<br>\n<b>Result:</b> Both produce similar quality embeddings",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_8c1d0f4d",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What makes FastText different from Word2Vec?",
        "<b>Key difference:</b> FastText learns subword representations<br>\n<b>Method:</b> Word represented as sum of character n-gram vectors<br>\n<b>Advantage:</b> Handles rare words and morphological variations<br>\n<b>Example:</b> \"running\" uses info from \"run\", \"runn\", \"unni\", \"nning\"",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_c1bfee16",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What are contextual vs static word embeddings?",
        "<b>Static:</b> One vector per word regardless of context (Word2Vec, GloVe)<br>\n<b>Contextual:</b> Different vectors based on context (ELMo, BERT)<br>\n<b>Advantage:</b> Contextual handles polysemy (\"bank\" = financial vs river)<br>\n<b>Trade-off:</b> Contextual more accurate but computationally expensive",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_cffe8463",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the curse of dimensionality in word embeddings?",
        "<b>Problem:</b> High-dimensional vectors become sparse and hard to compare<br>\n<b>Solution:</b> Typical embeddings use 100-1000 dimensions, not higher<br>\n<b>Trade-off:</b> More dimensions capture more info but cause sparsity<br>\n<b>Optimal:</b> 300-500 dimensions common for most applications",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_729d6091",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "How do you evaluate word embedding quality?",
        "<b>Intrinsic:</b> Word similarity tasks, analogy tasks (king - man + woman = queen)<br>\n<b>Extrinsic:</b> Performance on downstream tasks (classification, NER)<br>\n<b>Metrics:</b> Spearman correlation with human judgments<br>\n<b>Best practice:</b> Evaluate on target application, not just benchmarks",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_ea05e53d",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is negative sampling in Word2Vec?",
        "<b>Problem:</b> Softmax over large vocabulary is computationally expensive<br>\n<b>Solution:</b> Instead of all words, sample few negative examples<br>\n<b>Method:</b> For each positive pair, sample k negative words<br>\n<b>Efficiency:</b> Reduces computation from V to k+1 (where k≈5-20)",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_c7bc2eae",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is subword tokenization and why is it useful for embeddings?",
        "<b>Methods:</b> BPE, WordPiece, character n-grams<br>\n<b>Benefits:</b> Handle OOV words, reduce vocabulary size, share info<br>\n<b>Principle:</b> Rare words broken into frequent subword units<br>\n<b>Example:</b> \"unhappiness\" → [\"un\", \"happy\", \"ness\"] shares with \"happy\"",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_5d064e9f",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the difference between dense and sparse representations?",
        "<b>Sparse:</b> Most values are zero (one-hot, bag-of-words)<br>\n<b>Dense:</b> Most values are non-zero (word embeddings)<br>\n<b>Advantages:</b> Dense captures similarity, sparse is interpretable<br>\n<b>Efficiency:</b> Dense better for neural networks, sparse for traditional ML",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_9c7f9167",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "How do you handle out-of-vocabulary (OOV) words?",
        "<b>Static embeddings:</b> UNK token, subword fallback<br>\n<b>Subword models:</b> Decompose into known pieces (FastText, BPE)<br>\n<b>Character-level:</b> Build representation from characters<br>\n<b>Contextual:</b> BERT/GPT can handle unseen words via WordPiece",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_8cd98638",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the distributional hypothesis?",
        "<b>Principle:</b> \"You shall know a word by the company it keeps\"<br>\n<b>Meaning:</b> Words appearing in similar contexts have similar meanings<br>\n<b>Foundation:</b> Basis for all modern word embedding methods<br>\n<b>Evidence:</b> Words like \"dog\" and \"cat\" appear near \"pet\", \"animal\", \"feed\"",
        "Word Representations",
        "concept"
      ],
      "guid": "nlp_c1b1da3c",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "word_representations",
        "concept"
      ]
    }
  ]
}