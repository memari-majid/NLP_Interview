{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-modern-neural-architectures-2024",
  "deck_config_uuid": "nlp-comprehensive-config",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-comprehensive-config",
      "dyn": false,
      "name": "Modern Neural Architectures",
      "new": {
        "bury": true,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 15,
        "separate": true
      },
      "rev": {
        "bury": true,
        "ease4": 1.3,
        "fuzz": 0.05,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "minSpace": 1,
        "perDay": 100
      }
    }
  ],
  "desc": "Transformer-based models and attention mechanisms",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "Modern Neural Architectures",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-comprehensive-model",
      "css": "\n.card {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-size: 16px;\n    line-height: 1.4;\n    color: #333;\n    background-color: #fafafa;\n    padding: 20px;\n    max-width: 500px;\n    margin: 0 auto;\n}\n\n.front { background-color: #e3f2fd; }\n.back { background-color: #f3e5f5; }\n\nb { color: #1976d2; font-weight: 600; }\n.formula { font-family: 'Courier New', monospace; background: #f5f5f5; padding: 2px 4px; }\n.example { font-style: italic; color: #555; }\n                    ",
      "flds": [
        {
          "__type__": "NoteModelField",
          "name": "Front",
          "ord": 0,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Back",
          "ord": 1,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Topic",
          "ord": 2,
          "sticky": false
        },
        {
          "__type__": "NoteModelField",
          "name": "Type",
          "ord": 3,
          "sticky": false
        }
      ],
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "any",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "__type__": "CardTemplate",
          "afmt": "<div class=\"card back\">{{Front}}<hr>{{Back}}<br><br><small><i>{{Topic}} • {{Type}}</i></small></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card",
          "ord": 0,
          "qfmt": "<div class=\"card front\">{{Front}}</div>"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "What is self-attention and how does it work?",
        "<b>Intuition:</b> Weigh tokens by relevance to each other in sequence<br>\n<b>Formula:</b> Attention(Q,K,V) = softmax(QK^T/√d_k)V<br>\n<b>Symbols:</b> Q=query, K=key, V=value, d_k=key dimension<br>\n<b>Scaling:</b> √d_k prevents vanishing gradients in softmax",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_702ecf3a",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Why use attention instead of RNNs?",
        "<b>Parallelization:</b> All positions computed simultaneously, not sequentially<br>\n<b>Long dependencies:</b> Direct connections between distant tokens<br>\n<b>No forgetting:</b> No information bottleneck like RNN hidden state<br>\n<b>Interpretability:</b> Attention weights show what model focuses on",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_a9e3e2b0",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the Transformer architecture?",
        "<b>Components:</b> Encoder-decoder with self-attention and feed-forward layers<br>\n<b>Key features:</b> Multi-head attention, residual connections, layer normalization<br>\n<b>No recurrence:</b> Relies entirely on attention mechanisms<br>\n<b>Positional:</b> Uses positional encoding since attention is position-agnostic",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_f34860fe",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is multi-head attention?",
        "<b>Concept:</b> Run h attention functions in parallel with different learned projections<br>\n<b>Formula:</b> MultiHead(Q,K,V) = Concat(head₁,...,headₕ)W^O<br>\n<b>Benefit:</b> Attend to different representation subspaces simultaneously<br>\n<b>Typical:</b> h=8 or h=12 heads in most transformer models",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_bcd483e6",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is BERT and how does it work?",
        "<b>Architecture:</b> Bidirectional encoder-only transformer<br>\n<b>Training:</b> Masked Language Model + Next Sentence Prediction<br>\n<b>Innovation:</b> Deep bidirectional representations (not left-to-right)<br>\n<b>Usage:</b> Pre-train on large corpus, fine-tune for specific tasks",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_4c455dae",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the difference between BERT and GPT?",
        "<b>BERT:</b> Bidirectional encoder, masked language modeling<br>\n<b>GPT:</b> Unidirectional decoder, autoregressive generation<br>\n<b>Tasks:</b> BERT better for understanding, GPT better for generation<br>\n<b>Training:</b> BERT sees full context, GPT only sees previous context",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_641e37f9",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is masked language modeling?",
        "<b>Task:</b> Predict randomly masked tokens using bidirectional context<br>\n<b>Masking:</b> 15% of tokens - 80% [MASK], 10% random, 10% unchanged<br>\n<b>Advantage:</b> Learns bidirectional representations<br>\n<b>Used by:</b> BERT, RoBERTa, ELECTRA for pre-training",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_b20db1d1",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is positional encoding in transformers?",
        "<b>Purpose:</b> Add position information since attention is permutation-invariant<br>\n<b>Methods:</b> Sinusoidal encoding, learned embeddings<br>\n<b>Formula:</b> PE(pos,2i) = sin(pos/10000^(2i/d)), PE(pos,2i+1) = cos(pos/10000^(2i/d))<br>\n<b>Benefit:</b> Model can distinguish \"cat chased dog\" vs \"dog chased cat\"",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_bd255fab",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is transfer learning in NLP?",
        "<b>Approach:</b> Pre-train on large corpus, fine-tune on specific task<br>\n<b>Benefits:</b> Better performance, less task-specific data needed<br>\n<b>Examples:</b> BERT for classification, GPT for generation<br>\n<b>Revolution:</b> Transformed NLP from task-specific to general-purpose models",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_dcadf471",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the attention mechanism advantage over CNNs?",
        "<b>Receptive field:</b> Immediate global connections vs local then global<br>\n<b>Variable length:</b> Natural handling of sequences vs fixed windows<br>\n<b>Interpretability:</b> Attention weights show focus vs black box<br>\n<b>Efficiency:</b> Parallel computation vs hierarchical processing",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_79544f5b",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is layer normalization in transformers?",
        "<b>Purpose:</b> Stabilize training by normalizing layer inputs<br>\n<b>Formula:</b> LayerNorm(x) = γ(x-μ)/σ + β<br>\n<b>Location:</b> Applied before attention and feed-forward layers<br>\n<b>Benefit:</b> Faster convergence, less sensitive to initialization",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_de53f4d1",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the feed-forward network in transformers?",
        "<b>Structure:</b> Two linear transformations with ReLU activation<br>\n<b>Formula:</b> FFN(x) = max(0, xW₁ + b₁)W₂ + b₂<br>\n<b>Dimension:</b> Usually 4x the model dimension (512 → 2048)<br>\n<b>Purpose:</b> Add non-linearity and position-wise processing",
        "Modern Neural Architectures",
        "concept"
      ],
      "guid": "nlp_e7de7404",
      "note_model_uuid": "nlp-comprehensive-model",
      "tags": [
        "modern_neural_architectures",
        "concept"
      ]
    }
  ]
}