{
  "meta": {
    "title": "NLP Theory Interview Flashcards",
    "description": "Comprehensive theoretical NLP concepts for ML interviews",
    "version": "1.0",
    "categories": [
      "fundamentals",
      "embeddings",
      "attention",
      "transformers",
      "llms",
      "evaluation",
      "preprocessing",
      "classical_nlp",
      "applications"
    ]
  },
  "flashcards": [
    {
      "id": "nlp_001",
      "category": "fundamentals",
      "difficulty": "easy",
      "question": "What is tokenization in NLP?",
      "answer": "Tokenization is the process of breaking text into smaller units (tokens) such as words, subwords, or characters. It's the first step in most NLP pipelines.",
      "key_points": [
        "Word tokenization: splits on spaces/punctuation",
        "Subword tokenization: BPE, WordPiece, SentencePiece",
        "Character tokenization: individual characters as tokens"
      ],
      "follow_up": "What are the trade-offs between different tokenization methods?"
    },
    {
      "id": "nlp_002",
      "category": "fundamentals",
      "difficulty": "easy",
      "question": "What is the difference between stemming and lemmatization?",
      "answer": "Stemming cuts off word endings using rules (faster but crude), while lemmatization uses vocabulary and morphological analysis to return the base form (slower but accurate).",
      "examples": {
        "stemming": "running → runn, better → bett",
        "lemmatization": "running → run, better → good"
      },
      "when_to_use": "Stemming for speed/indexing, lemmatization for accuracy/understanding"
    },
    {
      "id": "nlp_003",
      "category": "embeddings",
      "difficulty": "medium",
      "question": "Explain Word2Vec and its two architectures",
      "answer": "Word2Vec learns dense vector representations of words based on their context. Two architectures: CBOW (predicts word from context) and Skip-gram (predicts context from word).",
      "details": {
        "CBOW": "Faster, better for frequent words, predicts center word from context window",
        "Skip-gram": "Better for rare words, predicts context words from center word",
        "training": "Uses negative sampling or hierarchical softmax for efficiency"
      },
      "dimension_typical": "50-300 dimensions"
    },
    {
      "id": "nlp_004",
      "category": "embeddings",
      "difficulty": "medium",
      "question": "What are the limitations of Word2Vec?",
      "answer": "1) Out-of-vocabulary (OOV) problem, 2) Single representation per word (no polysemy), 3) Doesn't capture word order, 4) No subword information",
      "solutions": {
        "OOV": "FastText with subword embeddings",
        "Polysemy": "Contextualized embeddings (ELMo, BERT)",
        "Word order": "Position embeddings, RNNs, Transformers"
      }
    },
    {
      "id": "nlp_005",
      "category": "embeddings",
      "difficulty": "hard",
      "question": "Compare GloVe vs Word2Vec",
      "answer": "GloVe uses global word co-occurrence statistics while Word2Vec uses local context windows. GloVe combines count-based and predictive methods.",
      "comparison": {
        "Word2Vec": "Local context, predictive, online learning possible",
        "GloVe": "Global statistics, count-based + predictive, batch learning",
        "Performance": "Similar quality, GloVe often faster training on large corpora"
      }
    },
    {
      "id": "nlp_006",
      "category": "attention",
      "difficulty": "medium",
      "question": "What is the attention mechanism and why is it important?",
      "answer": "Attention allows models to focus on relevant parts of input when producing output. It solves the information bottleneck problem in seq2seq models.",
      "formula": "Attention(Q,K,V) = softmax(QK^T/√d_k)V",
      "benefits": [
        "Handles long sequences better",
        "Provides interpretability",
        "Enables parallelization",
        "Captures long-range dependencies"
      ]
    },
    {
      "id": "nlp_007",
      "category": "attention",
      "difficulty": "hard",
      "question": "Explain self-attention vs cross-attention",
      "answer": "Self-attention: Q, K, V from same sequence (e.g., BERT encoder). Cross-attention: Q from one sequence, K, V from another (e.g., decoder attending to encoder).",
      "use_cases": {
        "self_attention": "Understanding context within a sentence, BERT-style models",
        "cross_attention": "Translation, summarization, encoder-decoder models"
      }
    },
    {
      "id": "nlp_008",
      "category": "attention",
      "difficulty": "hard",
      "question": "What is multi-head attention and why use it?",
      "answer": "Multi-head attention runs multiple attention operations in parallel with different learned projections, then concatenates results. Captures different types of relationships.",
      "formula": "MultiHead = Concat(head_1, ..., head_h)W^O",
      "benefits": [
        "Attends to different positions",
        "Captures different relationship types",
        "More expressive than single attention",
        "Typical: 8-16 heads"
      ]
    },
    {
      "id": "nlp_009",
      "category": "transformers",
      "difficulty": "medium",
      "question": "What are positional encodings and why are they needed?",
      "answer": "Transformers have no inherent notion of sequence order. Positional encodings add position information to embeddings so the model knows word order.",
      "types": {
        "sinusoidal": "sin/cos functions of different frequencies (original Transformer)",
        "learned": "Trainable position embeddings (BERT, GPT)",
        "relative": "Encodes relative positions (T5, Transformer-XL)"
      }
    },
    {
      "id": "nlp_010",
      "category": "transformers",
      "difficulty": "hard",
      "question": "Compare BERT vs GPT architectures",
      "answer": "BERT: Bidirectional encoder, masked language modeling, good for understanding. GPT: Autoregressive decoder, causal masking, good for generation.",
      "details": {
        "BERT": "Encoder-only, sees full context, MLM + NSP training, classification/NER tasks",
        "GPT": "Decoder-only, left-to-right, next token prediction, generation tasks",
        "attention_mask": "BERT: no masking, GPT: causal (triangular) mask"
      }
    },
    {
      "id": "nlp_011",
      "category": "transformers",
      "difficulty": "medium",
      "question": "What is the difference between encoder, decoder, and encoder-decoder transformers?",
      "answer": "Encoder: bidirectional context (BERT), Decoder: autoregressive/causal (GPT), Encoder-Decoder: seq2seq tasks (T5, BART)",
      "use_cases": {
        "encoder": "Classification, NER, sentiment analysis",
        "decoder": "Text generation, completion",
        "encoder_decoder": "Translation, summarization, Q&A"
      }
    },
    {
      "id": "nlp_012",
      "category": "llms",
      "difficulty": "easy",
      "question": "What is fine-tuning vs prompting in LLMs?",
      "answer": "Fine-tuning updates model weights on task-specific data. Prompting uses the frozen model with carefully designed input prompts.",
      "comparison": {
        "fine_tuning": "Better performance, requires data & compute, task-specific model",
        "prompting": "No training, immediate use, single model for many tasks",
        "few_shot": "Middle ground - examples in prompt without weight updates"
      }
    },
    {
      "id": "nlp_013",
      "category": "llms",
      "difficulty": "medium",
      "question": "What is in-context learning?",
      "answer": "The ability of LLMs to learn from examples provided in the prompt without updating weights. Includes zero-shot, one-shot, and few-shot learning.",
      "types": {
        "zero_shot": "Task description only",
        "one_shot": "One example + task",
        "few_shot": "Multiple examples + task"
      },
      "emergent": "Ability emerges with scale (>10B parameters typically)"
    },
    {
      "id": "nlp_014",
      "category": "llms",
      "difficulty": "hard",
      "question": "Explain instruction tuning and RLHF",
      "answer": "Instruction tuning: fine-tuning on (instruction, output) pairs. RLHF: using human feedback to train a reward model, then optimizing via reinforcement learning.",
      "pipeline": {
        "1_SFT": "Supervised fine-tuning on demonstrations",
        "2_Reward": "Train reward model on human preferences",
        "3_PPO": "Optimize policy with PPO against reward model"
      },
      "examples": "InstructGPT, ChatGPT, Claude"
    },
    {
      "id": "nlp_015",
      "category": "llms",
      "difficulty": "medium",
      "question": "What are common decoding strategies for text generation?",
      "answer": "Greedy (max probability), Beam search (top-k paths), Sampling methods (top-k, top-p/nucleus, temperature)",
      "details": {
        "greedy": "Deterministic, can be repetitive",
        "beam_search": "Better quality, still deterministic",
        "top_k": "Sample from k most likely tokens",
        "top_p": "Sample from smallest set with cumulative prob > p",
        "temperature": "Controls randomness (T<1 focused, T>1 diverse)"
      }
    },
    {
      "id": "nlp_016",
      "category": "evaluation",
      "difficulty": "medium",
      "question": "What is BLEU score and its limitations?",
      "answer": "BLEU measures n-gram overlap between generated and reference text. Limitations: no semantics, favors shorter text, single reference bias.",
      "formula": "BLEU = BP × exp(Σ w_n log p_n)",
      "alternatives": {
        "ROUGE": "Recall-oriented, better for summarization",
        "METEOR": "Considers synonyms and paraphrases",
        "BERTScore": "Semantic similarity using BERT embeddings"
      }
    },
    {
      "id": "nlp_017",
      "category": "evaluation",
      "difficulty": "easy",
      "question": "What is perplexity in language modeling?",
      "answer": "Perplexity measures how well a model predicts text. Lower is better. It's the exponentiated average negative log-likelihood.",
      "formula": "PPL = exp(-1/N Σ log P(w_i|context))",
      "interpretation": "PPL=10 means model is as confused as if choosing uniformly from 10 options",
      "typical_values": "Good LM: 20-50, Human-level: ~10"
    },
    {
      "id": "nlp_018",
      "category": "preprocessing",
      "difficulty": "easy",
      "question": "What is TF-IDF and when to use it?",
      "answer": "Term Frequency-Inverse Document Frequency weights words by importance: common in document but rare in corpus = high weight.",
      "formula": "TF-IDF = TF(t,d) × log(N/DF(t))",
      "use_cases": [
        "Document retrieval",
        "Keyword extraction",  
        "Feature for classical ML",
        "Simple baseline for text classification"
      ]
    },
    {
      "id": "nlp_019",
      "category": "preprocessing",
      "difficulty": "medium",
      "question": "Why and when to remove stop words?",
      "answer": "Stop words are common words (the, is, at) that may not carry meaning. Remove for: BoW/TF-IDF, topic modeling. Keep for: neural models, sentiment (not is important!).",
      "considerations": {
        "pros": "Reduces dimensionality, focuses on content words",
        "cons": "Can lose meaning (not happy → happy), breaks phrases",
        "modern_approach": "Neural models learn to ignore them automatically"
      }
    },
    {
      "id": "nlp_020",
      "category": "classical_nlp",
      "difficulty": "medium",
      "question": "What is Named Entity Recognition (NER)?",
      "answer": "NER identifies and classifies named entities (person, location, organization, etc.) in text. It's a sequence labeling task.",
      "approaches": {
        "rule_based": "Patterns and gazetteers",
        "statistical": "CRF, HMM",
        "neural": "BiLSTM-CRF, BERT + token classification"
      },
      "schemes": "IOB/BIO tagging (B-begin, I-inside, O-outside)"
    },
    {
      "id": "nlp_021",
      "category": "classical_nlp",
      "difficulty": "medium",
      "question": "Explain Part-of-Speech (POS) tagging",
      "answer": "POS tagging assigns grammatical categories (noun, verb, adjective) to each word. Uses context to resolve ambiguities.",
      "example": "The/DT cat/NN sat/VBD on/IN the/DT mat/NN",
      "approaches": {
        "rule_based": "Hand-crafted rules",
        "statistical": "HMM, Maximum Entropy",
        "neural": "RNN/LSTM/Transformer based"
      },
      "accuracy": "Modern systems: 97%+ on English"
    },
    {
      "id": "nlp_022",
      "category": "classical_nlp",
      "difficulty": "hard",
      "question": "What is dependency parsing vs constituency parsing?",
      "answer": "Dependency: shows grammatical relationships between words (head-dependent). Constituency: shows phrase structure (nested constituents).",
      "examples": {
        "dependency": "cat → is → cute (subj → root → attr)",
        "constituency": "[S [NP The cat] [VP is cute]]"
      },
      "use_cases": {
        "dependency": "Information extraction, question answering",
        "constituency": "Grammar checking, syntactic analysis"
      }
    },
    {
      "id": "nlp_023",
      "category": "applications",
      "difficulty": "medium",
      "question": "How does question answering work in modern NLP?",
      "answer": "Two main types: Extractive QA (finds answer span in context) and Generative QA (generates answer). Often uses retrieval + reading comprehension.",
      "pipeline": {
        "1_retrieval": "Find relevant documents (BM25, dense retrieval)",
        "2_reading": "Extract/generate answer from documents",
        "models": "BERT for extractive, T5/GPT for generative"
      },
      "datasets": "SQuAD, Natural Questions, MS MARCO"
    },
    {
      "id": "nlp_024",
      "category": "applications",
      "difficulty": "hard",
      "question": "Explain dense retrieval vs sparse retrieval",
      "answer": "Sparse: keyword matching (TF-IDF, BM25). Dense: learned embeddings with semantic similarity. Dense better for semantic search.",
      "comparison": {
        "sparse": "Exact matching, interpretable, no training needed",
        "dense": "Semantic understanding, requires training, handles paraphrases",
        "hybrid": "Combines both for best performance"
      },
      "examples": "DPR, ColBERT, Contriever"
    },
    {
      "id": "nlp_025",
      "category": "fundamentals",
      "difficulty": "medium",
      "question": "What is the vocabulary size vs model size trade-off?",
      "answer": "Larger vocabulary: better coverage, larger embedding matrix. Smaller vocabulary: more OOV, smaller model, relies on subwords.",
      "typical_sizes": {
        "Word-level": "30k-50k tokens",
        "Subword": "30k-100k tokens",
        "Character": "100-1000 tokens"
      },
      "consideration": "Embedding matrix = vocab_size × embedding_dim"
    },
    {
      "id": "nlp_026",
      "category": "embeddings",
      "difficulty": "hard",
      "question": "What are contextualized embeddings?",
      "answer": "Embeddings that change based on context, unlike static embeddings (Word2Vec). Same word gets different representations in different contexts.",
      "examples": {
        "ELMo": "BiLSTM-based, concatenates forward/backward",
        "BERT": "Transformer-based, bidirectional",
        "GPT": "Transformer-based, autoregressive"
      },
      "advantage": "Handles polysemy, captures context-dependent meaning"
    },
    {
      "id": "nlp_027",
      "category": "llms",
      "difficulty": "hard",
      "question": "What is chain-of-thought prompting?",
      "answer": "Prompting technique where model explains reasoning steps before final answer. Improves performance on complex reasoning tasks.",
      "example": "Q: Roger has 5 balls... Let's think step by step...",
      "variants": {
        "zero_shot_CoT": "Add 'Let's think step by step'",
        "few_shot_CoT": "Provide examples with reasoning",
        "self_consistency": "Sample multiple paths, vote on answer"
      }
    },
    {
      "id": "nlp_028",
      "category": "transformers",
      "difficulty": "hard",
      "question": "What is KV-cache in transformer inference?",
      "answer": "Caches key-value pairs from previous tokens to avoid recomputation during autoregressive generation. Major speed optimization.",
      "memory": "Cache size = num_layers × 2 × batch × seq_len × hidden_dim",
      "trade_off": "Speed vs memory: longer sequences need more cache"
    },
    {
      "id": "nlp_029",
      "category": "llms",
      "difficulty": "medium",
      "question": "What is catastrophic forgetting in LLMs?",
      "answer": "When fine-tuning on new task causes model to forget previously learned knowledge. Major challenge in continual learning.",
      "mitigation": {
        "regularization": "EWC, L2 penalty to original weights",
        "replay": "Mix old task data with new",
        "adapter": "Train only small modules, freeze base model",
        "LoRA": "Low-rank adaptation, minimal parameter updates"
      }
    },
    {
      "id": "nlp_030",
      "category": "evaluation",
      "difficulty": "hard",
      "question": "What are calibration and hallucination in LLMs?",
      "answer": "Calibration: model's confidence matches actual accuracy. Hallucination: generating plausible but false information.",
      "hallucination_types": {
        "factual": "Incorrect facts",
        "nonsensical": "Grammatical but meaningless",
        "context": "Ignoring provided context"
      },
      "mitigation": "Retrieval augmentation, fact checking, confidence scoring"
    },
    {
      "id": "nlp_031",
      "category": "fundamentals",
      "difficulty": "easy",
      "question": "What is text normalization?",
      "answer": "Standardizing text to reduce variations: lowercase, expand contractions, remove punctuation, handle numbers/dates.",
      "examples": {
        "lowercase": "Hello → hello",
        "contractions": "don't → do not",
        "numbers": "10 → ten",
        "dates": "3/14/21 → March 14, 2021"
      },
      "caution": "Can lose information (US vs us, 3.14 vs 314)"
    },
    {
      "id": "nlp_032",
      "category": "classical_nlp",
      "difficulty": "medium",
      "question": "What are n-grams and their applications?",
      "answer": "N-grams are contiguous sequences of n items (words/characters). Used for language modeling, text generation, and feature extraction.",
      "types": {
        "unigram": "single words",
        "bigram": "word pairs",
        "trigram": "word triples"
      },
      "applications": "Spell checking, speech recognition, machine translation baselines"
    },
    {
      "id": "nlp_033",
      "category": "classical_nlp",
      "difficulty": "hard",
      "question": "Explain Latent Dirichlet Allocation (LDA)",
      "answer": "Probabilistic topic modeling that discovers abstract topics in documents. Each document is a mixture of topics, each topic is a distribution over words.",
      "assumptions": {
        "bag_of_words": "Order doesn't matter",
        "topics": "Fixed number K",
        "process": "Choose topic dist → for each word, choose topic → choose word from topic"
      },
      "vs_LSA": "LDA probabilistic, LSA uses SVD; LDA more interpretable"
    },
    {
      "id": "nlp_034",
      "category": "applications",
      "difficulty": "medium",
      "question": "How does machine translation work (neural approach)?",
      "answer": "Encoder-decoder architecture: encoder processes source language, decoder generates target language. Attention helps alignment.",
      "evolution": {
        "RNN_seq2seq": "Early neural MT",
        "Attention": "Solved bottleneck problem",
        "Transformer": "Current SOTA (no recurrence)",
        "Multilingual": "Single model for many language pairs"
      },
      "challenges": "Rare languages, maintaining meaning, cultural context"
    },
    {
      "id": "nlp_035",
      "category": "applications",
      "difficulty": "easy",
      "question": "What is sentiment analysis?",
      "answer": "Determining emotional tone or opinion in text (positive, negative, neutral). Can be binary or fine-grained (1-5 stars).",
      "approaches": {
        "lexicon": "Sentiment dictionaries (VADER, TextBlob)",
        "ML": "Naive Bayes, SVM with features",
        "Deep": "LSTM, BERT fine-tuning"
      },
      "challenges": "Sarcasm, negation, domain-specific sentiment"
    },
    {
      "id": "nlp_036",
      "category": "llms",
      "difficulty": "hard",
      "question": "What is mixture of experts (MoE) in LLMs?",
      "answer": "Architecture where different 'expert' sub-networks specialize in different inputs. Router network decides which experts to use.",
      "benefits": {
        "efficiency": "Only activate relevant experts",
        "capacity": "More parameters without proportional compute",
        "specialization": "Experts can specialize in domains"
      },
      "examples": "Switch Transformer, GLaM, Mixtral"
    },
    {
      "id": "nlp_037",
      "category": "transformers",
      "difficulty": "medium",
      "question": "What is Flash Attention?",
      "answer": "Optimized attention computation that's memory-efficient and faster by tiling and avoiding materialization of large attention matrix.",
      "improvements": {
        "memory": "O(n) instead of O(n²)",
        "speed": "2-4x faster on long sequences",
        "enables": "Longer context windows feasible"
      }
    },
    {
      "id": "nlp_038",
      "category": "llms",
      "difficulty": "medium",
      "question": "What is retrieval-augmented generation (RAG)?",
      "answer": "Combining LLMs with external knowledge retrieval. Retrieves relevant documents then conditions generation on them.",
      "pipeline": {
        "1_index": "Create document embeddings",
        "2_retrieve": "Find relevant docs for query",
        "3_generate": "LLM generates using retrieved context"
      },
      "benefits": "Up-to-date info, reduces hallucination, no retraining needed"
    },
    {
      "id": "nlp_039",
      "category": "preprocessing",
      "difficulty": "easy",
      "question": "What is byte-pair encoding (BPE)?",
      "answer": "Subword tokenization that iteratively merges most frequent character pairs. Balances vocabulary size and coverage.",
      "process": [
        "Start with character vocabulary",
        "Count pair frequencies",
        "Merge most frequent pair",
        "Repeat until vocabulary size reached"
      ],
      "advantages": "Handles OOV, compact vocabulary, language agnostic"
    },
    {
      "id": "nlp_040",
      "category": "evaluation",
      "difficulty": "medium",
      "question": "What is F1 score and when to use it in NLP?",
      "answer": "Harmonic mean of precision and recall. Used when both false positives and false negatives matter.",
      "formula": "F1 = 2 × (precision × recall) / (precision + recall)",
      "nlp_uses": [
        "NER evaluation",
        "Classification with imbalanced classes",
        "Information extraction"
      ],
      "variants": "Micro-F1 (aggregate), Macro-F1 (average), Weighted-F1"
    },
    {
      "id": "nlp_041",
      "category": "embeddings",
      "difficulty": "medium",
      "question": "What is negative sampling in Word2Vec?",
      "answer": "Training technique that samples negative examples (words not in context) to make training tractable. Avoids computing full softmax.",
      "purpose": "Reduces computation from O(V) to O(k) where k << V",
      "typical_k": "5-20 negative samples",
      "formula": "Maximize log σ(v_w · v_c) + Σ E[log σ(-v_w' · v_c)]"
    },
    {
      "id": "nlp_042",
      "category": "transformers",
      "difficulty": "hard",
      "question": "What is gradient checkpointing?",
      "answer": "Memory optimization that trades compute for memory by not storing all activations, recomputing them during backward pass.",
      "trade_off": {
        "memory": "Reduces from O(n) to O(√n)",
        "speed": "~20-30% slower training",
        "use_case": "Enables larger batch sizes or longer sequences"
      }
    },
    {
      "id": "nlp_043",
      "category": "llms",
      "difficulty": "medium",
      "question": "What is prompt injection and how to prevent it?",
      "answer": "Attack where malicious input overrides intended instructions. User input treated as instructions rather than data.",
      "prevention": {
        "separation": "Clear delimiter between instruction and input",
        "validation": "Input/output filtering",
        "sandboxing": "Limit model capabilities",
        "monitoring": "Detect unusual patterns"
      }
    },
    {
      "id": "nlp_044",
      "category": "classical_nlp",
      "difficulty": "easy",
      "question": "What is cosine similarity in NLP?",
      "answer": "Measures angle between vectors, regardless of magnitude. Range [-1, 1], with 1 being identical direction.",
      "formula": "cos(θ) = (A·B) / (||A|| × ||B||)",
      "use_cases": [
        "Document similarity",
        "Word similarity in embedding space",
        "Duplicate detection"
      ],
      "vs_euclidean": "Cosine ignores magnitude, focuses on direction"
    },
    {
      "id": "nlp_045",
      "category": "applications",
      "difficulty": "hard",
      "question": "What is coreference resolution?",
      "answer": "Identifying when different expressions refer to the same entity. Critical for document understanding.",
      "example": "John said he would come → 'he' refers to 'John'",
      "approaches": {
        "rule_based": "Syntactic patterns, gender/number agreement",
        "statistical": "Mention-pair, mention-ranking models",
        "neural": "End-to-end neural coref (SpanBERT)"
      }
    },
    {
      "id": "nlp_046",
      "category": "llms",
      "difficulty": "hard",
      "question": "What is constitutional AI?",
      "answer": "Training AI systems with a set of principles (constitution) to self-critique and revise outputs. Reduces harmful outputs without human labels.",
      "process": [
        "Generate response",
        "Critique based on principles",
        "Revise response",
        "Train on revised responses"
      ],
      "benefit": "Scalable alignment without extensive human feedback"
    },
    {
      "id": "nlp_047",
      "category": "transformers",
      "difficulty": "medium",
      "question": "What is layer normalization vs batch normalization?",
      "answer": "LayerNorm: normalizes across features for each sample. BatchNorm: normalizes across batch for each feature. Transformers use LayerNorm.",
      "why_layernorm": {
        "sequence_length": "Variable lengths problematic for BatchNorm",
        "batch_independence": "Each sample normalized independently",
        "stability": "More stable for attention mechanisms"
      }
    },
    {
      "id": "nlp_048",
      "category": "fundamentals",
      "difficulty": "easy",
      "question": "What is one-hot encoding in NLP?",
      "answer": "Representing words as sparse vectors with 1 at word's index, 0s elsewhere. Vocabulary size determines vector dimension.",
      "example": "cat = [0, 1, 0, 0, ...], dog = [0, 0, 1, 0, ...]",
      "limitations": [
        "High dimensionality (vocab size)",
        "No semantic similarity",
        "Sparse representation"
      ],
      "superseded_by": "Dense embeddings (Word2Vec, GloVe)"
    },
    {
      "id": "nlp_049",
      "category": "evaluation",
      "difficulty": "medium",
      "question": "What is ROUGE score?",
      "answer": "Recall-Oriented Understudy for Gisting Evaluation. Measures overlap between generated and reference summaries.",
      "variants": {
        "ROUGE-N": "N-gram overlap",
        "ROUGE-L": "Longest common subsequence",
        "ROUGE-W": "Weighted LCS"
      },
      "vs_BLEU": "ROUGE focuses on recall (coverage), BLEU on precision"
    },
    {
      "id": "nlp_050",
      "category": "llms",
      "difficulty": "hard",
      "question": "What is parameter-efficient fine-tuning (PEFT)?",
      "answer": "Methods to adapt large models with minimal trainable parameters. Includes LoRA, adapters, prefix tuning, prompt tuning.",
      "methods": {
        "LoRA": "Low-rank decomposition of weight updates",
        "Adapters": "Small trainable modules between layers",
        "Prefix_tuning": "Trainable continuous prompts",
        "BitFit": "Only train bias terms"
      },
      "benefits": "Less memory, multiple tasks per model, faster training"
    },
    {
      "id": "nlp_051",
      "category": "applications",
      "difficulty": "medium",
      "question": "What is text summarization (extractive vs abstractive)?",
      "answer": "Extractive: selects important sentences from original. Abstractive: generates new sentences capturing meaning.",
      "comparison": {
        "extractive": "Easier, guaranteed fluency, may lack coherence",
        "abstractive": "More natural, can paraphrase, harder to control",
        "hybrid": "Extract then rewrite for best of both"
      },
      "models": "BERT for extractive, BART/T5 for abstractive"
    },
    {
      "id": "nlp_052",
      "category": "classical_nlp",
      "difficulty": "medium",
      "question": "What is the CRF layer in sequence labeling?",
      "answer": "Conditional Random Field models dependencies between labels. Ensures valid label sequences (e.g., I-PER cannot follow B-LOC).",
      "benefits": [
        "Global optimization over sequence",
        "Enforces label constraints",
        "Better than independent classification"
      ],
      "common_use": "BiLSTM-CRF for NER, POS tagging"
    },
    {
      "id": "nlp_053",
      "category": "transformers",
      "difficulty": "hard",
      "question": "What is rotary position embedding (RoPE)?",
      "answer": "Position encoding that rotates query/key vectors by angle based on position. Encodes relative positions naturally.",
      "advantages": {
        "extrapolation": "Better length generalization",
        "relative": "Decays with relative distance",
        "efficient": "No extra parameters"
      },
      "used_in": "LLaMA, PaLM, GPT-NeoX"
    },
    {
      "id": "nlp_054",
      "category": "llms",
      "difficulty": "medium",
      "question": "What is quantization in LLMs?",
      "answer": "Reducing precision of weights/activations (e.g., float32 → int8) to reduce memory and increase speed.",
      "types": {
        "post_training": "Quantize after training (PTQ)",
        "aware_training": "Train with quantization (QAT)",
        "dynamic": "Quantize activations on-the-fly"
      },
      "trade_offs": "4-bit: 4x memory savings, <1% accuracy loss typically"
    },
    {
      "id": "nlp_055",
      "category": "fundamentals",
      "difficulty": "easy",
      "question": "What is the sequence-to-sequence problem?",
      "answer": "Mapping input sequence to output sequence, potentially different lengths. Core to many NLP tasks.",
      "examples": [
        "Translation: English → French",
        "Summarization: Long text → Short summary",
        "Dialogue: Question → Answer"
      ],
      "architectures": "RNN-based, Transformer-based (T5, BART)"
    },
    {
      "id": "nlp_056",
      "category": "embeddings",
      "difficulty": "hard",
      "question": "What is contrastive learning in NLP?",
      "answer": "Learning representations by contrasting positive pairs (similar) against negative pairs (dissimilar).",
      "examples": {
        "SimCLR": "Different augmentations of same text",
        "CLIP": "Matching images and captions",
        "Sentence-BERT": "Paraphrases as positives"
      },
      "loss": "InfoNCE, triplet loss"
    },
    {
      "id": "nlp_057",
      "category": "applications",
      "difficulty": "medium",
      "question": "How does semantic search differ from keyword search?",
      "answer": "Semantic search understands meaning and intent, keyword search matches exact terms. Semantic handles synonyms and paraphrases.",
      "implementation": {
        "keyword": "TF-IDF, BM25, inverted index",
        "semantic": "Dense embeddings, vector similarity",
        "hybrid": "Combine both for best results"
      }
    },
    {
      "id": "nlp_058",
      "category": "llms",
      "difficulty": "hard",
      "question": "What is the chinchilla scaling law?",
      "answer": "Optimal model size and training tokens scale equally: for compute budget, balance parameters and data. Most models were undertrained.",
      "finding": "For optimal compute, tokens ≈ 20 × parameters",
      "impact": "Shift from larger models to more data (LLaMA vs GPT-3)"
    },
    {
      "id": "nlp_059",
      "category": "classical_nlp",
      "difficulty": "easy",
      "question": "What is bag-of-words representation?",
      "answer": "Representing text as unordered collection of words with frequencies. Ignores grammar and word order.",
      "representation": "Vector where each dimension is word count/frequency",
      "limitations": [
        "Loses word order",
        "No semantic understanding",
        "High dimensionality"
      ],
      "still_useful": "Quick baselines, document classification, information retrieval"
    },
    {
      "id": "nlp_060",
      "category": "evaluation",
      "difficulty": "hard",
      "question": "What is the difference between intrinsic and extrinsic evaluation?",
      "answer": "Intrinsic: evaluates component in isolation (perplexity for LM). Extrinsic: evaluates on downstream task (accuracy on classification).",
      "trade_offs": {
        "intrinsic": "Fast, focused, may not correlate with real performance",
        "extrinsic": "Real-world performance, expensive, task-specific"
      },
      "example": "Word embeddings: intrinsic (analogy task) vs extrinsic (NER performance)"
    }
  ],
  "study_tips": {
    "spaced_repetition": [
      "Review new cards daily",
      "Difficult cards: multiple times per day",
      "Easy cards: increase interval gradually"
    ],
    "active_recall": [
      "Try to answer before revealing",
      "Explain in your own words",
      "Connect to related concepts"
    ],
    "interview_practice": [
      "Practice explaining to non-technical audience",
      "Draw diagrams when explaining architectures",
      "Prepare examples from your experience"
    ]
  }
}