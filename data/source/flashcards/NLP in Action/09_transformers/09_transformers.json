{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-5964",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 09 Transformers",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:09 Transformers",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: 'Segoe UI', 'Roboto', Arial, sans-serif;\n font-size: 18px;\n text-align: center;\n color: #2c3e50;\n background-color: #fdfdfd;\n line-height: 1.5;\n}\n\n.front {\n font-weight: 600;\n color: #2c3e50;\n font-size: 20px;\n padding: 15px;\n}\n\n.back {\n text-align: left;\n padding: 25px;\n max-width: 800px;\n margin: 0 auto;\n}\n\n.concept {\n color: #e74c3c;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.concept strong {\n font-weight: 500;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.intuition strong {\n font-weight: 500;\n font-style: normal;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.mechanics strong {\n font-weight: 500;\n}\n\n.tradeoffs {\n color: #e67e22;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.tradeoffs strong {\n font-weight: 500;\n}\n\n.applications {\n color: #8e44ad;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.applications strong {\n font-weight: 500;\n}\n\n.memory-hook {\n background-color: #f8f9fa;\n padding: 15px;\n border-radius: 6px;\n border-left: 4px solid #6c757d;\n font-style: italic;\n color: #495057;\n margin-top: 15px;\n font-size: 15px;\n}\n\n.memory-hook strong {\n font-weight: 500;\n font-style: normal;\n color: #343a40;\n}\n\n@media (max-width: 768px) {\n .back {\n   padding: 20px 15px;\n }\n \n .card {\n   font-size: 16px;\n }\n \n .front {\n   font-size: 18px;\n   padding: 12px;\n }\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note--6645811740357786220-5964",
      "fields": [
        "What is the primary advantage of transformers over recurrent neural networks like LSTMs in terms of processing sequences?",
        "<div class=\"concept\">Concept: Transformers vs. RNNs</div><div class=\"intuition\">Intuition: Transformers process entire sequences at once using attention, avoiding sequential computation.</div><div class=\"mechanics\">Mechanics: Unlike RNNs, which process tokens sequentially and suffer from vanishing gradients, transformers use self-attention to connect all tokens in parallel.</div><div class=\"tradeoffs\">Trade-offs: Transformers require more memory for long sequences due to quadratic complexity in attention, but enable faster training on GPUs.</div><div class=\"applications\">Applications: Used in translation, summarization, and text generation tasks like GPT models.</div><div class=\"memory-hook\">Memory Hook: Think of transformers as a 'global view' party where everyone talks at once, unlike RNNs' 'conga line' where info passes one by one.</div>",
        "Transformers RNNs Attention Intuition",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--6645811740357786220-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Transformers",
        "RNNs",
        "Attention",
        "Intuition"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--6871731277012639437-5964",
      "fields": [
        "Explain how byte pair encoding (BPE) contributes to the efficiency of transformers.",
        "<div class=\"concept\">Concept: Byte Pair Encoding (BPE)</div><div class=\"intuition\">Intuition: BPE builds a compact vocabulary by merging frequent character pairs, reducing out-of-vocabulary issues.</div><div class=\"mechanics\">Mechanics: Start with characters, iteratively merge the most frequent pairs into new tokens until vocabulary size is reached, e.g., 5,000 tokens.</div><div class=\"tradeoffs\">Trade-offs: Reduces vocabulary size for efficiency but may split rare words into subwords, increasing sequence length slightly.</div><div class=\"applications\">Applications: Tokenization in models like BERT and GPT for handling diverse languages and compressing internet-scale data.</div><div class=\"memory-hook\">Memory Hook: BPE is like zip-compressing text by gluing common letter pairs, making big dictionaries tiny like a travel suitcase.</div>",
        "BPE Tokenization Theory",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--6871731277012639437-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "BPE",
        "Tokenization",
        "Theory"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8087592423536449987-5964",
      "fields": [
        "How does positional encoding enable transformers to handle sequence order without recurrence?",
        "<div class=\"concept\">Concept: Positional Encoding</div><div class=\"intuition\">Intuition: It adds a unique 'position signal' to each token's embedding, like numbering seats in a theater.</div><div class=\"mechanics\">Mechanics: Uses sine and cosine functions: PE(pos,2i) = sin(pos / 10000^{2i/d}), PE(pos,2i+1) = cos(pos / 10000^{2i/d}), added to embeddings.</div><div class=\"tradeoffs\">Trade-offs: Fixed and doesn't require learning, but limited to fixed max sequence length; learned alternatives exist but add parameters.</div><div class=\"applications\">Applications: Essential in BERT for bidirectional context and GPT for generation order.</div><div class=\"memory-hook\">Memory Hook: Imagine words on a wavy sine wave conveyor beltâ€”each position has a unique ripple pattern.</div>",
        "Positional Encoding Math Transformers",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--8087592423536449987-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Positional Encoding",
        "Math",
        "Transformers"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4584784221639618230-5964",
      "fields": [
        "Describe the mechanics of multi-head self-attention in transformers and its benefits.",
        "<div class=\"concept\">Concept: Multi-Head Self-Attention</div><div class=\"intuition\">Intuition: Multiple 'heads' focus on different aspects of relationships between words, like multiple spotlights on a stage.</div><div class=\"mechanics\">Mechanics: Project input to Q, K, V matrices per head; compute scaled dot-product attention; concatenate and project outputs.</div><div class=\"tradeoffs\">Trade-offs: Increases model capacity without much computation cost, but more heads require more memory.</div><div class=\"applications\">Applications: Core to BERT's bidirectional encoding and GPT's generation.</div><div class=\"memory-hook\">Memory Hook: Like a hydra with multiple heads, each attending to a different word connection in the text.</div>",
        "Attention Multi-Head Mechanics",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--4584784221639618230-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Attention",
        "Multi-Head",
        "Mechanics"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--1906994981567990761-5964",
      "fields": [
        "Compare the recursion in transformers to recurrence in RNNs, and discuss implications for scalability.",
        "<div class=\"concept\">Concept: Recursion vs. Recurrence</div><div class=\"intuition\">Intuition: Transformers recycle outputs at the network level, not per neuron, allowing parallel processing.</div><div class=\"mechanics\">Mechanics: Transformers run the full network per token prediction, recycling output tokens as input; RNNs loop internally per step.</div><div class=\"tradeoffs\">Trade-offs: No unrolling needed in transformers, enabling stacking and parallelism, but higher per-token compute.</div><div class=\"applications\">Applications: Enables massive models like GPT-3 for text generation.</div><div class=\"memory-hook\">Memory Hook: RNNs are like a single juggler passing balls hand-to-hand; transformers are a team juggling in sync.</div>",
        "Recursion Recurrence Comparisons",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--1906994981567990761-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Recursion",
        "Recurrence",
        "Comparisons"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2790658604882004647-5964",
      "fields": [
        "How can transformers be applied to extractive and abstractive summarization of long documents?",
        "<div class=\"concept\">Concept: Summarization with Transformers</div><div class=\"intuition\">Intuition: Extractive picks key sentences; abstractive generates new ones, mimicking human paraphrasing.</div><div class=\"mechanics\">Mechanics: Use encoder-decoder for abstractive (e.g., BART); attention scores for extractive to rank sentences.</div><div class=\"tradeoffs\">Trade-offs: Extractive is factual but less fluent; abstractive is creative but risks hallucinations.</div><div class=\"applications\">Applications: News summarization, document compression in legal or medical fields.</div><div class=\"memory-hook\">Memory Hook: Extractive is copying highlights; abstractive is rewriting notes in your own words.</div>",
        "Summarization Applications Transformers",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-2790658604882004647-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Summarization",
        "Applications",
        "Transformers"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4080939832949066047-5964",
      "fields": [
        "What are the trade-offs of fine-tuning a pretrained transformer like BERT for a specific application?",
        "<div class=\"concept\">Concept: Fine-Tuning Transformers</div><div class=\"intuition\">Intuition: Adapt a general expert to your niche by teaching it specifics.</div><div class=\"mechanics\">Mechanics: Load pretrained weights, add task-specific layers, train on labeled data with lower learning rate.</div><div class=\"tradeoffs\">Trade-offs: Quick and data-efficient but risks overfitting if data is small; may forget general knowledge (catastrophic forgetting).</div><div class=\"applications\">Applications: Toxic comment classification, sentiment analysis.</div><div class=\"memory-hook\">Memory Hook: Like retraining a polymath chef to specialize in vegan dishes without losing basic skills.</div>",
        "Fine-Tuning Trade-offs BERT",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--4080939832949066047-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Fine-Tuning",
        "Trade-offs",
        "BERT"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7828878304916274345-5964",
      "fields": [
        "Explain how to estimate the information capacity of a transformer model.",
        "<div class=\"concept\">Concept: Information Capacity of Transformers</div><div class=\"intuition\">Intuition: Capacity is like storage space for patterns; more layers/heads mean more room.</div><div class=\"mechanics\">Mechanics: Roughly, parameters count: layers * (hidden_size^2 * heads + feedforward).</div><div class=\"tradeoffs\">Trade-offs: Higher capacity improves performance on large data but increases compute and overfitting risk.</div><div class=\"applications\">Applications: Scaling for LLMs like GPT-3 to handle diverse tasks.</div><div class=\"memory-hook\">Memory Hook: Think of it as bookshelf sizeâ€”more shelves (layers) hold more books (patterns).</div>",
        "Capacity Math Scaling",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-7828878304916274345-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Capacity",
        "Math",
        "Scaling"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8698406415299322282-5964",
      "fields": [
        "How does BERT's bidirectional training differ from unidirectional models like GPT, and what are the connections to other architectures?",
        "<div class=\"concept\">Concept: BERT's Bidirectionality</div><div class=\"intuition\">Intuition: BERT reads both ways for full context, like scanning a page before filling blanks.</div><div class=\"mechanics\">Mechanics: Pretrained with masked language modeling (MLM) and next sentence prediction (NSP).</div><div class=\"tradeoffs\">Trade-offs: Better for understanding but not autoregressive, so less suited for pure generation.</div><div class=\"applications\">Applications: Question answering, semantic search.</div><div class=\"memory-hook\">Memory Hook: BERT is a detective looking left and right for clues; GPT is a storyteller predicting forward.</div>",
        "BERT Bidirectional Connections",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-8698406415299322282-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "BERT",
        "Bidirectional",
        "Connections"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--6654826601932398582-5964",
      "fields": [
        "Discuss the applications and limitations of using transformers for generating grammatically correct text.",
        "<div class=\"concept\">Concept: Text Generation with Transformers</div><div class=\"intuition\">Intuition: Autoregressively predict next tokens based on patterns learned from data.</div><div class=\"mechanics\">Mechanics: Decoder stacks generate tokens recursively, using beam search for better outputs.</div><div class=\"tradeoffs\">Trade-offs: Produces fluent text but can hallucinate facts or lack reasoning.</div><div class=\"applications\">Applications: Chatbots, story generation, code completion.</div><div class=\"memory-hook\">Memory Hook: Like an improv actor continuing a scene plausibly, but sometimes forgetting the plot.</div>",
        "Generation Applications Limitations",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--6654826601932398582-5964",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Generation",
        "Applications",
        "Limitations"
      ],
      "__type__": "Note"
    }
  ]
}