{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-8186",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 03 TF-IDF",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:03 TF-IDF",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: 'Segoe UI', 'Roboto', Arial, sans-serif;\n font-size: 18px;\n text-align: center;\n color: #2c3e50;\n background-color: #fdfdfd;\n line-height: 1.5;\n}\n\n.front {\n font-weight: 600;\n color: #2c3e50;\n font-size: 20px;\n padding: 15px;\n}\n\n.back {\n text-align: left;\n padding: 25px;\n max-width: 800px;\n margin: 0 auto;\n}\n\n.concept {\n color: #e74c3c;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.concept strong {\n font-weight: 500;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.intuition strong {\n font-weight: 500;\n font-style: normal;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.mechanics strong {\n font-weight: 500;\n}\n\n.tradeoffs {\n color: #e67e22;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.tradeoffs strong {\n font-weight: 500;\n}\n\n.applications {\n color: #8e44ad;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.applications strong {\n font-weight: 500;\n}\n\n.memory-hook {\n background-color: #f8f9fa;\n padding: 15px;\n border-radius: 6px;\n border-left: 4px solid #6c757d;\n font-style: italic;\n color: #495057;\n margin-top: 15px;\n font-size: 15px;\n}\n\n.memory-hook strong {\n font-weight: 500;\n font-style: normal;\n color: #343a40;\n}\n\n@media (max-width: 768px) {\n .back {\n   padding: 20px 15px;\n }\n \n .card {\n   font-size: 16px;\n }\n \n .front {\n   font-size: 18px;\n   padding: 12px;\n }\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note--2645794198076619719-8186",
      "fields": [
        "What is a bag-of-words (BOW) vector?",
        "<div class=\"concept\">Concept: A bag-of-words vector is a numerical representation of text where each dimension corresponds to a unique word in the vocabulary, and the value is the count or frequency of that word in the document.</div><div class=\"intuition\">Intuition: It treats text as an unordered collection (bag) of words, ignoring grammar and order, focusing on word occurrences to capture basic content.</div><div class=\"mechanics\">Mechanics: Tokenize the text, build a vocabulary of unique words, then create a vector where each position holds the count of the corresponding word.</div><div class=\"tradeoffs\">Trade-offs: Simple and efficient but loses word order and context, leading to potential loss of meaning.</div><div class=\"applications\">Applications: Used in document classification, spam filtering, and basic search engines.</div><div class=\"memory-hook\">Memory Hook: Imagine dumping all words from a document into a bag and counting how many of each type you pull out.</div>",
        "Bag-of-Words Vector Representation Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--2645794198076619719-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Bag-of-Words",
        "Vector Representation",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6182551501977049345-8186",
      "fields": [
        "Explain the intuition behind term frequency (TF).",
        "<div class=\"concept\">Concept: Term frequency is the number of times a word appears in a document, often normalized by the total number of words in the document.</div><div class=\"intuition\">Intuition: Words that appear more frequently in a document are likely more important to its meaning, like 'bias' in an article about algorithmic bias.</div><div class=\"mechanics\">Mechanics: TF(t, d) = count of t in d / total words in d.</div><div class=\"tradeoffs\">Trade-offs: Overemphasizes common words like 'the' or 'and', which may not carry much meaning.</div><div class=\"applications\">Applications: Basic feature in text analysis, combined with IDF for better relevance.</div><div class=\"memory-hook\">Memory Hook: Think of TF as a spotlight that brightens words repeated often in a story.</div>",
        "Term Frequency Intuition Easy",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-6182551501977049345-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Term Frequency",
        "Intuition",
        "Easy"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-929929532141739201-8186",
      "fields": [
        "How do you compute inverse document frequency (IDF)?",
        "<div class=\"concept\">Concept: IDF measures how rare a word is across the corpus, giving higher weight to unique words.</div><div class=\"intuition\">Intuition: Common words across documents add little unique information; rare ones distinguish documents.</div><div class=\"mechanics\">Mechanics: IDF(t) = log(total documents / documents containing t), often with smoothing like adding 1 to avoid division by zero.</div><div class=\"tradeoffs\">Trade-offs: Sensitive to corpus size; very rare words might be overemphasized if noisy.</div><div class=\"applications\">Applications: Weights terms in search engines to prioritize distinctive keywords.</div><div class=\"memory-hook\">Memory Hook: IDF is like a rarity score in a game—common items are cheap, rare ones valuable.</div>",
        "Inverse Document Frequency Math Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-929929532141739201-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Inverse Document Frequency",
        "Math",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8134078055732042294-8186",
      "fields": [
        "What is TF-IDF and how is it calculated?",
        "<div class=\"concept\">Concept: TF-IDF is a score reflecting a word's importance in a document relative to the corpus.</div><div class=\"intuition\">Intuition: Balances how often a word appears in one document against its commonality elsewhere.</div><div class=\"mechanics\">Mechanics: TF-IDF(t, d) = TF(t, d) * IDF(t), often using log for IDF to follow Zipf's law.</div><div class=\"tradeoffs\">Trade-offs: Ignores word order and semantics; performs poorly on short texts or small corpora.</div><div class=\"applications\">Applications: Document similarity, keyword extraction, search ranking.</div><div class=\"memory-hook\">Memory Hook: TF-IDF is a word's 'VIP pass'—frequent locally but rare globally gets high access.</div>",
        "TF-IDF Theory Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--8134078055732042294-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Theory",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-5965998036540456521-8186",
      "fields": [
        "Compare Euclidean distance and cosine similarity for text vectors.",
        "<div class=\"concept\">Concept: Euclidean distance measures straight-line distance between points; cosine similarity measures angle between vectors.</div><div class=\"intuition\">Intuition: Euclidean cares about magnitude (length); cosine focuses on direction (overlap in content).</div><div class=\"mechanics\">Mechanics: Euclidean: sqrt(sum((x_i - y_i)^2)); Cosine: dot(x,y) / (norm(x) * norm(y)).</div><div class=\"tradeoffs\">Trade-offs: Euclidean suffers in high dimensions (curse of dimensionality); cosine is robust for varying document lengths.</div><div class=\"applications\">Applications: Cosine for text similarity in search; Euclidean less common for sparse text vectors.</div><div class=\"memory-hook\">Memory Hook: Euclidean is walking distance on a map; cosine is how aligned two arrows point.</div>",
        "Vector Similarity Comparisons Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-5965998036540456521-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vector Similarity",
        "Comparisons",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6419516913633319396-8186",
      "fields": [
        "What is Zipf's law and its implications for NLP?",
        "<div class=\"concept\">Concept: Zipf's law states word frequency is inversely proportional to its rank in the frequency table.</div><div class=\"intuition\">Intuition: A few words are very common; most are rare, creating a power-law distribution.</div><div class=\"mechanics\">Mechanics: Frequency of rank k word ≈ 1/k * frequency of rank 1 word; plot log(freq) vs log(rank) is linear.</div><div class=\"tradeoffs\">Trade-offs: Explains why stop words dominate; requires logging in TF-IDF to balance frequencies.</div><div class=\"applications\">Applications: Predicts word distributions; justifies stop word removal and log scaling in models.</div><div class=\"memory-hook\">Memory Hook: Like city populations: New York (the) is huge, smaller ones drop off quickly.</div>",
        "Zipf's Law Theory Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-6419516913633319396-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Zipf's Law",
        "Theory",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2144943578505276795-8186",
      "fields": [
        "How do n-grams improve upon basic bag-of-words?",
        "<div class=\"concept\">Concept: N-grams are sequences of n consecutive tokens, capturing local context.</div><div class=\"intuition\">Intuition: BOW ignores order; n-grams preserve some phrasing, like 'New York' as a bigram.</div><div class=\"mechanics\">Mechanics: For n=2, split text into overlapping pairs; count frequencies like words.</div><div class=\"tradeoffs\">Trade-offs: Increases dimensionality exponentially; sparse for higher n.</div><div class=\"applications\">Applications: Language modeling, spelling correction, authorship attribution.</div><div class=\"memory-hook\">Memory Hook: N-grams are word buddies—alone they're lost, together they tell a mini-story.</div>",
        "N-grams Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-2144943578505276795-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "N-grams",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-1644131795236545922-8186",
      "fields": [
        "Describe the mechanics of using scikit-learn's CountVectorizer.",
        "<div class=\"concept\">Concept: CountVectorizer converts text to a matrix of token counts.</div><div class=\"intuition\">Intuition: Automates building BOW vectors from a corpus.</div><div class=\"mechanics\">Mechanics: Initialize with params like ngram_range; fit() builds vocab; transform() creates sparse matrix of counts.</div><div class=\"tradeoffs\">Trade-offs: Defaults to word tokens; customizable but may need preprocessing for punctuation.</div><div class=\"applications\">Applications: Feature extraction for ML models on text data.</div><div class=\"memory-hook\">Memory Hook: It's a word counter robot: feeds on text, spits out numbered bags.</div>",
        "CountVectorizer Mechanics Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-1644131795236545922-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "CountVectorizer",
        "Mechanics",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6122957643168558787-8186",
      "fields": [
        "What are the trade-offs of using TF-IDF in a very small corpus?",
        "<div class=\"concept\">Concept: TF-IDF relies on document frequencies across a corpus.</div><div class=\"intuition\">Intuition: In small corpora, rare words might be overvalued due to limited samples.</div><div class=\"mechanics\">Mechanics: IDF becomes unstable; log( small N / df ) can exaggerate uniqueness.</div><div class=\"tradeoffs\">Trade-offs: Poor generalization; overfitting to noise; better for large, diverse corpora.</div><div class=\"applications\">Applications: Still useful for tiny FAQ bots, but add smoothing.</div><div class=\"memory-hook\">Memory Hook: Small pond, big fish—rare words seem too important without enough water (data).</div>",
        "TF-IDF Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-6122957643168558787-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-9048109414618986474-8186",
      "fields": [
        "How does cosine similarity connect to dot products in vector spaces?",
        "<div class=\"concept\">Concept: Cosine similarity is the normalized dot product of two vectors.</div><div class=\"intuition\">Intuition: Measures how much vectors 'agree' in direction, ignoring size.</div><div class=\"mechanics\">Mechanics: Cos_sim = dot(A,B) / (||A|| * ||B||), where || || is L2 norm.</div><div class=\"tradeoffs\">Trade-offs: Efficient for sparse vectors; but assumes Euclidean space.</div><div class=\"applications\">Applications: Document matching, recommendation systems.</div><div class=\"memory-hook\">Memory Hook: Dot product is raw overlap; cosine adjusts for vector 'volume'.</div>",
        "Cosine Similarity Math Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-9048109414618986474-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Cosine Similarity",
        "Math",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-793104567053585018-8186",
      "fields": [
        "Explain an application of character n-grams.",
        "<div class=\"concept\">Concept: Character n-grams break text into sequences of characters.</div><div class=\"intuition\">Intuition: Captures subword patterns, robust to spelling variations.</div><div class=\"mechanics\">Mechanics: For n=3, 'hello' -> 'hel', 'ell', 'llo'; count frequencies.</div><div class=\"tradeoffs\">Trade-offs: Higher dimensionality than words; but handles OOV better.</div><div class=\"applications\">Applications: Language detection, typo-tolerant search, authorship analysis.</div><div class=\"memory-hook\">Memory Hook: Like DNA sequencing—short snippets reveal the organism (language).</div>",
        "N-grams Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-793104567053585018-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "N-grams",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--1706555586041496425-8186",
      "fields": [
        "What is the curse of dimensionality in text vectors?",
        "<div class=\"concept\">Concept: High-dimensional spaces make distances less meaningful.</div><div class=\"intuition\">Intuition: As dimensions grow, points spread out, similarities dilute.</div><div class=\"mechanics\">Mechanics: In high-D, Euclidean distances concentrate; cosine helps but not fully.</div><div class=\"tradeoffs\">Trade-offs: Sparsity increases computation; requires dimensionality reduction.</div><div class=\"applications\">Applications: Affects large vocab TF-IDF; mitigated in embeddings.</div><div class=\"memory-hook\">Memory Hook: Crowded party in a huge hall—everyone feels far apart.</div>",
        "Vector Spaces Trade-offs Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--1706555586041496425-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Vector Spaces",
        "Trade-offs",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--3410993029050119354-8186",
      "fields": [
        "How can TF-IDF be used to build a simple FAQ bot?",
        "<div class=\"concept\">Concept: Vectorize questions, match user query to closest FAQ via similarity.</div><div class=\"intuition\">Intuition: Treat query as document, find most relevant stored question, return its answer.</div><div class=\"mechanics\">Mechanics: Fit TfidfVectorizer on FAQs; transform query; compute cosine sim; pick max.</div><div class=\"tradeoffs\">Trade-offs: Literal matching; fails on synonyms/typos without enhancements.</div><div class=\"applications\">Applications: Customer support bots, knowledge bases.</div><div class=\"memory-hook\">Memory Hook: FAQ bot as a librarian: matches your question to the closest book title.</div>",
        "TF-IDF Application Medium",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--3410993029050119354-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Application",
        "Medium"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--465106690625677162-8186",
      "fields": [
        "Compare TF-IDF with Okapi BM25 for ranking.",
        "<div class=\"concept\">Concept: BM25 is a probabilistic ranking function improving on TF-IDF.</div><div class=\"intuition\">Intuition: BM25 saturates TF growth and adjusts for document length nonlinearly.</div><div class=\"mechanics\">Mechanics: BM25 adds parameters for TF saturation and length normalization.</div><div class=\"tradeoffs\">Trade-offs: BM25 better for relevance in search; more params to tune.</div><div class=\"applications\">Applications: Modern search engines prefer BM25 over plain TF-IDF.</div><div class=\"memory-hook\">Memory Hook: TF-IDF is basic scale; BM25 adds smart dampers for balance.</div>",
        "TF-IDF Comparisons Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--465106690625677162-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Comparisons",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6107642476117540781-8186",
      "fields": [
        "What connections exist between Zipf's law and IDF?",
        "<div class=\"concept\">Concept: Both deal with frequency distributions in language.</div><div class=\"intuition\">Intuition: Zipf explains why common words have low info; IDF logs to compress scale.</div><div class=\"mechanics\">Mechanics: IDF uses log to linearize Zipf's power law in frequencies.</div><div class=\"tradeoffs\">Trade-offs: Without log, rare words dominate; with it, balances importance.</div><div class=\"applications\">Applications: Justifies log in TF-IDF for uniform weighting.</div><div class=\"memory-hook\">Memory Hook: Zipf's steep hill; log IDF flattens it for fair climbing.</div>",
        "Zipf's Law Connections Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-6107642476117540781-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Zipf's Law",
        "Connections",
        "Hard"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--7279676607282247588-8186",
      "fields": [
        "Describe an edge case where TF-IDF fails.",
        "<div class=\"concept\">Concept: TF-IDF assumes independence and frequency-based importance.</div><div class=\"intuition\">Intuition: Synonyms or context changes meaning without frequency shift.</div><div class=\"mechanics\">Mechanics: Two docs with same words different orders (e.g., 'man bites dog' vs 'dog bites man').</div><div class=\"tradeoffs\">Trade-offs: No semantic understanding; needs embeddings for better handling.</div><div class=\"applications\">Applications: Poor for sentiment with negation or irony.</div><div class=\"memory-hook\">Memory Hook: TF-IDF blind to word puzzles—same pieces, different picture.</div>",
        "TF-IDF Edge Cases Hard",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--7279676607282247588-8186",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Edge Cases",
        "Hard"
      ],
      "__type__": "Note"
    }
  ]
}