{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "deck-2217",
  "deck_config_uuid": "default-config",
  "deck_configurations": [
    {
      "crowdanki_uuid": "default-config",
      "name": "Default",
      "autoplay": true,
      "dyn": false,
      "lapse": {
        "delays": [
          10
        ],
        "leechAction": 0,
        "leechFails": 8,
        "minInt": 1,
        "mult": 0
      },
      "maxTaken": 60,
      "new": {
        "bury": false,
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          0
        ],
        "order": 1,
        "perDay": 20
      },
      "replayq": true,
      "rev": {
        "bury": false,
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1,
        "maxIvl": 36500,
        "perDay": 200
      },
      "timer": 0,
      "__type__": "DeckConfig"
    }
  ],
  "desc": "Comprehensive flashcards for 04 Semantic Analysis",
  "dyn": false,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "ML:NLP:04 Semantic Analysis",
  "note_models": [
    {
      "crowdanki_uuid": "nlp-comprehensive-note-model",
      "css": ".card {\n font-family: 'Segoe UI', 'Roboto', Arial, sans-serif;\n font-size: 18px;\n text-align: center;\n color: #2c3e50;\n background-color: #fdfdfd;\n line-height: 1.5;\n}\n\n.front {\n font-weight: 600;\n color: #2c3e50;\n font-size: 20px;\n padding: 15px;\n}\n\n.back {\n text-align: left;\n padding: 25px;\n max-width: 800px;\n margin: 0 auto;\n}\n\n.concept {\n color: #e74c3c;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.concept strong {\n font-weight: 500;\n}\n\n.intuition {\n color: #3498db;\n font-style: italic;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.intuition strong {\n font-weight: 500;\n font-style: normal;\n}\n\n.mechanics {\n color: #27ae60;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.mechanics strong {\n font-weight: 500;\n}\n\n.tradeoffs {\n color: #e67e22;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.tradeoffs strong {\n font-weight: 500;\n}\n\n.applications {\n color: #8e44ad;\n margin-bottom: 12px;\n font-size: 16px;\n}\n\n.applications strong {\n font-weight: 500;\n}\n\n.memory-hook {\n background-color: #f8f9fa;\n padding: 15px;\n border-radius: 6px;\n border-left: 4px solid #6c757d;\n font-style: italic;\n color: #495057;\n margin-top: 15px;\n font-size: 15px;\n}\n\n.memory-hook strong {\n font-weight: 500;\n font-style: normal;\n color: #343a40;\n}\n\n@media (max-width: 768px) {\n .back {\n   padding: 20px 15px;\n }\n \n .card {\n   font-size: 16px;\n }\n \n .front {\n   font-size: 18px;\n   padding: 12px;\n }\n}",
      "flds": [
        {
          "font": "Arial",
          "media": [],
          "name": "Front",
          "ord": 0,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Back",
          "ord": 1,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Tags",
          "ord": 2,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        },
        {
          "font": "Arial",
          "media": [],
          "name": "Difficulty",
          "ord": 3,
          "rtl": false,
          "size": 20,
          "sticky": false,
          "__type__": "NoteModelField"
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
      "name": "NLP Comprehensive",
      "req": [
        [
          0,
          "all"
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n<div class=\"back\">\n{{Back}}\n</div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "<div class=\"front\">{{Front}}</div>",
          "__type__": "CardTemplate"
        }
      ],
      "type": 0,
      "__type__": "NoteModel"
    }
  ],
  "notes": [
    {
      "crowdanki_uuid": "note--7107351939917629510-2217",
      "fields": [
        "What are the limitations of TF-IDF vectors in capturing meaning, and how do they relate to synonyms and polysemy?",
        "<div class=\"concept\">Concept: TF-IDF vectors represent text based on term frequency and inverse document frequency, but they fail to capture semantic meaning beyond exact word matches.</div><div class=\"intuition\">Intuition: TF-IDF treats words as independent, so synonyms like 'beautiful' and 'pretty' are seen as unrelated, and polysemous words like 'band' (music group or hair accessory) are not disambiguated.</div><div class=\"mechanics\">Mechanics: TF-IDF computes importance as term frequency times inverse document frequency, but doesn't account for word co-occurrences or contextual meanings.</div><div class=\"tradeoffs\">Trade-offs: Effective for keyword search but poor for semantic similarity; lemmatization helps with word forms but not synonyms or antonyms.</div><div class=\"applications\">Applications: Used in basic search engines or document ranking where exact matches suffice.</div><div class=\"memory-hook\">Memory Hook: Imagine TF-IDF as a word counter in a dark room—it counts shadows but misses the shapes (meanings) behind them.</div>",
        "TF-IDF Limitations Semantics",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--7107351939917629510-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TF-IDF",
        "Limitations",
        "Semantics"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6759806957825186855-2217",
      "fields": [
        "Explain the intuition behind topic vectors and how they address TF-IDF's shortcomings.",
        "<div class=\"concept\">Concept: Topic vectors are lower-dimensional representations that capture semantic meaning by grouping co-occurring words into topics.</div><div class=\"intuition\">Intuition: Like mixing colors to represent a painting's essence, topic vectors combine word frequencies to represent underlying themes, allowing synonyms to align in similar directions.</div><div class=\"mechanics\">Mechanics: Derived from TF-IDF or BOW matrices via methods like LSA or LDiA, resulting in vectors where dimensions are topics rather than individual words.</div><div class=\"tradeoffs\">Trade-offs: Reduces dimensionality and noise but may lose fine-grained details; computationally intensive for large corpora.</div><div class=\"applications\">Applications: Semantic search, document clustering, and similarity comparisons.</div><div class=\"memory-hook\">Memory Hook: Think of topic vectors as a compressed zip file of meaning—unzipping reveals themes, not just raw words.</div>",
        "Topic Vectors Intuition Semantics",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-6759806957825186855-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Topic Vectors",
        "Intuition",
        "Semantics"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--1568572186054844491-2217",
      "fields": [
        "Describe the mechanics of creating topic vectors in a thought experiment with words like 'cat', 'dog', and 'NYC'.",
        "<div class=\"concept\">Concept: Manual assignment of word weights to topics to form vectors.</div><div class=\"intuition\">Intuition: Humans intuitively group related words (e.g., cat and dog for 'petness') and assign weights based on association.</div><div class=\"mechanics\">Mechanics: Define topics (e.g., petness, animalness, cityness); assign weights (positive/negative) to words; compute vector as weighted sum of TF-IDF values.</div><div class=\"tradeoffs\">Trade-offs: Intuitive but subjective and non-scalable; doesn't automate discovery.</div><div class=\"applications\">Applications: Illustrates how algorithms like LSA automate this process.</div><div class=\"memory-hook\">Memory Hook: Like baking a cake—words are ingredients, topics are flavors, weights are measurements for the mix.</div>",
        "Topic Vectors Mechanics Thought Experiment",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--1568572186054844491-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Topic Vectors",
        "Mechanics",
        "Thought Experiment"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-8075025494518043682-2217",
      "fields": [
        "Compare the mathematical foundations of LSA and LDiA for topic modeling.",
        "<div class=\"concept\">Concept: LSA uses linear algebra (SVD) on TF-IDF; LDiA uses probabilistic modeling assuming Dirichlet distributions.</div><div class=\"intuition\">Intuition: LSA spreads vectors to maximize variance; LDiA groups words probabilistically like drawing from topic 'bags'.</div><div class=\"mechanics\">Mechanics: LSA: Truncated SVD decomposes matrix into U, S, V; LDiA: Iterative allocation of words to topics via Gibbs sampling or variational inference.</div><div class=\"tradeoffs\">Trade-offs: LSA faster and scalable but topics less interpretable; LDiA slower but topics more coherent.</div><div class=\"applications\">Applications: LSA for classification; LDiA for summarization.</div><div class=\"memory-hook\">Memory Hook: LSA is a straight highway (linear); LDiA is a winding path through probabilities.</div>",
        "LSA LDiA Math Comparison",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-8075025494518043682-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "LSA",
        "LDiA",
        "Math",
        "Comparison"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-6571990827540260580-2217",
      "fields": [
        "What is the role of SVD in LSA, and how does truncated SVD differ from full SVD?",
        "<div class=\"concept\">Concept: SVD decomposes a matrix into three factors for dimensionality reduction in LSA.</div><div class=\"intuition\">Intuition: Like factoring a number, SVD breaks down term-document relationships into principal components (topics).</div><div class=\"mechanics\">Mechanics: A = U Σ V^T; truncated keeps top k singular values for approximation.</div><div class=\"tradeoffs\">Trade-offs: Truncated is efficient for sparse matrices but approximate; full is precise but computationally expensive.</div><div class=\"applications\">Applications: Topic extraction from TF-IDF matrices.</div><div class=\"memory-hook\">Memory Hook: SVD as a matrix sandwich: U and V^T as bread, Σ as the filling—truncate to make it bite-sized.</div>",
        "SVD LSA Theory",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-6571990827540260580-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "SVD",
        "LSA",
        "Theory"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8687490307376431366-2217",
      "fields": [
        "How does LDiA model document generation, and what assumptions does it make?",
        "<div class=\"concept\">Concept: LDiA assumes documents are mixtures of topics, topics are distributions over words.</div><div class=\"intuition\">Intuition: Imagines a 'dice-rolling' machine generating words from topic distributions.</div><div class=\"mechanics\">Mechanics: Uses Dirichlet prior for topic mixtures; infers parameters via EM or sampling.</div><div class=\"tradeoffs\">Trade-offs: Captures word co-occurrences well but sensitive to hyperparameters like number of topics.</div><div class=\"applications\">Applications: Topic discovery in unlabeled corpora.</div><div class=\"memory-hook\">Memory Hook: LDiA as a topic lottery—draw topics, then words from those topic ' urns'.</div>",
        "LDiA Mechanics Probabilistic",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--8687490307376431366-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "LDiA",
        "Mechanics",
        "Probabilistic"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7417100029675027988-2217",
      "fields": [
        "In the toxicity detection challenge, why does LDA classifier overfit on TF-IDF but improve with LSA topics?",
        "<div class=\"concept\">Concept: High-dimensional TF-IDF leads to overfitting; LSA reduces dimensions.</div><div class=\"intuition\">Intuition: TF-IDF has too many sparse features; LSA compresses to meaningful topics.</div><div class=\"mechanics\">Mechanics: Train LDA on TF-IDF vs. LSA vectors; evaluate with train/test split.</div><div class=\"tradeoffs\">Trade-offs: LSA improves generalization but may lose some specificity.</div><div class=\"applications\">Applications: Classifying toxic comments in imbalanced datasets.</div><div class=\"memory-hook\">Memory Hook: TF-IDF as a noisy crowd; LSA quiets it to a focused group discussion.</div>",
        "Toxicity Detection Application Overfitting",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-7417100029675027988-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Toxicity Detection",
        "Application",
        "Overfitting"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7066930388441684764-2217",
      "fields": [
        "Explain precision, recall, and F1 score in the context of toxicity classification.",
        "<div class=\"concept\">Concept: Metrics for binary classification: precision (positive predictive value), recall (sensitivity), F1 (harmonic mean).</div><div class=\"intuition\">Intuition: Precision: How many flagged toxic are truly toxic? Recall: How many toxic were caught?</div><div class=\"mechanics\">Mechanics: Precision = TP/(TP+FP); Recall = TP/(TP+FN); F1 = 2PR/(P+R).</div><div class=\"tradeoffs\">Trade-offs: Tradeoff between catching all toxic (high recall) vs. avoiding false alarms (high precision).</div><div class=\"applications\">Applications: Evaluating imbalanced classifiers like in spam/toxicity detection.</div><div class=\"memory-hook\">Memory Hook: Precision as a precise archer (hits what aims for); recall as a net (catches most fish).</div>",
        "Metrics Classification Theory",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-7066930388441684764-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Metrics",
        "Classification",
        "Theory"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-845854082267122280-2217",
      "fields": [
        "What is the curse of dimensionality, and how does it affect NLP tasks like topic modeling?",
        "<div class=\"concept\">Concept: Data becomes sparse in high dimensions, making distances meaningless and models prone to overfitting.</div><div class=\"intuition\">Intuition: In high-D space, points are like isolated islands—hard to find neighbors.</div><div class=\"mechanics\">Mechanics: Volume grows exponentially; requires exponentially more data.</div><div class=\"tradeoffs\">Trade-offs: High dims capture nuance but increase computation and noise.</div><div class=\"applications\">Applications: TF-IDF vectors in large vocabularies; mitigated by dim reduction.</div><div class=\"memory-hook\">Memory Hook: Curse as a balloon inflating—surface area grows, but inside feels empty.</div>",
        "Curse of Dimensionality Intuition NLP",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-845854082267122280-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Curse of Dimensionality",
        "Intuition",
        "NLP"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-2549824766989118914-2217",
      "fields": [
        "Compare TruncatedSVD and PCA for dimensionality reduction in sparse NLP data.",
        "<div class=\"concept\">Concept: Both use SVD; PCA centers data, TruncatedSVD does not.</div><div class=\"intuition\">Intuition: PCA adjusts for mean like leveling a table; TruncatedSVD skips for efficiency.</div><div class=\"mechanics\">Mechanics: PCA: Subtract mean then SVD; TruncatedSVD: Direct on sparse matrices.</div><div class=\"tradeoffs\">Trade-offs: TruncatedSVD faster on sparse data; PCA better for dense with mean variance.</div><div class=\"applications\">Applications: TruncatedSVD for TF-IDF in LSA; PCA for general dim reduction.</div><div class=\"memory-hook\">Memory Hook: TruncatedSVD as a quick sketch; PCA as a polished portrait.</div>",
        "TruncatedSVD PCA Comparison",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-2549824766989118914-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "TruncatedSVD",
        "PCA",
        "Comparison"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--412134941483611528-2217",
      "fields": [
        "What are alternative dimensionality reduction methods to SVD, and their trade-offs in NLP?",
        "<div class=\"concept\">Concept: Random Projection (RP) and Nonnegative Matrix Factorization (NMF).</div><div class=\"intuition\">Intuition: RP randomly projects; NMF enforces nonnegativity like positive word counts.</div><div class=\"mechanics\">Mechanics: RP: Stochastic projection; NMF: Factorizes into nonnegative matrices.</div><div class=\"tradeoffs\">Trade-offs: RP fast but less accurate; NMF interpretable but slower than SVD.</div><div class=\"applications\">Applications: RP for very high-D data; NMF for topic modeling with positivity.</div><div class=\"memory-hook\">Memory Hook: RP as a random dart throw; NMF as sorting positives only.</div>",
        "Dim Reduction Alternatives Trade-offs",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--412134941483611528-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Dim Reduction",
        "Alternatives",
        "Trade-offs"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-3387919072356836412-2217",
      "fields": [
        "List and explain common distance metrics used in semantic vector spaces.",
        "<div class=\"concept\">Concept: Measures like Euclidean (L2), Manhattan (L1), Cosine for vector similarity.</div><div class=\"intuition\">Intuition: Euclidean: straight-line distance; Cosine: angle ignoring magnitude.</div><div class=\"mechanics\">Mechanics: Cosine = dot product / (norms); Euclidean = sqrt(sum squared diffs).</div><div class=\"tradeoffs\">Trade-offs: Cosine robust to length; Euclidean sensitive to scale.</div><div class=\"applications\">Applications: Cosine in text similarity; Euclidean in clustering.</div><div class=\"memory-hook\">Memory Hook: Cosine as comparing directions on a compass; Euclidean as map miles.</div>",
        "Distance Metrics Theory Semantics",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-3387919072356836412-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Distance Metrics",
        "Theory",
        "Semantics"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-7833553660983466440-2217",
      "fields": [
        "How does steering with feedback improve topic vectors, and what's an example application?",
        "<div class=\"concept\">Concept: Adjusts distances based on labeled similarities to minimize a cost function.</div><div class=\"intuition\">Intuition: Like nudging vectors closer if they should be similar, based on feedback.</div><div class=\"mechanics\">Mechanics: Compute mean differences (e.g., centroids) and add bias to vectors.</div><div class=\"tradeoffs\">Trade-offs: Improves relevance but requires labeled data; risk of bias amplification.</div><div class=\"applications\">Applications: Resume-job matching by steering good pairs closer.</div><div class=\"memory-hook\">Memory Hook: Steering as herding sheep—guide vectors to cluster meaningfully.</div>",
        "Steering Feedback Application",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-7833553660983466440-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Steering",
        "Feedback",
        "Application"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--7348633745311032014-2217",
      "fields": [
        "What is semantic search, and why is it challenging with topic vectors?",
        "<div class=\"concept\">Concept: Search based on meaning, using topic vectors for similarity.</div><div class=\"intuition\">Intuition: Finds docs with similar semantics, not just keywords.</div><div class=\"mechanics\">Mechanics: Compute cosine similarity between query and doc vectors.</div><div class=\"tradeoffs\">Trade-offs: More accurate than keyword but O(N) exhaustive search.</div><div class=\"applications\">Applications: Advanced search engines like Google for intent matching.</div><div class=\"memory-hook\">Memory Hook: Semantic search as mind-reading—understands query intent.</div>",
        "Semantic Search Intuition Challenges",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid--7348633745311032014-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Semantic Search",
        "Intuition",
        "Challenges"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-4577917022621451147-2217",
      "fields": [
        "Explain why exact indexing fails for high-D topic vectors and how approximate methods help.",
        "<div class=\"concept\">Concept: Curse of dimensionality makes exact indexes inefficient.</div><div class=\"intuition\">Intuition: High-D spaces are vast; points sparse, distances uniform.</div><div class=\"mechanics\">Mechanics: Inverted indexes work for sparse; dense needs LSH for approx NN.</div><div class=\"tradeoffs\">Trade-offs: Approx (e.g., ANN) fast but may miss some matches.</div><div class=\"applications\">Applications: Large-scale semantic search with FAISS or Annoy.</div><div class=\"memory-hook\">Memory Hook: High-D as a haystack universe—approx as a magnet for needles.</div>",
        "Indexing High-D Trade-offs",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid-4577917022621451147-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Indexing",
        "High-D",
        "Trade-offs"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--46174162612765809-2217",
      "fields": [
        "How does using LSA improve a question-answering bot over TF-IDF alone?",
        "<div class=\"concept\">Concept: LSA captures semantics, allowing synonym and concept matching.</div><div class=\"intuition\">Intuition: Handles variations like 'decrease' vs. 'reduce' overfitting.</div><div class=\"mechanics\">Mechanics: Transform TF-IDF to low-D topics; find max cosine similarity.</div><div class=\"tradeoffs\">Trade-offs: Better generalization but added computation for SVD.</div><div class=\"applications\">Applications: FAQ bots for robust query handling.</div><div class=\"memory-hook\">Memory Hook: LSA bot as a wise owl—sees beyond words to meaning.</div>",
        "QA Bot LSA Application",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid--46174162612765809-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "QA Bot",
        "LSA",
        "Application"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--4981864005006486480-2217",
      "fields": [
        "Discuss connections between topic modeling and other ML techniques like clustering.",
        "<div class=\"concept\">Concept: Topic modeling as soft clustering of words/documents.</div><div class=\"intuition\">Intuition: Topics as clusters; words assigned probabilistically.</div><div class=\"mechanics\">Mechanics: LDiA similar to GMM; LSA to PCA-based clustering.</div><div class=\"tradeoffs\">Trade-offs: Soft assignments handle ambiguity; harder to interpret than hard clusters.</div><div class=\"applications\">Applications: Document grouping, anomaly detection.</div><div class=\"memory-hook\">Memory Hook: Topics as fuzzy clouds; clustering as sharp borders.</div>",
        "Topic Modeling Connections Clustering",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--4981864005006486480-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Topic Modeling",
        "Connections",
        "Clustering"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note--8086005496424954492-2217",
      "fields": [
        "What edge cases might cause LDiA to perform poorly, and how to mitigate?",
        "<div class=\"concept\">Concept: Sensitive to short docs, imbalanced topics, or poor hyperparams.</div><div class=\"intuition\">Intuition: Short texts lack co-occurrences; wrong topic count distorts.</div><div class=\"mechanics\">Mechanics: Tune num_topics with coherence scores; preprocess for balance.</div><div class=\"tradeoffs\">Trade-offs: More topics capture nuance but increase noise/overfitting.</div><div class=\"applications\">Applications: Twitter analysis—use aggregation or alternatives like BERT.</div><div class=\"memory-hook\">Memory Hook: LDiA as a picky eater—needs balanced, meaty docs.</div>",
        "LDiA Edge Cases Mitigation",
        "Hard"
      ],
      "flags": 0,
      "guid": "guid--8086005496424954492-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "LDiA",
        "Edge Cases",
        "Mitigation"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-256905777203979191-2217",
      "fields": [
        "How do polysemy and synonymy challenge TF-IDF, and how does LSA address them?",
        "<div class=\"concept\">Concept: Polysemy: multiple meanings; Synonymy: different words same meaning.</div><div class=\"intuition\">Intuition: TF-IDF confuses 'band' meanings; ignores 'pretty/beautiful'.</div><div class=\"mechanics\">Mechanics: LSA groups via co-occurrences in latent space.</div><div class=\"tradeoffs\">Trade-offs: LSA resolves but may merge unrelated if co-occur.</div><div class=\"applications\">Applications: Improving search recall in ambiguous queries.</div><div class=\"memory-hook\">Memory Hook: TF-IDF as literal translator; LSA as context-savvy interpreter.</div>",
        "Polysemy Synonymy LSA",
        "Medium"
      ],
      "flags": 0,
      "guid": "guid-256905777203979191-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Polysemy",
        "Synonymy",
        "LSA"
      ],
      "__type__": "Note"
    },
    {
      "crowdanki_uuid": "note-1391717383663867072-2217",
      "fields": [
        "What is a hyperparameter table, and why is it useful in NLP experiments?",
        "<div class=\"concept\">Concept: Table recording model params and performance metrics.</div><div class=\"intuition\">Intuition: Tracks experiments like a lab notebook for comparison.</div><div class=\"mechanics\">Mechanics: Columns: features, accuracy, F1; rows: model variants.</div><div class=\"tradeoffs\">Trade-offs: Organizes but requires discipline to maintain.</div><div class=\"applications\">Applications: Tuning toxicity classifiers across vector types.</div><div class=\"memory-hook\">Memory Hook: Hyperparam table as a scorecard in a tournament of models.</div>",
        "Hyperparameter Table Experimentation NLP",
        "Easy"
      ],
      "flags": 0,
      "guid": "guid-1391717383663867072-2217",
      "note_model_uuid": "nlp-comprehensive-note-model",
      "tags": [
        "Hyperparameter Table",
        "Experimentation",
        "NLP"
      ],
      "__type__": "Note"
    }
  ]
}