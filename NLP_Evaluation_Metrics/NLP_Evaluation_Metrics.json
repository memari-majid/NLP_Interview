{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-evaluation_metrics-deck",
  "deck_config_uuid": "nlp-evaluation_metrics-config",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-evaluation_metrics-config",
      "dyn": false,
      "name": "Evaluation & Metrics",
      "new": {
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 20
      },
      "rev": {
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "perDay": 100
      }
    }
  ],
  "desc": "How to measure NLP model performance",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "Evaluation & Metrics",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-evaluation_metrics-model",
      "css": "\n.card {\n    font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n    font-size: 18px;\n    line-height: 1.5;\n    color: #2c3e50;\n    background: #f8f9fa;\n    text-align: left;\n    padding: 20px;\n    border-radius: 8px;\n    max-width: 100%;\n    margin: 0 auto;\n}\n\n@media (max-width: 480px) {\n    .card {\n        font-size: 16px;\n        padding: 15px;\n        line-height: 1.4;\n    }\n}\n\nb, strong {\n    color: #2980b9;\n    font-weight: 600;\n}\n\n.metadata {\n    margin-top: 15px;\n    padding-top: 10px;\n    border-top: 1px solid #bdc3c7;\n    font-size: 12px;\n    color: #7f8c8d;\n    text-align: center;\n}\n",
      "flds": [
        {
          "name": "Front",
          "ord": 0,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Back",
          "ord": 1,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Topic",
          "ord": 2,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 16
        },
        {
          "name": "Type",
          "ord": 3,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 14
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\\special{papersize=3in,5in}\\usepackage{amssymb,amsmath}\\pagestyle{empty}\\setlength{\\parindent}{0in}\\begin{document}",
      "name": "NLP Evaluation & Metrics",
      "req": [
        [
          0,
          "all",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}<hr id=answer>{{Back}}<br><br><div class='metadata'><span style='background: #3498db; color: white; padding: 4px 8px; border-radius: 4px; font-weight: 600;'>{{Topic}}</span> • <span style='background: #95a5a6; color: white; padding: 4px 8px; border-radius: 4px; margin-left: 8px;'>{{Type}}</span></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "{{Front}}"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "What is the difference between precision and recall?",
        "<b>Precision:</b> TP/(TP+FP) - accuracy of positive predictions<br>\n<b>Recall:</b> TP/(TP+FN) - fraction of positives found<br>\n<b>Trade-off:</b> High precision → low recall, vice versa<br>\n<b>F1:</b> Harmonic mean balances both metrics",
        "Evaluation & Metrics",
        "concept"
      ],
      "guid": "nlp_74a3c6e971e06",
      "note_model_uuid": "nlp-evaluation_metrics-model",
      "tags": [
        "evaluation__metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is BLEU score?",
        "<b>Use:</b> Evaluate machine translation quality<br>\n<b>Method:</b> N-gram overlap between prediction and reference<br>\n<b>Range:</b> 0-100, higher is better<br>\n<b>Limitations:</b> Doesn't consider semantics, multiple valid translations",
        "Evaluation & Metrics",
        "concept"
      ],
      "guid": "nlp_442b37da4cbf8",
      "note_model_uuid": "nlp-evaluation_metrics-model",
      "tags": [
        "evaluation__metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is ROUGE score?",
        "<b>Use:</b> Evaluate text summarization quality<br>\n<b>Types:</b> ROUGE-N (n-gram), ROUGE-L (LCS), ROUGE-S (skip-gram)<br>\n<b>Method:</b> Overlap between generated and reference summaries<br>\n<b>Interpretation:</b> Higher scores indicate better summaries",
        "Evaluation & Metrics",
        "concept"
      ],
      "guid": "nlp_4d612fcea12b4",
      "note_model_uuid": "nlp-evaluation_metrics-model",
      "tags": [
        "evaluation__metrics",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "How do you evaluate language models?",
        "<b>Perplexity:</b> How well model predicts held-out text<br>\n<b>Downstream tasks:</b> Performance on specific applications<br>\n<b>Human evaluation:</b> Fluency, coherence, relevance<br>\n<b>Intrinsic vs extrinsic:</b> Model-specific vs task-specific",
        "Evaluation & Metrics",
        "concept"
      ],
      "guid": "nlp_33e567f947c09",
      "note_model_uuid": "nlp-evaluation_metrics-model",
      "tags": [
        "evaluation__metrics",
        "concept"
      ]
    }
  ]
}