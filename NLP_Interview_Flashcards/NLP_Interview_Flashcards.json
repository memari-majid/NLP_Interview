{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-interview-deck-rules-optimized-2024",
  "deck_config_uuid": "nlp-deck-config-rules-optimized",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-deck-config-rules-optimized",
      "dyn": false,
      "name": "NLP Interview Prep (Rules Optimized)",
      "new": {
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 25
      },
      "rev": {
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "perDay": 100
      }
    }
  ],
  "desc": "Rules-optimized NLP interview cards following research-backed design principles for maximum memorization effectiveness and rapid recall under interview pressure.",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "NLP Interview Prep (Rules Optimized)",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-model-rules-optimized",
      "css": ".card {\n    font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n    font-size: 18px;\n    line-height: 1.6;\n    color: #2c3e50;\n    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n    text-align: left;\n    padding: 20px;\n    border-radius: 12px;\n    box-shadow: 0 4px 20px rgba(0,0,0,0.1);\n    max-width: 100%;\n    margin: 0 auto;\n}\n\n/* Mobile-first responsive design */\n@media (max-width: 480px) {\n    .card {\n        font-size: 16px;\n        padding: 15px;\n        margin: 10px;\n    }\n}\n\n/* Typography hierarchy */\nh1, h2, h3 {\n    color: #34495e;\n    margin-top: 0;\n    font-weight: 600;\n}\n\nh2 {\n    font-size: 20px;\n    color: #3498db;\n    border-bottom: 2px solid #3498db;\n    padding-bottom: 5px;\n}\n\nh3 {\n    font-size: 18px;\n    color: #e74c3c;\n}\n\n/* Color coding system */\n.concept { color: #3498db; font-weight: 600; }\n.implementation { color: #27ae60; }\n.formula { color: #f39c12; font-weight: 700; }\n.edge-case { color: #e74c3c; }\n.interview-tip { color: #9b59b6; font-style: italic; }\n\n/* Code styling for mobile */\npre, code {\n    font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;\n    font-size: 14px;\n    background: #2c3e50;\n    color: #ecf0f1;\n    padding: 12px;\n    border-radius: 8px;\n    overflow-x: auto;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    line-height: 1.4;\n    margin: 10px 0;\n}\n\n@media (max-width: 480px) {\n    pre, code {\n        font-size: 13px;\n        padding: 8px;\n    }\n}\n\n/* Inline code */\ncode {\n    display: inline;\n    padding: 2px 6px;\n    background: #34495e;\n    border-radius: 4px;\n}\n\n/* Metadata styling */\n.metadata {\n    margin-top: 20px;\n    padding-top: 15px;\n    border-top: 1px solid #bdc3c7;\n    font-size: 12px;\n    color: #7f8c8d;\n    text-align: center;\n}\n\n.topic {\n    background: #3498db;\n    color: white;\n    padding: 4px 8px;\n    border-radius: 4px;\n    font-weight: 600;\n}\n\n.type {\n    background: #95a5a6;\n    color: white;\n    padding: 4px 8px;\n    border-radius: 4px;\n    margin-left: 8px;\n}\n\n/* Visual emphasis */\nstrong, b {\n    color: #2c3e50;\n    font-weight: 700;\n}\n\nem, i {\n    color: #7f8c8d;\n    font-style: italic;\n}\n\n/* Interactive elements */\ndetails {\n    margin: 10px 0;\n    padding: 10px;\n    background: rgba(52, 152, 219, 0.1);\n    border-radius: 6px;\n    border-left: 4px solid #3498db;\n}\n\nsummary {\n    cursor: pointer;\n    font-weight: 600;\n    color: #3498db;\n    outline: none;\n}\n\n/* Lists */\nul, ol {\n    padding-left: 20px;\n}\n\nli {\n    margin-bottom: 8px;\n}\n\n/* Focus on readability */\np {\n    margin-bottom: 15px;\n}\n\n/* Answer separator */\nhr#answer {\n    border: none;\n    height: 2px;\n    background: linear-gradient(90deg, #3498db, #9b59b6);\n    margin: 20px 0;\n    border-radius: 1px;\n}",
      "flds": [
        {
          "name": "Front",
          "ord": 0,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Back",
          "ord": 1,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Topic",
          "ord": 2,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 16
        },
        {
          "name": "Type",
          "ord": 3,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 14
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\\special{papersize=3in,5in}\\usepackage{amssymb,amsmath}\\pagestyle{empty}\\setlength{\\parindent}{0in}\\begin{document}",
      "name": "NLP Rules Optimized",
      "req": [
        [
          0,
          "all",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}<hr id=answer>{{Back}}<br><br><div class='metadata'><span class='topic'>{{Topic}}</span> • <span class='type'>{{Type}}</span></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "{{Front}}"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Self-Attention from Scratch</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 25 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Attention Mechanisms",
        "problem_understanding"
      ],
      "guid": "nlp_657d72a490e8f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>softmax()</code>",
        "<pre><code>def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"\n    Numerically stable softmax implementation.\n    \n    Why stable? Subtracting max prevents overflow when exponentiating large numbers.\n    This is critical for attention weights which can have large values.\n    \"\"\"\n    # Subtract maximum value for numerical stability\n    # This doesn't change the relative probabilities but prevents exp() overflow\n    x_max = np.max(x, axis=axis, keepdims=True)\n    exp_x = np.exp(x - x_max)\n    \n    # Normalize to get probabilities (sum to 1 along specified axis)\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_5f56d39bfd36c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>self_attention_part_1()</code>",
        "<pre><code>def self_attention(X: np.ndarray, d_k: int) -> np.ndarray:\n    \"\"\"\n    Implement scaled dot-product self-attention mechanism.\n    \n    This is the CORE of all transformer models (BERT, GPT, etc.)\n    \n    Formula: Attention(Q,K,V) = softmax(QK^T / √d_k)V\n    \n    Args:\n        X: Input embeddings (seq_len, d_model)\n        d_k: Dimension of queries and keys (for scaling)\n    \n    Returns:\n        Attention output (seq_len, d_model)\n    \"\"\"\n    seq_len, d_model = X.shape</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_dfa23f65d83e2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>self_attention_part_2()</code>",
        "<pre><code>    \n    # STEP 1: Initialize weight matrices\n    # In practice, these are learned parameters\n    # Using small random values for demonstration\n    np.random.seed(42)  # For reproducible results in interview\n    W_q = np.random.randn(d_model, d_k) * 0.1    # Query projection\n    W_k = np.random.randn(d_model, d_k) * 0.1    # Key projection  \n    W_v = np.random.randn(d_model, d_model) * 0.1 # Value projection\n    \n    # STEP 2: Create Query, Key, Value matrices\n    # These are linear transformations of the input\n    # Q: what we're looking for, K: what we're comparing against, V: what we return\n    Q = X @ W_q  # Shape: (seq_len, d_k)\n    K = X @ W_k  # Shape: (seq_len, d_k)  \n    V = X @ W_v  # Shape: (seq_len, d_model)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_62b152ef236c7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>self_attention_part_3()</code>",
        "<pre><code>    \n    # STEP 3: Calculate attention scores\n    # QK^T gives us similarity between all pairs of positions\n    scores = Q @ K.T  # Shape: (seq_len, seq_len)\n    \n    # STEP 4: Scale by sqrt(d_k)\n    # Prevents dot products from getting too large (which makes softmax too peaked)\n    # This scaling is CRUCIAL for stable training\n    scores = scores / np.sqrt(d_k)\n    \n    # STEP 5: Apply softmax to get attention weights\n    # Each row sums to 1.0 - these are the attention probabilities\n    # Higher score = more attention to that position\n    attention_weights = softmax(scores, axis=-1)\n    </code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_40f74a24e0427",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>self_attention_part_4()</code>",
        "<pre><code>    # STEP 6: Apply attention to values\n    # Weighted sum of value vectors based on attention weights\n    # This is where information actually flows between positions\n    output = attention_weights @ V  # Shape: (seq_len, d_model)\n    \n    return output</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_104cab8f007fd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>self_attention_with_mask_part_1()</code>",
        "<pre><code>def self_attention_with_mask(X: np.ndarray, d_k: int, mask: np.ndarray = None) -> np.ndarray:\n    \"\"\"\n    Self-attention with causal masking (for GPT-style models).\n    \n    Causal mask prevents positions from attending to future positions.\n    Essential for autoregressive generation (predict next token).\n    \"\"\"\n    seq_len, d_model = X.shape\n    \n    # Same weight initialization as before\n    np.random.seed(42)\n    W_q = np.random.randn(d_model, d_k) * 0.1\n    W_k = np.random.randn(d_model, d_k) * 0.1\n    W_v = np.random.randn(d_model, d_model) * 0.1\n    </code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_484def3d36688",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>self_attention_with_mask_part_2()</code>",
        "<pre><code>    # Create Q, K, V matrices\n    Q = X @ W_q\n    K = X @ W_k  \n    V = X @ W_v\n    \n    # Calculate attention scores\n    scores = (Q @ K.T) / np.sqrt(d_k)\n    \n    # STEP: Apply mask BEFORE softmax\n    # Masked positions get large negative values (-inf conceptually)\n    # After softmax, these become ~0 probability\n    if mask is not None:\n        # Add large negative value to masked positions\n        scores = scores + (mask * -1e9)\n    </code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_3fb69243f3ec3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>self_attention_with_mask_part_3()</code>",
        "<pre><code>    # Apply softmax and compute output\n    attention_weights = softmax(scores, axis=-1)\n    output = attention_weights @ V\n    \n    return output</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_f9383b182db1e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>create_causal_mask()</code>",
        "<pre><code>def create_causal_mask(seq_len: int) -> np.ndarray:\n    \"\"\"\n    Create causal mask for autoregressive attention.\n    \n    Used in GPT to prevent \"looking into the future\" during training.\n    \n    Returns:\n        Mask matrix where 1 = mask (don't attend), 0 = allow\n    \"\"\"\n    # Create lower triangular matrix (1s below and on diagonal)\n    # This allows attending to current and previous positions only\n    lower_triangle = np.tril(np.ones((seq_len, seq_len)))\n    \n    # Convert to mask format: 0 = allow attention, 1 = mask\n    return 1 - lower_triangle</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_429320bc567c4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>multi_head_attention_part_1()</code>",
        "<pre><code>def multi_head_attention(X: np.ndarray, d_k: int, num_heads: int = 8) -> np.ndarray:\n    \"\"\"\n    Multi-head self-attention (simplified version).\n    \n    Key insight: Multiple attention heads can focus on different aspects\n    - Head 1 might focus on syntax\n    - Head 2 might focus on semantics\n    - Head 3 might focus on long-range dependencies\n    \"\"\"\n    seq_len, d_model = X.shape\n    \n    # REQUIREMENT: d_model must be divisible by num_heads\n    # Each head gets d_model/num_heads dimensions\n    assert d_model % num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n    \n    head_dim = d_model // num_heads\n    outputs = []</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_ff4fe6548386f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Implement: <code>multi_head_attention_part_2()</code>",
        "<pre><code>    \n    # STEP: Apply attention for each head independently\n    for head in range(num_heads):\n        # Each head operates on a subset of the input dimensions\n        start_idx = head * head_dim\n        end_idx = start_idx + head_dim\n        X_head = X[:, start_idx:end_idx]\n        \n        # Apply self-attention to this head's slice\n        head_output = self_attention(X_head, head_dim)\n        outputs.append(head_output)\n    \n    # STEP: Concatenate all head outputs\n    # This gives us the full d_model dimensional output\n    return np.concatenate(outputs, axis=-1)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_aec5c598d6d5c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Write the Attention Mechanism",
        "<h3>Attention(Q,K,V) = softmax(QK^T / √d_k)V</h3><br><p><i>Attention(Q,K,V) = softmax(QK^T/√d)V</i></p><br><details><summary>Context</summary><pre>This is the CORE of all transformer models (BERT, GPT, etc.)\nFormula: Attention(Q,K,V) = softmax(QK^T / √d_k)V\nArgs:</pre></details>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_8fb65deecbcd3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>What's the Time Complexity?",
        "<b>O(n²d)</b><br><i>See Big O notation reference</i>",
        "Attention Mechanisms",
        "complexity"
      ],
      "guid": "nlp_3dcf672bc1874",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>What's the Space Complexity?",
        "<b>O(n²)</b><br><i>Quadratic time - nested loops, can be slow for large inputs</i>",
        "Attention Mechanisms",
        "complexity"
      ],
      "guid": "nlp_7d87fc96709e5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMONSTRATION CODE</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_4d69cfb0fec7e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Attention allows models to focus on relevant parts of input</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_2c4c521b152a9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Self-attention computes relationships between all positions</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_17dc77b4c1ad2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Scaled dot-product prevents vanishing gradients</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_44e3f85dc085b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Bag of Words from Scratch</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 20 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Bow Vectors",
        "problem_understanding"
      ],
      "guid": "nlp_27e9f00e615b5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>create_bow_vector_part_1()</code>",
        "<pre><code>def create_bow_vector(documents: List[str]) -> Tuple[List[str], List[List[int]]]:\n    \"\"\"\n    Create bag-of-words representation from scratch.\n    \n    BAG-OF-WORDS CONCEPT:\n    - Represent text as vector of word counts\n    - \"Order doesn't matter, just word presence/frequency\"\n    - Foundation of many NLP systems before embeddings\n    \n    STEPS:\n    1. Build vocabulary (all unique words)\n    2. For each document, count occurrences of each vocab word\n    3. Return vocabulary and count vectors\n    \n    INTERVIEW INSIGHT: Simple but effective. Foundation for TF-IDF.\n    \"\"\"</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_cacb9d0ba7723",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>create_bow_vector_part_2()</code>",
        "<pre><code>    \n    # STEP 1: Handle edge case\n    if not documents:\n        return [], []\n    \n    # STEP 2: Build vocabulary from all documents\n    # We need consistent vocabulary across all documents for vector comparison\n    vocab_set = set()\n    \n    for doc in documents:\n        # Simple tokenization: lowercase and split on whitespace\n        # In interviews, mention this could be more sophisticated\n        words = doc.lower().split()\n        vocab_set.update(words)\n    </code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_e7a6cd438dc34",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>create_bow_vector_part_3()</code>",
        "<pre><code>    # STEP 3: Create ordered vocabulary\n    # Sorting ensures consistent feature ordering across runs\n    vocabulary = sorted(list(vocab_set))\n    \n    # Create word-to-index mapping for efficient lookup\n    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n    \n    # STEP 4: Convert each document to vector\n    vectors = []\n    \n    for doc in documents:\n        # Tokenize document\n        words = doc.lower().split()\n        \n        # Count word occurrences in this document\n        word_counts = Counter(words)</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_ead0ef8e6c73a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>create_bow_vector_part_4()</code>",
        "<pre><code>        \n        # Create vector: count for each vocabulary word\n        # Index i contains count of vocabulary[i] in this document\n        vector = []\n        for word in vocabulary:\n            count = word_counts.get(word, 0)  # 0 if word not in document\n            vector.append(count)\n        \n        vectors.append(vector)\n    \n    return vocabulary, vectors</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_9b17b98543ea0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>cosine_similarity_part_1()</code>",
        "<pre><code>def cosine_similarity(vec1: List[int], vec2: List[int]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two BoW vectors.\n    \n    COSINE SIMILARITY INTUITION:\n    - Measures angle between vectors, not magnitude\n    - Good for text: \"I love cats\" vs \"I really really love cats\"\n    - Both have same direction (similar meaning) despite different lengths\n    \n    FORMULA: cos(θ) = (A·B) / (||A|| × ||B||)\n    \n    INTERVIEW TIP: Always explain why cosine > Euclidean for text\n    \"\"\"\n    \n    # STEP 1: Input validation\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_63674f8cd38f8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>cosine_similarity_part_2()</code>",
        "<pre><code>    \n    # STEP 2: Calculate dot product (numerator)\n    # Sum of element-wise products\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    \n    # STEP 3: Calculate vector norms (denominators)\n    # ||v|| = sqrt(sum of squared elements) = vector magnitude\n    norm1 = math.sqrt(sum(x * x for x in vec1))\n    norm2 = math.sqrt(sum(x * x for x in vec2))\n    \n    # STEP 4: Handle zero vectors (edge case)\n    # Zero vector has no direction, so similarity is undefined\n    if norm1 == 0 or norm2 == 0:\n        return 0.0\n    </code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_60d7525245055",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>cosine_similarity_part_3()</code>",
        "<pre><code>    # STEP 5: Return cosine similarity\n    # Result is between -1 (opposite) and 1 (identical)\n    # For BoW with counts, result is between 0 and 1\n    return dot_product / (norm1 * norm2)</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_1231087867c55",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>find_most_similar_part_1()</code>",
        "<pre><code>def find_most_similar(documents: List[str], query_idx: int) -> int:\n    \"\"\"\n    Find document most similar to query document.\n    \n    REAL-WORLD APPLICATION:\n    - Document search and retrieval\n    - \"Find documents similar to this one\"\n    - Recommendation systems (\"users who read this also read...\")\n    \n    ALGORITHM:\n    1. Convert all documents to BoW vectors\n    2. Calculate similarity between query and each document\n    3. Return index of most similar document\n    \"\"\"\n    </code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_93ae66d08030b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>find_most_similar_part_2()</code>",
        "<pre><code>    # STEP 1: Convert documents to BoW representation\n    vocab, vectors = create_bow_vector(documents)\n    \n    # STEP 2: Validate query index\n    if query_idx >= len(vectors) or query_idx < 0:\n        return -1  # Invalid query index\n    \n    # STEP 3: Get query vector\n    query_vector = vectors[query_idx]\n    \n    # STEP 4: Calculate similarity with all other documents\n    max_similarity = -1  # Start with impossible low value\n    most_similar_idx = -1\n    \n    for i, doc_vector in enumerate(vectors):</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_f8c3948589a5b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>find_most_similar_part_3()</code>",
        "<pre><code>        # Don't compare document with itself\n        if i != query_idx:\n            similarity = cosine_similarity(query_vector, doc_vector)\n            \n            # Track the highest similarity found\n            if similarity > max_similarity:\n                max_similarity = similarity\n                most_similar_idx = i\n    \n    return most_similar_idx</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_43653afdfbffe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>analyze_vocabulary_distribution_part_1()</code>",
        "<pre><code>def analyze_vocabulary_distribution(documents: List[str]) -> Dict[str, int]:\n    \"\"\"\n    Analyze vocabulary characteristics - good for follow-up questions.\n    \n    INTERVIEW INSIGHTS:\n    - Most words appear very rarely (Zipf's law)\n    - Top 100 words account for ~50% of text\n    - Vocabulary size grows with more documents\n    \"\"\"\n    vocab, _ = create_bow_vector(documents)\n    \n    # Count how often each word appears across documents\n    word_doc_counts = {}\n    for word in vocab:\n        count = sum(1 for doc in documents if word in doc.lower())\n        word_doc_counts[word] = count</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_f219c66415460",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Implement: <code>analyze_vocabulary_distribution_part_2()</code>",
        "<pre><code>    \n    return word_doc_counts</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_1b653d2c51a0c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>What's the Time Complexity?",
        "<b>O(d × n × v)</b><br><i>See Big O notation reference</i>",
        "Bow Vectors",
        "complexity"
      ],
      "guid": "nlp_79d0f8097a625",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>What's the Space Complexity?",
        "<b>O(d × v)</b><br><i>See Big O notation reference</i>",
        "Bow Vectors",
        "complexity"
      ],
      "guid": "nlp_103f2daf77b26",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Text CNN for Classification</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 25 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Cnn Text",
        "problem_understanding"
      ],
      "guid": "nlp_59857999bc877",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>text_to_sequence()</code>",
        "<pre><code>def text_to_sequence(text: str, vocab: Dict[str, int], max_len: int = 10) -> List[int]:\n    \"\"\"Convert text to padded integer sequence.\"\"\"\n    words = text.lower().split()\n    sequence = [vocab.get(word, 0) for word in words]  # 0 for unknown words\n    \n    # Pad or truncate to max_len\n    if len(sequence) < max_len:\n        sequence += [0] * (max_len - len(sequence))\n    else:\n        sequence = sequence[:max_len]\n    \n    return sequence</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_02ed750c904d0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>embedding_lookup()</code>",
        "<pre><code>def embedding_lookup(sequence: List[int], embedding_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Look up embeddings for sequence.\"\"\"\n    return np.array([embedding_matrix[idx] for idx in sequence])</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_be1b09a078a72",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>conv1d_part_1()</code>",
        "<pre><code>def conv1d(embeddings: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n    \"\"\"Apply 1D convolution with kernel size 3.\"\"\"\n    seq_len, embed_dim = embeddings.shape\n    kernel_size = len(kernel)\n    \n    conv_output = []\n    \n    # Slide kernel over sequence\n    for i in range(seq_len - kernel_size + 1):\n        window = embeddings[i:i + kernel_size]  # Shape: (3, embed_dim)\n        \n        # Element-wise multiply and sum\n        conv_value = np.sum(window * kernel[:, np.newaxis])\n        conv_output.append(max(0, conv_value))  # ReLU activation\n    \n    return np.array(conv_output)</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_e998d07d85683",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>max_pool()</code>",
        "<pre><code>def max_pool(conv_output: np.ndarray) -> float:\n    \"\"\"Global max pooling.\"\"\"\n    return np.max(conv_output) if len(conv_output) > 0 else 0.0</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_1bfa7e0406939",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>sigmoid()</code>",
        "<pre><code>def sigmoid(x: float) -> float:\n    \"\"\"Sigmoid activation.\"\"\"\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_3bee57886219f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>text_cnn_predict_part_1()</code>",
        "<pre><code>def text_cnn_predict(text: str, vocab: Dict[str, int], \n                    weights: Dict, max_len: int = 10) -> float:\n    \"\"\"CNN forward pass for text classification.\"\"\"\n    \n    # 1. Text to sequence\n    sequence = text_to_sequence(text, vocab, max_len)\n    \n    # 2. Embedding lookup\n    embeddings = embedding_lookup(sequence, weights['embedding'])\n    \n    # 3. Convolution\n    conv_output = conv1d(embeddings, weights['conv'])\n    \n    # 4. Max pooling\n    pooled = max_pool(conv_output)</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_0fd4f06c5fa5e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>text_cnn_predict_part_2()</code>",
        "<pre><code>    \n    # 5. Dense layer + sigmoid\n    dense_output = pooled * weights['dense'] + weights['bias']\n    probability = sigmoid(dense_output)\n    \n    return probability</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_34178a33803a3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>create_sample_weights()</code>",
        "<pre><code>def create_sample_weights(vocab_size: int = 100, embed_dim: int = 50):\n    \"\"\"Create sample weights for demonstration.\"\"\"\n    return {\n        'embedding': np.random.randn(vocab_size, embed_dim) * 0.1,\n        'conv': np.random.randn(3) * 0.1,  # Kernel size 3\n        'dense': np.random.randn() * 0.1,\n        'bias': 0.0\n    }</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_212444200ecd4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>test_cnn_part_1()</code>",
        "<pre><code>def test_cnn():\n    # Sample vocabulary\n    vocab = {\n        'good': 1, 'bad': 2, 'movie': 3, 'film': 4, \n        'great': 5, 'terrible': 6, 'love': 7, 'hate': 8\n    }\n    \n    # Sample weights\n    weights = create_sample_weights(vocab_size=len(vocab) + 1, embed_dim=10)\n    \n    # Test sentences\n    test_texts = [\n        \"good movie\",\n        \"bad film\", \n        \"great love\",\n        \"terrible hate\"\n    ]</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_b48c0e4488316",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>Implement: <code>test_cnn_part_2()</code>",
        "<pre><code>    \n    print(\"CNN Text Classification Results:\")\n    for text in test_texts:\n        prob = text_cnn_predict(text, vocab, weights)\n        prediction = \"positive\" if prob > 0.5 else \"negative\"\n        print(f\"'{text}' -> {prob:.3f} ({prediction})\")</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_bbb06dcc3e720",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Word2Vec Skip-gram</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 20 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Embeddings",
        "problem_understanding"
      ],
      "guid": "nlp_8691f93342050",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>sigmoid()</code>",
        "<pre><code>def sigmoid(x: float) -> float:\n    \"\"\"\n    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n    \n    Used in Word2Vec to convert dot products to probabilities.\n    Clamp input to prevent numerical overflow/underflow.\n    \"\"\"\n    # Clamp x to prevent overflow in exp() function\n    # This is crucial for numerical stability\n    clamped_x = max(-500, min(500, x))\n    return 1 / (1 + math.exp(-clamped_x))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_310d869b02d20",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>dot_product()</code>",
        "<pre><code>def dot_product(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"\n    Calculate dot product between two vectors.\n    \n    Dot product measures similarity between vectors:\n    - High positive value = vectors point in same direction (similar)\n    - Zero = vectors are orthogonal (unrelated)\n    - Negative = vectors point in opposite directions (dissimilar)\n    \"\"\"\n    # Element-wise multiplication, then sum\n    return sum(a * b for a, b in zip(vec1, vec2))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_e136b59ea527e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>skipgram_step_part_1()</code>",
        "<pre><code>def skipgram_step(center_word: str, context_word: str, \n                  embeddings: Dict[str, List[float]], \n                  learning_rate: float = 0.01) -> None:\n    \"\"\"\n    Perform one training step of Skip-gram Word2Vec.\n    \n    SKIP-GRAM OBJECTIVE: Given center word, predict context words\n    - Makes words that appear in similar contexts have similar embeddings\n    - \"king\" and \"queen\" both appear near \"royal\", \"crown\" -> similar embeddings\n    \n    TRAINING STEP:\n    1. Calculate current similarity between center and context word\n    2. If they co-occur, increase their similarity (gradient ascent)\n    3. Update both embeddings to be more similar\n    \n    Args:\n        center_word: The word we're using to predict context\n        context_word: The word in the context window\n        embeddings: Current word embeddings (will be modified in-place)\n        learning_rate: How big steps to take during learning\n    \"\"\"</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_7e249b3f8ce49",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>skipgram_step_part_2()</code>",
        "<pre><code>    \n    # STEP 1: Handle missing words\n    # In real implementation, you'd skip unknown words or use subword tokens\n    if center_word not in embeddings or context_word not in embeddings:\n        return  # Skip this training example\n    \n    # STEP 2: Get current embeddings\n    center_vec = embeddings[center_word]     # Current embedding for center word\n    context_vec = embeddings[context_word]   # Current embedding for context word\n    \n    # STEP 3: FORWARD PASS - Calculate current similarity\n    # Dot product measures how similar the embeddings currently are\n    dot_prod = dot_product(center_vec, context_vec)\n    \n    # Convert to probability using sigmoid</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_15f796bb85db7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>skipgram_step_part_3()</code>",
        "<pre><code>    # High dot product -> high probability of co-occurrence\n    prob = sigmoid(dot_prod)\n    \n    # STEP 4: BACKWARD PASS - Calculate gradients\n    # We want to MAXIMIZE the probability of this positive pair\n    # Gradient of log(sigmoid(x)) is (1 - sigmoid(x))\n    gradient_scale = (1 - prob) * learning_rate\n    \n    # STEP 5: UPDATE EMBEDDINGS\n    # Move embeddings to increase their similarity\n    \n    # Update center word embedding:\n    # Add context_word's embedding scaled by gradient\n    for i in range(len(center_vec)):\n        embeddings[center_word][i] += gradient_scale * context_vec[i]</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_dab13aec39bb3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>skipgram_step_part_4()</code>",
        "<pre><code>    \n    # Update context word embedding:\n    # Add center_word's embedding scaled by gradient  \n    for i in range(len(context_vec)):\n        embeddings[context_word][i] += gradient_scale * center_vec[i]</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_fd09eae2944f7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>word_similarity_part_1()</code>",
        "<pre><code>def word_similarity(word1: str, word2: str, \n                   embeddings: Dict[str, List[float]]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two word embeddings.\n    \n    Cosine similarity = dot_product(v1, v2) / (||v1|| × ||v2||)\n    - Returns value between -1 and 1\n    - 1.0 = identical direction (very similar words)\n    - 0.0 = orthogonal (unrelated words)\n    - -1.0 = opposite direction (opposite meaning)\n    \"\"\"\n    \n    # STEP 1: Handle missing words\n    if word1 not in embeddings or word2 not in embeddings:\n        return 0.0  # No similarity if words not in vocabulary</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_9cd336521a75a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>word_similarity_part_2()</code>",
        "<pre><code>    \n    # STEP 2: Get word vectors\n    vec1 = embeddings[word1]\n    vec2 = embeddings[word2]\n    \n    # STEP 3: Calculate dot product (numerator)\n    dot_prod = dot_product(vec1, vec2)\n    \n    # STEP 4: Calculate vector norms (denominators)\n    # ||v|| = sqrt(sum of squared elements)\n    norm1 = math.sqrt(sum(x * x for x in vec1))\n    norm2 = math.sqrt(sum(x * x for x in vec2))\n    \n    # STEP 5: Handle zero vectors (shouldn't happen with proper training)\n    if norm1 == 0 or norm2 == 0:\n        return 0.0</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_21a878af62dfa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>word_similarity_part_3()</code>",
        "<pre><code>    \n    # STEP 6: Return cosine similarity\n    return dot_prod / (norm1 * norm2)</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_a6b70bfad5867",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>find_most_similar_part_1()</code>",
        "<pre><code>def find_most_similar(target_word: str, embeddings: Dict[str, List[float]], \n                     top_k: int = 3) -> List[Tuple[str, float]]:\n    \"\"\"\n    Find k most similar words to target word.\n    \n    This is how you'd implement word analogy and similarity queries.\n    Core functionality of Word2Vec for exploration and evaluation.\n    \"\"\"\n    if target_word not in embeddings:\n        return []  # Target word not in vocabulary\n    \n    # Calculate similarity with all other words\n    similarities = []\n    \n    for word in embeddings:\n        if word != target_word:  # Don't compare word with itself\n            sim = word_similarity(target_word, word, embeddings)\n            similarities.append((word, sim))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_65aed1a824d5f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>Implement: <code>find_most_similar_part_2()</code>",
        "<pre><code>    \n    # Sort by similarity (highest first) and return top-k\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    return similarities[:top_k]</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_32d3fb7e536c4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMONSTRATION</strong></p>",
        "Embeddings",
        "interview_insights"
      ],
      "guid": "nlp_51a2b5d9c2a2f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Embeddings map discrete tokens to continuous vector space</strong></p>",
        "Embeddings",
        "interview_insights"
      ],
      "guid": "nlp_a20d55211a958",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Word2Vec uses skip-gram or CBOW for training</strong></p>",
        "Embeddings",
        "interview_insights"
      ],
      "guid": "nlp_d5ea52efd99aa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Contextual embeddings capture word meaning in context</strong></p>",
        "Embeddings",
        "interview_insights"
      ],
      "guid": "nlp_b8d9d7f966c55",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Example: Anki-Optimized Solution Structure</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 20 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Example Anki Refactor",
        "problem_understanding"
      ],
      "guid": "nlp_4ae277b0314f1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>spell_check_approach()</code>",
        "<pre><code>def spell_check_approach():\n    \"\"\"\n    KEY: Use edit distance + frequency ranking\n    STEPS: 1) Find close words 2) Rank by frequency\n    INSIGHT: Most typos are 1-2 edits away\n    \"\"\"\n    pass</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_8bb01e9dd08c0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>edit_distance_recursive_part_1()</code>",
        "<pre><code>def edit_distance_recursive(s1: str, s2: str) -> int:\n    \"\"\"\n    FORMULA: ED(i,j) = min(\n        ED(i-1,j) + 1,    # deletion\n        ED(i,j-1) + 1,    # insertion  \n        ED(i-1,j-1) + 0/1 # substitution\n    )\n    \"\"\"\n    if not s1: return len(s2)\n    if not s2: return len(s1)\n    \n    if s1[0] == s2[0]:\n        return edit_distance_recursive(s1[1:], s2[1:])\n    \n    return 1 + min(\n        edit_distance_recursive(s1[1:], s2),    # delete\n        edit_distance_recursive(s1, s2[1:]),    # insert\n        edit_distance_recursive(s1[1:], s2[1:]) # replace\n    )</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_c6890ef488333",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>edit_distance_dp_part_1()</code>",
        "<pre><code>def edit_distance_dp(s1: str, s2: str) -> int:\n    \"\"\"\n    COMPLEXITY: O(m*n) time, O(m*n) space\n    OPTIMIZE: Can reduce to O(min(m,n)) space\n    \"\"\"\n    m, n = len(s1), len(s2)\n    dp = [[0] * (n+1) for _ in range(m+1)]\n    \n    # Base cases\n    for i in range(m+1): dp[i][0] = i\n    for j in range(n+1): dp[0][j] = j\n    \n    # Fill matrix\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            if s1[i-1] == s2[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i-1][j],    # delete\n                    dp[i][j-1],    # insert\n                    dp[i-1][j-1]   # replace\n                )</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_0d5a410941046",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>edit_distance_dp_part_2()</code>",
        "<pre><code>    \n    return dp[m][n]</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_113d3f284e90f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>find_candidates_part_1()</code>",
        "<pre><code>def find_candidates(word: str, dictionary: Dict[str, int], \n                   max_distance: int = 2) -> List[Tuple[str, int]]:\n    \"\"\"\n    STRATEGY: Only check words within length ±2\n    EDGE: Empty dictionary returns empty list\n    \"\"\"\n    if not dictionary:\n        return []\n    \n    candidates = []\n    word_len = len(word)\n    \n    for dict_word in dictionary:\n        # Pruning: skip if length difference > max_distance\n        if abs(len(dict_word) - word_len) > max_distance:\n            continue</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_a285f9bbf5de3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>find_candidates_part_2()</code>",
        "<pre><code>            \n        distance = edit_distance_dp(word, dict_word)\n        if distance <= max_distance:\n            candidates.append((dict_word, distance))\n    \n    return candidates</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_96b67eb0d024d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>rank_by_frequency_part_1()</code>",
        "<pre><code>def rank_by_frequency(candidates: List[Tuple[str, int]], \n                     dictionary: Dict[str, int]) -> List[str]:\n    \"\"\"\n    FORMULA: score = frequency / (distance + 1)\n    KEY: Higher frequency, lower distance = better\n    \"\"\"\n    scored = []\n    \n    for word, distance in candidates:\n        freq = dictionary.get(word, 1)\n        score = freq / (distance + 1)\n        scored.append((word, score))\n    \n    # Sort by score descending\n    scored.sort(key=lambda x: x[1], reverse=True)</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_518ce892a2640",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>rank_by_frequency_part_2()</code>",
        "<pre><code>    \n    return [word for word, _ in scored[:3]]</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_6fccafacd3507",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>spell_checker_part_1()</code>",
        "<pre><code>def spell_checker(word: str, dictionary: Dict[str, int]) -> List[str]:\n    \"\"\"\n    INTERVIEW: Mention trade-offs:\n    - Edit distance vs phonetic matching\n    - Memory vs speed (DP vs recursive)\n    - Pruning strategies for large dictionaries\n    \"\"\"\n    # EDGE: Already correct word\n    if word in dictionary:\n        return [word]\n    \n    # EDGE: Empty dictionary\n    if not dictionary:\n        return []\n    </code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_78fb6b8dc0f36",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>spell_checker_part_2()</code>",
        "<pre><code>    # Find candidates within edit distance 2\n    candidates = find_candidates(word, dictionary, max_distance=2)\n    \n    # EDGE: No candidates found\n    if not candidates:\n        return []\n    \n    # Rank by frequency and return top 3\n    return rank_by_frequency(candidates, dictionary)</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_4c2e32b7840cb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Implement: <code>example_usage()</code>",
        "<pre><code>def example_usage():\n    \"\"\"\n    EXAMPLE:\n    dictionary = {\"hello\": 100, \"help\": 80, \"hell\": 20}\n    spell_checker(\"helo\", dictionary) → [\"hello\", \"help\", \"hell\"]\n    \n    TEST EDGE CASES:\n    - Empty string\n    - Word already in dictionary  \n    - No close matches\n    - Single character words\n    \"\"\"\n    pass</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_0a6c5505793c1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Key concept:",
        "<p>from typing import Dict, List, Tuple\n\n# Card 1: Algorithm Overview\ndef spell_check_approach():</p>",
        "Example Anki Refactor",
        "concepts"
      ],
      "guid": "nlp_16246265cf09b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Key concept:",
        "<p>from typing import Dict, List, Tuple\n\n# Card 1: Algorithm Overview\ndef spell_check_approach():</p>",
        "Example Anki Refactor",
        "concepts"
      ],
      "guid": "nlp_16246265cf09b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: LLM Fine-tuning for Classification</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 25 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Fine Tuning",
        "problem_understanding"
      ],
      "guid": "nlp_e7942c1161068",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>add_classification_head()</code>",
        "<pre><code>def add_classification_head(pretrained_model_dim: int, num_classes: int) -> Dict:\n    \"\"\"Add classification head to pretrained LLM.\"\"\"\n    \n    # Xavier/Glorot initialization for stable training\n    std = np.sqrt(2.0 / (pretrained_model_dim + num_classes))\n    \n    return {\n        'W_cls': np.random.randn(pretrained_model_dim, num_classes) * std,\n        'b_cls': np.zeros(num_classes)\n    }</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_9ba2a280d7800",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>compute_classification_loss_part_1()</code>",
        "<pre><code>def compute_classification_loss(logits: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Compute cross-entropy loss with numerical stability.\"\"\"\n    batch_size, num_classes = logits.shape\n    \n    # Numerical stability: subtract max from logits\n    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n    \n    # Softmax probabilities\n    exp_logits = np.exp(logits_stable)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Cross-entropy loss\n    loss = 0.0\n    for i in range(batch_size):\n        true_class = labels[i]\n        loss += -np.log(probs[i, true_class] + 1e-10)  # Add small epsilon</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_56c7c80b18343",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>compute_classification_loss_part_2()</code>",
        "<pre><code>    \n    return loss / batch_size</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_802452b8a0659",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>freeze_layers_part_1()</code>",
        "<pre><code>def freeze_layers(model_weights: Dict, freeze_ratio: float = 0.8) -> Dict:\n    \"\"\"Mark layers as frozen (simulate requires_grad=False).\"\"\"\n    frozen_info = {}\n    \n    # Sort layers by name to freeze bottom layers\n    layer_names = sorted([name for name in model_weights.keys() if 'layer_' in name])\n    \n    num_layers = len(layer_names)\n    num_frozen = int(num_layers * freeze_ratio)\n    \n    for i, layer_name in enumerate(layer_names):\n        frozen_info[layer_name] = i < num_frozen  # True if frozen\n    \n    # Never freeze classification head\n    for name in model_weights.keys():\n        if 'cls' in name:\n            frozen_info[name] = False</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_d98ab3134a414",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>freeze_layers_part_2()</code>",
        "<pre><code>    \n    return frozen_info</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_1cf86d9c57198",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>fine_tuning_step_part_1()</code>",
        "<pre><code>def fine_tuning_step(x: np.ndarray, labels: np.ndarray, \n                    pretrained_weights: Dict, cls_head: Dict,\n                    learning_rates: Dict) -> Tuple[float, Dict]:\n    \"\"\"Single fine-tuning step (forward + backward).\"\"\"\n    \n    # Forward pass through pretrained model (simplified)\n    # In practice, this would be the full LLM forward pass\n    pretrained_output = x @ pretrained_weights['final_layer'] + pretrained_weights['final_bias']\n    \n    # Classification head forward pass\n    logits = pretrained_output @ cls_head['W_cls'] + cls_head['b_cls']\n    \n    # Compute loss\n    loss = compute_classification_loss(logits, labels)\n    </code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_dc40b630b572e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>fine_tuning_step_part_2()</code>",
        "<pre><code>    # Backward pass (simplified gradients)\n    batch_size = x.shape[0]\n    \n    # Softmax for gradient calculation\n    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits_stable)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Gradient w.r.t logits\n    grad_logits = probs.copy()\n    for i in range(batch_size):\n        grad_logits[i, labels[i]] -= 1\n    grad_logits /= batch_size\n    \n    # Gradients for classification head\n    grad_W_cls = pretrained_output.T @ grad_logits\n    grad_b_cls = np.sum(grad_logits, axis=0)</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_ebbad617c55ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>fine_tuning_step_part_3()</code>",
        "<pre><code>    \n    # Update classification head with higher learning rate\n    cls_head['W_cls'] -= learning_rates['cls_head'] * grad_W_cls\n    cls_head['b_cls'] -= learning_rates['cls_head'] * grad_b_cls\n    \n    # Update pretrained layers with lower learning rate (if not frozen)\n    grad_pretrained = grad_logits @ cls_head['W_cls'].T\n    \n    if not pretrained_weights.get('frozen', True):\n        pretrained_weights['final_layer'] -= learning_rates['pretrained'] * (x.T @ grad_pretrained)\n        pretrained_weights['final_bias'] -= learning_rates['pretrained'] * np.sum(grad_pretrained, axis=0)\n    \n    return loss, {'grad_W_cls': grad_W_cls, 'grad_b_cls': grad_b_cls}</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_5321a8ac82ea7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>compute_accuracy()</code>",
        "<pre><code>def compute_accuracy(logits: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Compute classification accuracy.\"\"\"\n    predictions = np.argmax(logits, axis=1)\n    return np.mean(predictions == labels)</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_c7974aec83f42",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>lora_approximation()</code>",
        "<pre><code>def lora_approximation(weight_matrix: np.ndarray, rank: int = 4) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Simulate LoRA (Low-Rank Adaptation) decomposition.\n    \n    Instead of updating full weight matrix W, update W + A*B where:\n    - A is (d, rank) and B is (rank, d) \n    - Only A and B are trainable (much fewer parameters)\n    \"\"\"\n    d_in, d_out = weight_matrix.shape\n    \n    # Initialize LoRA matrices\n    A = np.random.randn(d_in, rank) * 0.01\n    B = np.zeros((rank, d_out))  # B initialized to zero\n    \n    return A, B</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_70c04a99f54ab",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>test_fine_tuning_part_1()</code>",
        "<pre><code>def test_fine_tuning():\n    \"\"\"Demonstrate fine-tuning workflow.\"\"\"\n    \n    # Sample data\n    batch_size, seq_len, d_model = 8, 10, 64\n    num_classes = 3\n    \n    # Simulated pretrained model output\n    x = np.random.randn(batch_size, d_model)  # [CLS] token representations\n    labels = np.random.randint(0, num_classes, batch_size)\n    \n    print(\"Fine-tuning Simulation\")\n    print(\"=\" * 30)\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Model dimension: {d_model}\")\n    print(f\"Number of classes: {num_classes}\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_4e234ea009aae",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>test_fine_tuning_part_2()</code>",
        "<pre><code>    \n    # Add classification head\n    cls_head = add_classification_head(d_model, num_classes)\n    print(f\"\\nClassification head shape: {cls_head['W_cls'].shape}\")\n    \n    # Create pretrained weights\n    pretrained_weights = {\n        'final_layer': np.random.randn(d_model, d_model) * 0.02,\n        'final_bias': np.zeros(d_model),\n        'frozen': False  # Will be set by freeze_layers\n    }\n    \n    # Demonstrate freezing\n    freeze_info = freeze_layers({'layer_0': None, 'layer_1': None, 'layer_2': None}, freeze_ratio=0.67)\n    print(f\"\\nLayer freezing (freeze_ratio=0.67): {freeze_info}\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_1895dac323606",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>test_fine_tuning_part_3()</code>",
        "<pre><code>    \n    # Different learning rates\n    learning_rates = {\n        'pretrained': 1e-5,  # Lower LR for pretrained layers\n        'cls_head': 1e-3     # Higher LR for new classification head\n    }\n    \n    print(f\"\\nLearning rates: {learning_rates}\")\n    \n    # Training loop simulation\n    print(f\"\\nTraining simulation:\")\n    for epoch in range(3):\n        loss, grads = fine_tuning_step(x, labels, pretrained_weights, cls_head, learning_rates)\n        \n        # Calculate accuracy\n        logits = x @ cls_head['W_cls'] + cls_head['b_cls']\n        accuracy = compute_accuracy(logits, labels)</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_67bff34016766",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>test_fine_tuning_part_4()</code>",
        "<pre><code>        \n        print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}, Accuracy = {accuracy:.3f}\")\n    \n    # Demonstrate LoRA\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"LoRA Parameter-Efficient Fine-tuning\")\n    \n    # Original weight matrix\n    W_original = np.random.randn(512, 512)\n    A, B = lora_approximation(W_original, rank=8)\n    \n    original_params = W_original.size\n    lora_params = A.size + B.size\n    reduction = (1 - lora_params / original_params) * 100\n    \n    print(f\"Original parameters: {original_params:,}\")\n    print(f\"LoRA parameters: {lora_params:,}\")\n    print(f\"Parameter reduction: {reduction:.1f}%\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_6bd270c197a61",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Implement: <code>test_fine_tuning_part_5()</code>",
        "<pre><code>    \n    # Show how LoRA update works\n    lora_update = A @ B\n    updated_weight = W_original + lora_update\n    print(f\"LoRA update shape: {lora_update.shape}\")\n    print(\"✓ LoRA allows efficient adaptation with few parameters\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_4be19b2466a90",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: GPT Transformer Block</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 30 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Gpt Implementation",
        "problem_understanding"
      ],
      "guid": "nlp_cb668bbf95470",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>gelu()</code>",
        "<pre><code>def gelu(x: np.ndarray) -> np.ndarray:\n    \"\"\"GELU activation function used in GPT.\"\"\"\n    return 0.5 * x * (1 + np.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_db6da96944671",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>layer_norm()</code>",
        "<pre><code>def layer_norm(x: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n    \"\"\"Apply layer normalization.\"\"\"\n    # Calculate mean and variance along last dimension\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    \n    # Normalize\n    normalized = (x - mean) / np.sqrt(variance + eps)\n    \n    # Scale and shift\n    return gamma * normalized + beta</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_8f7e5a8a72c2a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>causal_self_attention_part_1()</code>",
        "<pre><code>def causal_self_attention(x: np.ndarray, W_qkv: np.ndarray, W_out: np.ndarray) -> np.ndarray:\n    \"\"\"Causal self-attention for GPT.\"\"\"\n    seq_len, d_model = x.shape\n    \n    # Project to Q, K, V\n    qkv = x @ W_qkv  # (seq_len, 3 * d_model)\n    q, k, v = np.split(qkv, 3, axis=-1)\n    \n    # Attention scores\n    scores = q @ k.T / math.sqrt(d_model)\n    \n    # Apply causal mask (can't attend to future tokens)\n    mask = np.triu(np.ones((seq_len, seq_len)), k=1) * -1e9\n    scores = scores + mask\n    </code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_473d2e7eb51b9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>causal_self_attention_part_2()</code>",
        "<pre><code>    # Softmax\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Apply attention to values\n    attended = attention_weights @ v\n    \n    # Output projection\n    return attended @ W_out</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_1bdbd6b173186",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>feed_forward()</code>",
        "<pre><code>def feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray, \n                W2: np.ndarray, b2: np.ndarray) -> np.ndarray:\n    \"\"\"Feed-forward network with GELU activation.\"\"\"\n    # First layer\n    hidden = gelu(x @ W1 + b1)\n    \n    # Second layer  \n    output = hidden @ W2 + b2\n    \n    return output</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_4e8ea17d5d388",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>gpt_block()</code>",
        "<pre><code>def gpt_block(x: np.ndarray, weights: Dict) -> np.ndarray:\n    \"\"\"Single GPT transformer block.\"\"\"\n    seq_len, d_model = x.shape\n    \n    # 1. Layer norm + self-attention + residual\n    norm1 = layer_norm(x, weights['ln1_gamma'], weights['ln1_beta'])\n    attn_out = causal_self_attention(norm1, weights['W_qkv'], weights['W_out'])\n    x = x + attn_out  # Residual connection\n    \n    # 2. Layer norm + feed-forward + residual  \n    norm2 = layer_norm(x, weights['ln2_gamma'], weights['ln2_beta'])\n    ffn_out = feed_forward(norm2, weights['W1'], weights['b1'], weights['W2'], weights['b2'])\n    x = x + ffn_out  # Residual connection\n    \n    return x</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_fead48bb6497b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>create_gpt_weights_part_1()</code>",
        "<pre><code>def create_gpt_weights(d_model: int = 64, d_ff: int = 256) -> Dict:\n    \"\"\"Create sample weights for GPT block.\"\"\"\n    np.random.seed(42)\n    \n    return {\n        # Layer norm parameters\n        'ln1_gamma': np.ones(d_model),\n        'ln1_beta': np.zeros(d_model),\n        'ln2_gamma': np.ones(d_model), \n        'ln2_beta': np.zeros(d_model),\n        \n        # Attention weights\n        'W_qkv': np.random.randn(d_model, 3 * d_model) * 0.02,\n        'W_out': np.random.randn(d_model, d_model) * 0.02,\n        </code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_bb00c15de8893",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>create_gpt_weights_part_2()</code>",
        "<pre><code>        # Feed-forward weights  \n        'W1': np.random.randn(d_model, d_ff) * 0.02,\n        'b1': np.zeros(d_ff),\n        'W2': np.random.randn(d_ff, d_model) * 0.02,\n        'b2': np.zeros(d_model)\n    }</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_c8c9a275cfab4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>positional_encoding()</code>",
        "<pre><code>def positional_encoding(seq_len: int, d_model: int) -> np.ndarray:\n    \"\"\"Create sinusoidal positional encodings.\"\"\"\n    pos_enc = np.zeros((seq_len, d_model))\n    \n    for pos in range(seq_len):\n        for i in range(0, d_model, 2):\n            # Sine for even indices\n            pos_enc[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n            \n            # Cosine for odd indices\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n    \n    return pos_enc</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_61fc0a1558cde",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>simple_gpt_forward_part_1()</code>",
        "<pre><code>def simple_gpt_forward(token_ids: List[int], vocab_size: int = 1000, \n                      d_model: int = 64, num_layers: int = 2) -> np.ndarray:\n    \"\"\"Forward pass through a simple GPT model.\"\"\"\n    seq_len = len(token_ids)\n    \n    # Token embeddings (random for demo)\n    np.random.seed(42)\n    embedding_matrix = np.random.randn(vocab_size, d_model) * 0.02\n    \n    # Get embeddings for input tokens\n    x = np.array([embedding_matrix[token_id] for token_id in token_ids])\n    \n    # Add positional encodings\n    pos_enc = positional_encoding(seq_len, d_model)\n    x = x + pos_enc</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_c47ceffd359ff",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>Implement: <code>simple_gpt_forward_part_2()</code>",
        "<pre><code>    \n    # Pass through transformer blocks\n    for layer in range(num_layers):\n        weights = create_gpt_weights(d_model)\n        x = gpt_block(x, weights)\n    \n    return x</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_aa890a3f34640",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Instruction Following Setup</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 20 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Instruction Tuning",
        "problem_understanding"
      ],
      "guid": "nlp_75d5cbdd575f2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>format_instruction_data()</code>",
        "<pre><code>def format_instruction_data(instruction: str, response: str) -> str:\n    \"\"\"Format instruction-response pair for training.\"\"\"\n    return f\"### Instruction:\\n{instruction}\\n### Response:\\n{response}\"</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_0aae974030e49",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>tokenize_instruction_response_part_1()</code>",
        "<pre><code>def tokenize_instruction_response(formatted_text: str, \n                                tokenizer_vocab: Dict[str, int]) -> Tuple[List[int], int]:\n    \"\"\"\n    Tokenize formatted instruction-response and return instruction length.\n    \n    Returns:\n        (token_ids, instruction_length)\n    \"\"\"\n    # Simple tokenization for demo\n    tokens = formatted_text.lower().split()\n    token_ids = [tokenizer_vocab.get(token, 0) for token in tokens]  # 0 = UNK\n    \n    # Find where instruction ends (look for \"### Response:\")\n    instruction_length = 0\n    response_marker = \"### response:\"</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_a7ac2ba485e77",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>tokenize_instruction_response_part_2()</code>",
        "<pre><code>    \n    # Reconstruct text to find marker\n    text_lower = formatted_text.lower()\n    if response_marker in text_lower:\n        before_response = text_lower[:text_lower.index(response_marker)]\n        instruction_tokens = before_response.split()\n        instruction_length = len(instruction_tokens) + 2  # +2 for \"### response:\"\n    \n    return token_ids, instruction_length</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_5c8e9cad8c401",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>compute_instruction_loss_part_1()</code>",
        "<pre><code>def compute_instruction_loss(model_output: np.ndarray, \n                           target_tokens: List[int],\n                           instruction_length: int) -> float:\n    \"\"\"Compute loss only on response tokens.\"\"\"\n    seq_len, vocab_size = model_output.shape\n    \n    if instruction_length >= seq_len:\n        return 0.0  # No response tokens to train on\n    \n    # Only compute loss on response portion\n    response_logits = model_output[instruction_length:]\n    response_targets = target_tokens[instruction_length:]\n    \n    if len(response_targets) == 0:\n        return 0.0</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_e80cca09c5694",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>compute_instruction_loss_part_2()</code>",
        "<pre><code>    \n    # Cross-entropy loss on response tokens only\n    loss = 0.0\n    num_response_tokens = len(response_targets)\n    \n    for i, target_token in enumerate(response_targets):\n        if i < len(response_logits):\n            # Softmax\n            logits = response_logits[i]\n            max_logit = np.max(logits)\n            exp_logits = np.exp(logits - max_logit)\n            probs = exp_logits / np.sum(exp_logits)\n            \n            # Cross-entropy for this token\n            loss += -np.log(probs[target_token] + 1e-10)</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_38617dee34102",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>compute_instruction_loss_part_3()</code>",
        "<pre><code>    \n    return loss / num_response_tokens if num_response_tokens > 0 else 0.0</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_a32c2070c3b0a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>create_instruction_dataset_part_1()</code>",
        "<pre><code>def create_instruction_dataset(examples: List[Tuple[str, str]]) -> List[Dict]:\n    \"\"\"Create instruction tuning dataset.\"\"\"\n    dataset = []\n    \n    for instruction, response in examples:\n        formatted = format_instruction_data(instruction, response)\n        \n        # Simple tokenizer for demo\n        vocab = {word: i for i, word in enumerate(set(formatted.lower().split()))}\n        vocab['<UNK>'] = 0\n        \n        token_ids, inst_len = tokenize_instruction_response(formatted, vocab)\n        \n        dataset.append({\n            'formatted_text': formatted,\n            'token_ids': token_ids,\n            'instruction_length': inst_len,\n            'vocab': vocab\n        })</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_4e604f8f99f9a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>create_instruction_dataset_part_2()</code>",
        "<pre><code>    \n    return dataset</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_7042648e6682c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>evaluate_instruction_following_part_1()</code>",
        "<pre><code>def evaluate_instruction_following(model_responses: List[str], \n                                 instructions: List[str]) -> Dict[str, float]:\n    \"\"\"Simple evaluation metrics for instruction following.\"\"\"\n    \n    # Instruction following metrics (simplified)\n    scores = {\n        'avg_response_length': np.mean([len(response.split()) for response in model_responses]),\n        'response_rate': sum(1 for response in model_responses if len(response.strip()) > 0) / len(model_responses),\n        'keyword_compliance': 0.0\n    }\n    \n    # Check if response contains key instruction words\n    keyword_matches = 0\n    total_keywords = 0\n    \n    for instruction, response in zip(instructions, model_responses):</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_34e2d406e2944",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>evaluate_instruction_following_part_2()</code>",
        "<pre><code>        # Extract important words from instruction (simplified)\n        inst_words = set(instruction.lower().split())\n        resp_words = set(response.lower().split())\n        \n        # Remove common words\n        important_words = inst_words - {'the', 'a', 'an', 'is', 'are', 'and', 'or', 'but'}\n        \n        if important_words:\n            overlap = len(important_words & resp_words) / len(important_words)\n            keyword_matches += overlap\n            total_keywords += 1\n    \n    if total_keywords > 0:\n        scores['keyword_compliance'] = keyword_matches / total_keywords\n    \n    return scores</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_a35fca6f69029",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>sampling_strategies_part_1()</code>",
        "<pre><code>def sampling_strategies(logits: np.ndarray, temperature: float = 1.0, \n                       top_k: int = None, top_p: float = None) -> int:\n    \"\"\"Implement different sampling strategies for text generation.\"\"\"\n    \n    # Apply temperature scaling\n    if temperature != 1.0:\n        logits = logits / temperature\n    \n    # Convert to probabilities\n    max_logit = np.max(logits)\n    exp_logits = np.exp(logits - max_logit)\n    probs = exp_logits / np.sum(exp_logits)\n    \n    # Top-k sampling\n    if top_k is not None:\n        top_k_indices = np.argsort(probs)[-top_k:]\n        masked_probs = np.zeros_like(probs)\n        masked_probs[top_k_indices] = probs[top_k_indices]\n        probs = masked_probs / np.sum(masked_probs)</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_66ee42416995a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>sampling_strategies_part_2()</code>",
        "<pre><code>    \n    # Top-p (nucleus) sampling  \n    if top_p is not None:\n        sorted_indices = np.argsort(probs)[::-1]\n        cumsum_probs = np.cumsum(probs[sorted_indices])\n        \n        # Find cutoff where cumulative probability exceeds top_p\n        cutoff_idx = np.argmax(cumsum_probs >= top_p) + 1\n        \n        # Keep only top-p tokens\n        top_p_indices = sorted_indices[:cutoff_idx]\n        masked_probs = np.zeros_like(probs)\n        masked_probs[top_p_indices] = probs[top_p_indices]\n        probs = masked_probs / np.sum(masked_probs)\n    </code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_b1257b0af1130",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Implement: <code>sampling_strategies_part_3()</code>",
        "<pre><code>    # Sample from distribution\n    return np.random.choice(len(probs), p=probs)</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_3e58d383cad55",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>🔥 Edge case: Empty Collection",
        "<pre><code>response_targets return 0.0</code></pre>",
        "Instruction Tuning",
        "edge_cases"
      ],
      "guid": "nlp_8e82fbeb2e9d2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Text Generation with LLMs</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 25 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Llm Fundamentals",
        "problem_understanding"
      ],
      "guid": "nlp_5e4e87986873f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>softmax()</code>",
        "<pre><code>def softmax(logits: List[float], temperature: float = 1.0) -> List[float]:\n    \"\"\"Convert logits to probabilities with temperature scaling.\"\"\"\n    if temperature != 1.0:\n        logits = [l / temperature for l in logits]\n    \n    max_logit = max(logits)\n    exp_logits = [math.exp(l - max_logit) for l in logits]\n    sum_exp = sum(exp_logits)\n    \n    return [exp_l / sum_exp for exp_l in exp_logits]</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_d1d3723572fcb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>sample_token_part_1()</code>",
        "<pre><code>def sample_token(probs: List[float], strategy: str = 'greedy', \n                top_k: int = 5, top_p: float = 0.9) -> int:\n    \"\"\"Sample next token using different strategies.\"\"\"\n    \n    if strategy == 'greedy':\n        return probs.index(max(probs))\n    \n    elif strategy == 'random':\n        # Random sampling from full distribution\n        return np.random.choice(len(probs), p=probs)\n    \n    elif strategy == 'top_k':\n        # Sample from top k tokens only\n        top_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:top_k]\n        top_probs = [probs[i] for i in top_indices]</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_9115e818dde96",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>sample_token_part_2()</code>",
        "<pre><code>        \n        # Renormalize\n        sum_top = sum(top_probs)\n        if sum_top > 0:\n            top_probs = [p / sum_top for p in top_probs]\n            selected_idx = np.random.choice(len(top_probs), p=top_probs)\n            return top_indices[selected_idx]\n        else:\n            return 0\n    \n    elif strategy == 'top_p':\n        # Nucleus sampling\n        sorted_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)\n        \n        cumsum = 0.0\n        nucleus_indices = []</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_766dea4d31ca5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>sample_token_part_3()</code>",
        "<pre><code>        \n        for idx in sorted_indices:\n            cumsum += probs[idx]\n            nucleus_indices.append(idx)\n            if cumsum >= top_p:\n                break\n        \n        # Sample from nucleus\n        nucleus_probs = [probs[i] for i in nucleus_indices]\n        sum_nucleus = sum(nucleus_probs)\n        \n        if sum_nucleus > 0:\n            nucleus_probs = [p / sum_nucleus for p in nucleus_probs]\n            selected_idx = np.random.choice(len(nucleus_probs), p=nucleus_probs)\n            return nucleus_indices[selected_idx]\n        else:\n            return 0</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_0f5867672ead5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>sample_token_part_4()</code>",
        "<pre><code>    \n    return 0</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_74c4b0fe3fe65",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>generate_text_part_1()</code>",
        "<pre><code>def generate_text(model_fn: Callable, prompt: str, vocab: Dict, \n                 max_length: int = 20, strategy: str = 'greedy',\n                 temperature: float = 1.0, **kwargs) -> str:\n    \"\"\"Generate text using autoregressive language model.\"\"\"\n    \n    # Convert prompt to tokens\n    prompt_tokens = [vocab.get(word.lower(), vocab.get('<UNK>', 0)) \n                    for word in prompt.split()]\n    \n    if not prompt_tokens:\n        prompt_tokens = [vocab.get('<START>', 0)]\n    \n    # Generate tokens iteratively\n    current_tokens = prompt_tokens.copy()\n    generated_words = prompt.split()</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_305b800baf23f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>generate_text_part_2()</code>",
        "<pre><code>    \n    # Reverse vocabulary for decoding\n    id_to_word = {idx: word for word, idx in vocab.items()}\n    \n    for _ in range(max_length):\n        # Get next token logits from model\n        logits = model_fn(current_tokens)\n        \n        # Convert to probabilities\n        probs = softmax(logits, temperature)\n        \n        # Sample next token\n        next_token_id = sample_token(probs, strategy, \n                                   top_k=kwargs.get('top_k', 5),\n                                   top_p=kwargs.get('top_p', 0.9))</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_178f81a1aa869",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>generate_text_part_3()</code>",
        "<pre><code>        \n        # Convert to word\n        next_word = id_to_word.get(next_token_id, '<UNK>')\n        \n        # Stop if end token\n        if next_word in ['<END>', '<EOS>', '</s>']:\n            break\n        \n        # Add to sequence\n        current_tokens.append(next_token_id)\n        generated_words.append(next_word)\n    \n    return ' '.join(generated_words)</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_70b84955bfb56",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>beam_search_part_1()</code>",
        "<pre><code>def beam_search(model_fn: Callable, prompt_tokens: List[int], \n               beam_width: int = 3, max_length: int = 10,\n               vocab: Dict = None) -> List[Tuple[List[int], float]]:\n    \"\"\"Implement beam search for finding high-probability sequences.\"\"\"\n    \n    # Initialize beam with prompt\n    # Each beam entry: (sequence, log_probability)\n    beams = [(prompt_tokens.copy(), 0.0)]\n    \n    for step in range(max_length):\n        new_beams = []\n        \n        for sequence, log_prob in beams:\n            # Get next token logits\n            logits = model_fn(sequence)\n            probs = softmax(logits)</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_2cefcac277e72",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>beam_search_part_2()</code>",
        "<pre><code>            \n            # Consider top beam_width tokens\n            top_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:beam_width]\n            \n            for token_id in top_indices:\n                new_sequence = sequence + [token_id]\n                new_log_prob = log_prob + math.log(probs[token_id] + 1e-10)\n                \n                new_beams.append((new_sequence, new_log_prob))\n        \n        # Keep only top beam_width beams\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:beam_width]\n    \n    return beams</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_f426b8f79df04",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Implement: <code>mock_language_model()</code>",
        "<pre><code>def mock_language_model(tokens: List[int]) -> List[float]:\n    \"\"\"Mock language model that returns random logits.\"\"\"\n    vocab_size = 50\n    np.random.seed(sum(tokens) % 100)  # Deterministic based on input\n    return np.random.randn(vocab_size).tolist()</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_58e8352ac6be5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: LLM Evaluation Metrics</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 20 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Model Evaluation",
        "problem_understanding"
      ],
      "guid": "nlp_f9aad4067a18a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>calculate_perplexity_part_1()</code>",
        "<pre><code>def calculate_perplexity(model_probs: List[List[float]], \n                        target_tokens: List[int]) -> float:\n    \"\"\"Calculate perplexity from model probabilities.\"\"\"\n    if not model_probs or not target_tokens:\n        return float('inf')\n    \n    if len(model_probs) != len(target_tokens):\n        return float('inf')\n    \n    log_likelihood = 0.0\n    num_tokens = 0\n    \n    for probs, target_token in zip(model_probs, target_tokens):\n        if target_token < len(probs):\n            # Get probability of target token\n            prob = probs[target_token]</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_3c053fed26925",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>calculate_perplexity_part_2()</code>",
        "<pre><code>            \n            # Add log probability (with small epsilon to avoid log(0))\n            log_likelihood += math.log(max(prob, 1e-10))\n            num_tokens += 1\n    \n    if num_tokens == 0:\n        return float('inf')\n    \n    # Perplexity = exp(-avg_log_likelihood)\n    avg_log_likelihood = log_likelihood / num_tokens\n    perplexity = math.exp(-avg_log_likelihood)\n    \n    return perplexity</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_b5bc1f588ea1c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>get_ngrams()</code>",
        "<pre><code>def get_ngrams(text: str, n: int) -> List[str]:\n    \"\"\"Extract n-grams from text.\"\"\"\n    words = text.lower().split()\n    ngrams = []\n    \n    for i in range(len(words) - n + 1):\n        ngram = ' '.join(words[i:i + n])\n        ngrams.append(ngram)\n    \n    return ngrams</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_d4308df33d5ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>compute_bleu_score_part_1()</code>",
        "<pre><code>def compute_bleu_score(reference: str, candidate: str, n: int = 4) -> float:\n    \"\"\"Compute BLEU score for text generation evaluation.\"\"\"\n    \n    ref_words = reference.lower().split()\n    cand_words = candidate.lower().split()\n    \n    if not cand_words:\n        return 0.0\n    \n    # Brevity penalty\n    ref_len = len(ref_words)\n    cand_len = len(cand_words)\n    \n    if cand_len > ref_len:\n        bp = 1.0\n    else:\n        bp = math.exp(1 - ref_len / cand_len) if cand_len > 0 else 0.0</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_caf493d46a5d5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>compute_bleu_score_part_2()</code>",
        "<pre><code>    \n    # Calculate n-gram precisions\n    precisions = []\n    \n    for i in range(1, n + 1):\n        ref_ngrams = Counter(get_ngrams(reference, i))\n        cand_ngrams = Counter(get_ngrams(candidate, i))\n        \n        if not cand_ngrams:\n            precisions.append(0.0)\n            continue\n        \n        # Count matches (with clipping for multiple references)\n        matches = 0\n        for ngram, count in cand_ngrams.items():\n            matches += min(count, ref_ngrams.get(ngram, 0))</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_8865ac27bdfa1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>compute_bleu_score_part_3()</code>",
        "<pre><code>        \n        precision = matches / sum(cand_ngrams.values())\n        precisions.append(precision)\n    \n    # Geometric mean of precisions\n    if all(p > 0 for p in precisions):\n        geometric_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n    else:\n        geometric_mean = 0.0\n    \n    return bp * geometric_mean</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_d46f4a4a905a7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>calculate_cross_entropy_loss_part_1()</code>",
        "<pre><code>def calculate_cross_entropy_loss(logits: List[List[float]], \n                               targets: List[int]) -> float:\n    \"\"\"Calculate cross-entropy loss from logits.\"\"\"\n    if not logits or not targets or len(logits) != len(targets):\n        return float('inf')\n    \n    total_loss = 0.0\n    \n    for logit_vec, target in zip(logits, targets):\n        if target < len(logit_vec):\n            # Softmax with numerical stability\n            max_logit = max(logit_vec)\n            exp_logits = [math.exp(logit - max_logit) for logit in logit_vec]\n            sum_exp = sum(exp_logits)\n            </code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_34e6f0c314a3f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>calculate_cross_entropy_loss_part_2()</code>",
        "<pre><code>            # Probability of target token\n            prob = exp_logits[target] / sum_exp\n            \n            # Cross-entropy\n            total_loss += -math.log(max(prob, 1e-10))\n    \n    return total_loss / len(targets)</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_b30a2996c56c6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>evaluate_generation_quality_part_1()</code>",
        "<pre><code>def evaluate_generation_quality(references: List[str], \n                               candidates: List[str]) -> Dict[str, float]:\n    \"\"\"Comprehensive evaluation of generated text quality.\"\"\"\n    \n    if len(references) != len(candidates):\n        raise ValueError(\"Number of references and candidates must match\")\n    \n    # Calculate average BLEU scores\n    bleu_scores = []\n    for ref, cand in zip(references, candidates):\n        bleu = compute_bleu_score(ref, cand)\n        bleu_scores.append(bleu)\n    \n    # Calculate other metrics\n    length_ratios = []\n    for ref, cand in zip(references, candidates):\n        ref_len = len(ref.split())\n        cand_len = len(cand.split())\n        ratio = cand_len / ref_len if ref_len > 0 else 0\n        length_ratios.append(ratio)</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_f48f59fece117",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Implement: <code>evaluate_generation_quality_part_2()</code>",
        "<pre><code>    \n    return {\n        'avg_bleu': sum(bleu_scores) / len(bleu_scores),\n        'avg_length_ratio': sum(length_ratios) / len(length_ratios),\n        'min_bleu': min(bleu_scores),\n        'max_bleu': max(bleu_scores)\n    }</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_0e764c6394c89",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Named Entity Recognition with Custom Entities</b><br>What's the key approach?",
        "<b>Approach:</b> Implement `extract_entities(text: str) -> Dict[str, List[str]]` that:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Ner",
        "problem_understanding"
      ],
      "guid": "nlp_bb39fd3aa51bc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_entities()</code>",
        "<pre><code>def extract_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract named entities using spaCy.\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        entities[ent.label_].append(ent.text)\n    \n    # Remove duplicates while preserving order\n    for label in entities:\n        entities[label] = list(dict.fromkeys(entities[label]))\n    \n    return dict(entities)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_6dc6887dac214",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_custom_entities_part_1()</code>",
        "<pre><code>def extract_custom_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract custom entities like emails, phones, URLs using regex.\"\"\"\n    entities = defaultdict(list)\n    \n    # Email pattern\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    entities['EMAIL'] = re.findall(email_pattern, text)\n    \n    # Phone pattern (US-style)\n    phone_pattern = r'\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b'\n    phones = re.findall(phone_pattern, text)\n    entities['PHONE'] = ['-'.join(groups) for groups in phones]\n    \n    # URL pattern\n    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&/=]*)'\n    entities['URL'] = re.findall(url_pattern, text)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_5d6013f7747cb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_custom_entities_part_2()</code>",
        "<pre><code>    \n    # Money pattern (simple)\n    money_pattern = r'\\$[\\d,]+\\.?\\d*|\\b\\d+\\s*(?:dollars?|cents?|USD|EUR|GBP)\\b'\n    entities['MONEY_CUSTOM'] = re.findall(money_pattern, text, re.IGNORECASE)\n    \n    # Remove empty categories\n    return {k: v for k, v in entities.items() if v}</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_17aea0458adab",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_entities_with_context_part_1()</code>",
        "<pre><code>def extract_entities_with_context(text: str, context_window: int = 30) -> Dict[str, List[Tuple[str, str]]]:\n    \"\"\"Extract entities with surrounding context.\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        # Get context\n        start = max(0, ent.start_char - context_window)\n        end = min(len(text), ent.end_char + context_window)\n        context = text[start:end].strip()\n        \n        # Mark entity in context\n        entity_start = ent.start_char - start\n        entity_end = ent.end_char - start\n        context_marked = (\n            context[:entity_start] + \n            f\"[{context[entity_start:entity_end]}]\" + \n            context[entity_end:]\n        )</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_58089508f7002",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_entities_with_context_part_2()</code>",
        "<pre><code>        \n        entities[ent.label_].append((ent.text, context_marked))\n    \n    return dict(entities)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_b1451d2bcf396",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_entity_relationships_part_1()</code>",
        "<pre><code>def extract_entity_relationships(text: str) -> List[Tuple[str, str, str]]:\n    \"\"\"Extract simple relationships between entities.\"\"\"\n    doc = nlp(text)\n    relationships = []\n    \n    # Simple pattern: PERSON + verb + ORG\n    for sent in doc.sents:\n        entities_in_sent = [(ent.text, ent.label_, ent.start, ent.end) for ent in sent.ents]\n        \n        # Look for patterns\n        for i, (ent1_text, ent1_label, _, _) in enumerate(entities_in_sent):\n            for j, (ent2_text, ent2_label, _, _) in enumerate(entities_in_sent[i+1:], i+1):\n                # Extract verb between entities\n                if ent1_label == \"PERSON\" and ent2_label == \"ORG\":\n                    # Find verb between entities\n                    between_tokens = sent[entities_in_sent[i][3]:entities_in_sent[j][2]]\n                    verbs = [token.text for token in between_tokens if token.pos_ == \"VERB\"]\n                    if verbs:\n                        relationships.append((ent1_text, verbs[0], ent2_text))</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_555040afd71f5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_entity_relationships_part_2()</code>",
        "<pre><code>    \n    return relationships</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_46597fb0852af",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>resolve_entity_coreferences_part_1()</code>",
        "<pre><code>def resolve_entity_coreferences(text: str) -> Dict[str, List[str]]:\n    \"\"\"Simple coreference resolution for entities (e.g., 'Apple' -> 'Apple Inc.')\"\"\"\n    doc = nlp(text)\n    entities = extract_entities(text)\n    \n    # Simple heuristic: map shorter versions to longer versions\n    entity_mapping = {}\n    \n    for label, ent_list in entities.items():\n        # Sort by length\n        sorted_ents = sorted(ent_list, key=len, reverse=True)\n        for i, longer in enumerate(sorted_ents):\n            for shorter in sorted_ents[i+1:]:\n                if shorter.lower() in longer.lower() and shorter != longer:\n                    entity_mapping[shorter] = longer</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_77633ab4c8f45",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>resolve_entity_coreferences_part_2()</code>",
        "<pre><code>    \n    # Apply mapping\n    resolved = defaultdict(set)\n    for label, ent_list in entities.items():\n        for ent in ent_list:\n            canonical = entity_mapping.get(ent, ent)\n            resolved[label].add(canonical)\n    \n    return {k: list(v) for k, v in resolved.items()}</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_94e2977bc612f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_entities_with_confidence_part_1()</code>",
        "<pre><code>def extract_entities_with_confidence(text: str) -> Dict[str, List[Tuple[str, float]]]:\n    \"\"\"Extract entities with confidence scores (using spaCy's scores if available).\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        # SpaCy doesn't always provide confidence, so we'll simulate\n        # In practice, you'd use a model that provides confidence scores\n        confidence = 0.95 if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"] else 0.85\n        \n        # Lower confidence for single-word entities\n        if len(ent.text.split()) == 1:\n            confidence *= 0.9\n            \n        entities[ent.label_].append((ent.text, confidence))</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_9940d7710602e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>Implement: <code>extract_entities_with_confidence_part_2()</code>",
        "<pre><code>    \n    return dict(entities)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_d2e49325ae5dd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: N-gram Language Model</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 25 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Ngrams",
        "problem_understanding"
      ],
      "guid": "nlp_4872894e6e029",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>Implement: <code>build_bigram_model_part_1()</code>",
        "<pre><code>def build_bigram_model(texts: List[str]) -> Dict[str, Dict[str, float]]:\n    \"\"\"Build bigram language model with probabilities.\"\"\"\n    if not texts:\n        return {}\n    \n    # Count bigrams\n    bigram_counts = defaultdict(Counter)\n    \n    for text in texts:\n        words = text.lower().split()\n        \n        # Add start token\n        words = ['<START>'] + words + ['<END>']\n        \n        # Count bigrams\n        for i in range(len(words) - 1):\n            w1, w2 = words[i], words[i + 1]\n            bigram_counts[w1][w2] += 1</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_e91f5899f9717",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>Implement: <code>build_bigram_model_part_2()</code>",
        "<pre><code>    \n    # Convert counts to probabilities\n    model = {}\n    for w1, w2_counts in bigram_counts.items():\n        total = sum(w2_counts.values())\n        model[w1] = {w2: count/total for w2, count in w2_counts.items()}\n    \n    return model</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_065da0bc51d32",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>Implement: <code>generate_text_part_1()</code>",
        "<pre><code>def generate_text(model: Dict, start_word: str = '<START>', length: int = 5) -> str:\n    \"\"\"Generate text using bigram model (deterministic - pick most probable).\"\"\"\n    if start_word not in model:\n        return \"\"\n    \n    words = []\n    current_word = start_word\n    \n    for _ in range(length):\n        if current_word not in model or not model[current_word]:\n            break\n        \n        # Pick most probable next word\n        next_word = max(model[current_word], key=model[current_word].get)\n        \n        if next_word == '<END>':\n            break</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_0b998b3070e8b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>Implement: <code>generate_text_part_2()</code>",
        "<pre><code>        \n        if next_word != '<START>':\n            words.append(next_word)\n        \n        current_word = next_word\n    \n    return ' '.join(words)</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_391118d9ee959",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>Implement: <code>calculate_probability()</code>",
        "<pre><code>def calculate_probability(model: Dict, text: str) -> float:\n    \"\"\"Calculate probability of text under the model.\"\"\"\n    words = ['<START>'] + text.lower().split() + ['<END>']\n    \n    prob = 1.0\n    for i in range(len(words) - 1):\n        w1, w2 = words[i], words[i + 1]\n        \n        if w1 in model and w2 in model[w1]:\n            prob *= model[w1][w2]\n        else:\n            return 0.0  # Unseen bigram\n    \n    return prob</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_ef3937ff3d59e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Neural Network from Scratch</b><br>What's the key approach?",
        "<b>Approach:</b> Implement basic neural networks from scratch:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Neural Fundamentals",
        "problem_understanding"
      ],
      "guid": "nlp_5d13d6f582ae4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>gradient_check_part_1()</code>",
        "<pre><code>def gradient_check(network: NeuralNetwork, X: np.ndarray, y: np.ndarray, epsilon: float = 1e-7) -> float:\n    \"\"\"Perform gradient checking to verify backpropagation implementation.\"\"\"\n    # Get gradients from backpropagation\n    y_pred = network.forward(X)\n    network.backward(X, y, y_pred)\n    \n    # Store analytical gradients\n    analytical_gradients = {}\n    for i in range(1, network.num_layers):\n        # Note: This is simplified - in practice you'd store gradients during backward pass\n        pass\n    \n    # Compute numerical gradients\n    numerical_gradients = {}\n    \n    for i in range(1, network.num_layers):\n        w_key = f'W{i}'\n        b_key = f'b{i}'</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_ba4a7cf4553ab",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>gradient_check_part_2()</code>",
        "<pre><code>        \n        # Check weights\n        w_shape = network.weights[w_key].shape\n        numerical_gradients[w_key] = np.zeros(w_shape)\n        \n        for idx in np.ndindex(w_shape):\n            # Perturb weight\n            network.weights[w_key][idx] += epsilon\n            y_pred_plus = network.forward(X)\n            cost_plus = network.compute_cost(y, y_pred_plus)\n            \n            network.weights[w_key][idx] -= 2 * epsilon\n            y_pred_minus = network.forward(X)\n            cost_minus = network.compute_cost(y, y_pred_minus)\n            </code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_b1660aa6f965b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>gradient_check_part_3()</code>",
        "<pre><code>            # Restore weight\n            network.weights[w_key][idx] += epsilon\n            \n            # Compute numerical gradient\n            numerical_gradients[w_key][idx] = (cost_plus - cost_minus) / (2 * epsilon)\n    \n    print(\"Gradient checking completed (simplified version)\")\n    return 0.0  # Would return actual difference in real implementation</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_6eb73263528ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>generate_xor_data()</code>",
        "<pre><code>def generate_xor_data() -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate XOR problem data.\"\"\"\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n    y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n    return X, y</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_3af405ea23baf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>generate_spiral_data()</code>",
        "<pre><code>def generate_spiral_data(n_samples: int = 100, n_classes: int = 3) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate spiral classification data.\"\"\"\n    X = np.zeros((n_samples * n_classes, 2))\n    y = np.zeros((n_samples * n_classes, n_classes))\n    \n    for class_idx in range(n_classes):\n        ix = range(n_samples * class_idx, n_samples * (class_idx + 1))\n        r = np.linspace(0.0, 1, n_samples)  # radius\n        t = np.linspace(class_idx * 4, (class_idx + 1) * 4, n_samples) + np.random.randn(n_samples) * 0.2\n        \n        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        y[ix, class_idx] = 1\n    \n    return X, y</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_0f78f5b54f368",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>plot_decision_boundary_part_1()</code>",
        "<pre><code>def plot_decision_boundary(network: NeuralNetwork, X: np.ndarray, y: np.ndarray, title: str = \"Decision Boundary\"):\n    \"\"\"Plot decision boundary for 2D data.\"\"\"\n    try:\n        h = 0.01\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        \n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                           np.arange(y_min, y_max, h))\n        \n        mesh_points = np.c_[xx.ravel(), yy.ravel()]\n        Z = network.predict_proba(mesh_points)\n        \n        if Z.shape[1] > 1:\n            Z = np.argmax(Z, axis=1)\n        else:\n            Z = Z.ravel()</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_6b12b7292ab8a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>plot_decision_boundary_part_2()</code>",
        "<pre><code>        \n        Z = Z.reshape(xx.shape)\n        \n        plt.figure(figsize=(10, 8))\n        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n        \n        # Plot data points\n        if y.shape[1] > 1:\n            y_labels = np.argmax(y, axis=1)\n        else:\n            y_labels = y.ravel()\n        \n        scatter = plt.scatter(X[:, 0], X[:, 1], c=y_labels, cmap=plt.cm.RdYlBu, edgecolors='black')\n        plt.colorbar(scatter)\n        plt.title(title)\n        plt.xlabel('X1')\n        plt.ylabel('X2')\n        plt.show()\n    except ImportError:\n        print(\"Matplotlib not available for plotting\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_608f36ae84732",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>sigmoid()</code>",
        "<pre><code>    def sigmoid(x: np.ndarray) -> np.ndarray:\n        \"\"\"Sigmoid activation function.\"\"\"\n        # Prevent overflow\n        x = np.clip(x, -500, 500)\n        return 1 / (1 + np.exp(-x))</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_dc2a29624f405",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>sigmoid_derivative()</code>",
        "<pre><code>    def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n        \"\"\"Derivative of sigmoid function.\"\"\"\n        s = ActivationFunctions.sigmoid(x)\n        return s * (1 - s)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_881018728aa9d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>tanh()</code>",
        "<pre><code>    def tanh(x: np.ndarray) -> np.ndarray:\n        \"\"\"Hyperbolic tangent activation function.\"\"\"\n        return np.tanh(x)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_a3ae98b93b1a9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>tanh_derivative()</code>",
        "<pre><code>    def tanh_derivative(x: np.ndarray) -> np.ndarray:\n        \"\"\"Derivative of tanh function.\"\"\"\n        return 1 - np.tanh(x) ** 2</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_be6c3b31e4eaa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>relu()</code>",
        "<pre><code>    def relu(x: np.ndarray) -> np.ndarray:\n        \"\"\"ReLU activation function.\"\"\"\n        return np.maximum(0, x)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_d1205f3b0ae6e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>relu_derivative()</code>",
        "<pre><code>    def relu_derivative(x: np.ndarray) -> np.ndarray:\n        \"\"\"Derivative of ReLU function.\"\"\"\n        return np.where(x > 0, 1, 0)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_03ef087ca5139",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>leaky_relu()</code>",
        "<pre><code>    def leaky_relu(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n        \"\"\"Leaky ReLU activation function.\"\"\"\n        return np.where(x > 0, x, alpha * x)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_afb4408274a8b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>leaky_relu_derivative()</code>",
        "<pre><code>    def leaky_relu_derivative(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n        \"\"\"Derivative of Leaky ReLU function.\"\"\"\n        return np.where(x > 0, 1, alpha)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_0fa9bcfc1d41a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>softmax()</code>",
        "<pre><code>    def softmax(x: np.ndarray) -> np.ndarray:\n        \"\"\"Softmax activation function.\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_5510c6d0c1969",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>__init__()</code>",
        "<pre><code>    def __init__(self, input_size: int, learning_rate: float = 0.01, \n                 activation: str = 'sigmoid'):\n        self.input_size = input_size\n        self.learning_rate = learning_rate\n        self.activation = activation\n        \n        # Initialize weights and bias\n        self.weights = np.random.randn(input_size) * 0.5\n        self.bias = 0.0\n        \n        # Set activation function\n        self.activation_func, self.activation_derivative = self._get_activation_functions(activation)\n        \n        # Training history\n        self.loss_history = []</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_9f15ba9f83474",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>_get_activation_functions()</code>",
        "<pre><code>    def _get_activation_functions(self, activation: str) -> Tuple[Callable, Callable]:\n        \"\"\"Get activation function and its derivative.\"\"\"\n        if activation == 'sigmoid':\n            return ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative\n        elif activation == 'tanh':\n            return ActivationFunctions.tanh, ActivationFunctions.tanh_derivative\n        elif activation == 'relu':\n            return ActivationFunctions.relu, ActivationFunctions.relu_derivative\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_95d1756c8a89f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>forward()</code>",
        "<pre><code>    def forward(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Forward propagation.\"\"\"\n        self.z = np.dot(X, self.weights) + self.bias\n        self.a = self.activation_func(self.z)\n        return self.a</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_f3c2cc4714dfb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>compute_cost()</code>",
        "<pre><code>    def compute_cost(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n        \"\"\"Compute binary cross-entropy loss.\"\"\"\n        # Avoid log(0)\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_69a0617f4a08e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>backward()</code>",
        "<pre><code>    def backward(self, X: np.ndarray, y_true: np.ndarray, y_pred: np.ndarray):\n        \"\"\"Backward propagation.\"\"\"\n        m = X.shape[0]\n        \n        # Compute gradients\n        dz = y_pred - y_true\n        dw = (1/m) * np.dot(X.T, dz)\n        db = (1/m) * np.sum(dz)\n        \n        # Update weights and bias\n        self.weights -= self.learning_rate * dw\n        self.bias -= self.learning_rate * db</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_07b4479b5151a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>train()</code>",
        "<pre><code>    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000):\n        \"\"\"Train the perceptron.\"\"\"\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X)\n            \n            # Compute cost\n            cost = self.compute_cost(y, y_pred)\n            self.loss_history.append(cost)\n            \n            # Backward pass\n            self.backward(X, y, y_pred)\n            \n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Cost: {cost:.4f}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_525dcf8c68b18",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>predict()</code>",
        "<pre><code>    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Make predictions.\"\"\"\n        probabilities = self.forward(X)\n        return (probabilities > 0.5).astype(int)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_785062ba54142",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>__init___part_1()</code>",
        "<pre><code>    def __init__(self, layers: List[int], activation: str = 'sigmoid', \n                 output_activation: str = 'sigmoid', learning_rate: float = 0.01):\n        self.layers = layers\n        self.num_layers = len(layers)\n        self.learning_rate = learning_rate\n        self.activation = activation\n        self.output_activation = output_activation\n        \n        # Initialize weights and biases\n        self.weights = {}\n        self.biases = {}\n        \n        for i in range(1, self.num_layers):\n            # Xavier/Glorot initialization\n            self.weights[f'W{i}'] = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2.0 / layers[i-1])\n            self.biases[f'b{i}'] = np.zeros((1, layers[i]))</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_5761c44364942",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>__init___part_2()</code>",
        "<pre><code>        \n        # Get activation functions\n        self.hidden_activation, self.hidden_activation_derivative = self._get_activation_functions(activation)\n        self.out_activation, self.out_activation_derivative = self._get_activation_functions(output_activation)\n        \n        # Training history\n        self.loss_history = []\n        self.accuracy_history = []\n        \n        # Cache for forward propagation\n        self.cache = {}</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_e6982fc5cc4d8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>_get_activation_functions()</code>",
        "<pre><code>    def _get_activation_functions(self, activation: str) -> Tuple[Callable, Callable]:\n        \"\"\"Get activation function and its derivative.\"\"\"\n        if activation == 'sigmoid':\n            return ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative\n        elif activation == 'tanh':\n            return ActivationFunctions.tanh, ActivationFunctions.tanh_derivative\n        elif activation == 'relu':\n            return ActivationFunctions.relu, ActivationFunctions.relu_derivative\n        elif activation == 'softmax':\n            return ActivationFunctions.softmax, lambda x: x  # Derivative handled separately\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_c4c6333f11989",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>forward_part_1()</code>",
        "<pre><code>    def forward(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Forward propagation through the network.\"\"\"\n        self.cache['A0'] = X\n        \n        # Hidden layers\n        for i in range(1, self.num_layers - 1):\n            Z = np.dot(self.cache[f'A{i-1}'], self.weights[f'W{i}']) + self.biases[f'b{i}']\n            A = self.hidden_activation(Z)\n            \n            self.cache[f'Z{i}'] = Z\n            self.cache[f'A{i}'] = A\n        \n        # Output layer\n        i = self.num_layers - 1\n        Z = np.dot(self.cache[f'A{i-1}'], self.weights[f'W{i}']) + self.biases[f'b{i}']</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_8ded67c223689",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>forward_part_2()</code>",
        "<pre><code>        \n        if self.output_activation == 'softmax':\n            A = self.out_activation(Z)\n        else:\n            A = self.out_activation(Z)\n        \n        self.cache[f'Z{i}'] = Z\n        self.cache[f'A{i}'] = A\n        \n        return A</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_a2edae911cd8a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>compute_cost_part_1()</code>",
        "<pre><code>    def compute_cost(self, y_true: np.ndarray, y_pred: np.ndarray, cost_type: str = 'cross_entropy') -> float:\n        \"\"\"Compute cost function.\"\"\"\n        m = y_true.shape[0]\n        \n        if cost_type == 'cross_entropy':\n            if y_true.shape[1] > 1:  # Multi-class\n                # Categorical cross-entropy\n                y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n                return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n            else:  # Binary\n                # Binary cross-entropy\n                y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n                return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        \n        elif cost_type == 'mse':\n            return np.mean((y_true - y_pred) ** 2)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_507589e5070a1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>compute_cost_part_2()</code>",
        "<pre><code>        \n        else:\n            raise ValueError(f\"Unsupported cost type: {cost_type}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_29b375ae6dc28",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>backward_part_1()</code>",
        "<pre><code>    def backward(self, X: np.ndarray, y_true: np.ndarray, y_pred: np.ndarray):\n        \"\"\"Backward propagation through the network.\"\"\"\n        m = X.shape[0]\n        gradients = {}\n        \n        # Output layer gradient\n        if self.output_activation == 'softmax' and y_true.shape[1] > 1:\n            # Softmax with categorical cross-entropy\n            dZ = y_pred - y_true\n        else:\n            # Other activations\n            dA = -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)\n            dZ = dA * self.out_activation_derivative(self.cache[f'Z{self.num_layers-1}'])\n        \n        # Gradients for output layer\n        i = self.num_layers - 1\n        gradients[f'dW{i}'] = (1/m) * np.dot(self.cache[f'A{i-1}'].T, dZ)\n        gradients[f'db{i}'] = (1/m) * np.sum(dZ, axis=0, keepdims=True)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_5376e265a190b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>backward_part_2()</code>",
        "<pre><code>        \n        # Propagate backwards through hidden layers\n        dA_prev = np.dot(dZ, self.weights[f'W{i}'].T)\n        \n        for i in range(self.num_layers - 2, 0, -1):\n            dZ = dA_prev * self.hidden_activation_derivative(self.cache[f'Z{i}'])\n            \n            gradients[f'dW{i}'] = (1/m) * np.dot(self.cache[f'A{i-1}'].T, dZ)\n            gradients[f'db{i}'] = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n            \n            if i > 1:\n                dA_prev = np.dot(dZ, self.weights[f'W{i}'].T)\n        \n        # Update weights and biases\n        for i in range(1, self.num_layers):\n            self.weights[f'W{i}'] -= self.learning_rate * gradients[f'dW{i}']\n            self.biases[f'b{i}'] -= self.learning_rate * gradients[f'db{i}']</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_a379ad3afce78",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>train_part_1()</code>",
        "<pre><code>    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, \n              cost_type: str = 'cross_entropy', verbose: bool = True):\n        \"\"\"Train the neural network.\"\"\"\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X)\n            \n            # Compute cost\n            cost = self.compute_cost(y, y_pred, cost_type)\n            self.loss_history.append(cost)\n            \n            # Compute accuracy\n            accuracy = self.compute_accuracy(y, y_pred)\n            self.accuracy_history.append(accuracy)\n            </code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_f2804367338c9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>train_part_2()</code>",
        "<pre><code>            # Backward pass\n            self.backward(X, y, y_pred)\n            \n            if verbose and epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Cost: {cost:.4f}, Accuracy: {accuracy:.4f}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_efdbfd14ed016",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>predict()</code>",
        "<pre><code>    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Make predictions.\"\"\"\n        y_pred = self.forward(X)\n        \n        if y_pred.shape[1] > 1:  # Multi-class\n            return np.argmax(y_pred, axis=1)\n        else:  # Binary\n            return (y_pred > 0.5).astype(int)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_926a973a77c04",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>predict_proba()</code>",
        "<pre><code>    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Get prediction probabilities.\"\"\"\n        return self.forward(X)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_8ef84a77a2a55",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>compute_accuracy()</code>",
        "<pre><code>    def compute_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n        \"\"\"Compute accuracy.\"\"\"\n        if y_true.shape[1] > 1:  # Multi-class\n            y_true_labels = np.argmax(y_true, axis=1)\n            y_pred_labels = np.argmax(y_pred, axis=1)\n        else:  # Binary\n            y_true_labels = y_true.flatten()\n            y_pred_labels = (y_pred > 0.5).astype(int).flatten()\n        \n        return np.mean(y_true_labels == y_pred_labels)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_04650330231fe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>__init__()</code>",
        "<pre><code>    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, \n                 beta2: float = 0.999, epsilon: float = 1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        \n        self.m_weights = {}\n        self.v_weights = {}\n        self.m_biases = {}\n        self.v_biases = {}\n        self.t = 0</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_b42332b9fc513",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>update_part_1()</code>",
        "<pre><code>    def update(self, network: NeuralNetwork, gradients: dict):\n        \"\"\"Update network parameters using Adam optimizer.\"\"\"\n        self.t += 1\n        \n        for i in range(1, network.num_layers):\n            w_key = f'W{i}'\n            b_key = f'b{i}'\n            dw_key = f'dW{i}'\n            db_key = f'db{i}'\n            \n            # Initialize momentum terms if first iteration\n            if w_key not in self.m_weights:\n                self.m_weights[w_key] = np.zeros_like(network.weights[w_key])\n                self.v_weights[w_key] = np.zeros_like(network.weights[w_key])\n                self.m_biases[b_key] = np.zeros_like(network.biases[b_key])\n                self.v_biases[b_key] = np.zeros_like(network.biases[b_key])</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_02332b0285805",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>update_part_2()</code>",
        "<pre><code>            \n            # Update momentum terms for weights\n            self.m_weights[w_key] = self.beta1 * self.m_weights[w_key] + (1 - self.beta1) * gradients[dw_key]\n            self.v_weights[w_key] = self.beta2 * self.v_weights[w_key] + (1 - self.beta2) * (gradients[dw_key] ** 2)\n            \n            # Update momentum terms for biases\n            self.m_biases[b_key] = self.beta1 * self.m_biases[b_key] + (1 - self.beta1) * gradients[db_key]\n            self.v_biases[b_key] = self.beta2 * self.v_biases[b_key] + (1 - self.beta2) * (gradients[db_key] ** 2)\n            \n            # Bias correction\n            m_w_corrected = self.m_weights[w_key] / (1 - self.beta1 ** self.t)\n            v_w_corrected = self.v_weights[w_key] / (1 - self.beta2 ** self.t)\n            m_b_corrected = self.m_biases[b_key] / (1 - self.beta1 ** self.t)\n            v_b_corrected = self.v_biases[b_key] / (1 - self.beta2 ** self.t)\n            </code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_f8ea0039b7d10",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Implement: <code>update_part_3()</code>",
        "<pre><code>            # Update parameters\n            network.weights[w_key] -= self.learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + self.epsilon)\n            network.biases[b_key] -= self.learning_rate * m_b_corrected / (np.sqrt(v_b_corrected) + self.epsilon)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_10f9175975bda",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Part-of-Speech Tagging with Accuracy Metrics</b><br>What's the key approach?",
        "<b>Approach:</b> Implement `pos_tag_text(text: str) -> List[Tuple[str, str]]` that:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Pos Tagging",
        "problem_understanding"
      ],
      "guid": "nlp_be84a3873b67f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>pos_tag_text()</code>",
        "<pre><code>def pos_tag_text(text: str) -> List[Tuple[str, str]]:\n    \"\"\"POS tag text using NLTK's default tagger (Penn Treebank tagset).\"\"\"\n    tokens = nltk.word_tokenize(text)\n    return nltk.pos_tag(tokens)</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_7f6cb490473a9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>extract_pos()</code>",
        "<pre><code>def extract_pos(tagged_text: List[Tuple[str, str]], pos_prefix: str) -> List[str]:\n    \"\"\"Extract words with specific POS tags.\n    \n    Args:\n        tagged_text: List of (word, pos) tuples\n        pos_prefix: POS tag prefix (e.g., 'NN' for nouns, 'VB' for verbs)\n    \"\"\"\n    return [word for word, pos in tagged_text if pos.startswith(pos_prefix)]</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_ec674e85bd076",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>find_noun_phrases_part_1()</code>",
        "<pre><code>def find_noun_phrases(tagged_text: List[Tuple[str, str]]) -> List[str]:\n    \"\"\"Extract simple noun phrases using POS patterns.\"\"\"\n    noun_phrases = []\n    current_phrase = []\n    \n    # Simple pattern: (DT)? (JJ)* (NN)+\n    for word, pos in tagged_text:\n        if pos == 'DT':  # Determiner\n            if current_phrase:\n                noun_phrases.append(' '.join(current_phrase))\n            current_phrase = [word]\n        elif pos.startswith('JJ'):  # Adjective\n            if current_phrase:\n                current_phrase.append(word)\n        elif pos.startswith('NN'):  # Noun\n            current_phrase.append(word)\n        else:\n            if current_phrase and any(pos.startswith('NN') for _, pos in \n                                    [(w, p) for w, p in zip(current_phrase, \n                                     [t[1] for t in tagged_text])]):\n                noun_phrases.append(' '.join(current_phrase))\n            current_phrase = []</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_2e8270570c8dd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>find_noun_phrases_part_2()</code>",
        "<pre><code>    \n    if current_phrase:\n        noun_phrases.append(' '.join(current_phrase))\n    \n    return noun_phrases</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_b2efe7cc8ddd7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>analyze_word_ambiguity_part_1()</code>",
        "<pre><code>def analyze_word_ambiguity(word: str, corpus_name: str = 'brown') -> Dict[str, float]:\n    \"\"\"Analyze POS tag distribution for an ambiguous word.\"\"\"\n    from nltk.corpus import brown\n    \n    # Get all occurrences of the word with their tags\n    word_lower = word.lower()\n    pos_counts = Counter()\n    \n    for sent in brown.tagged_sents(tagset='universal')[:10000]:  # Sample for speed\n        for token, pos in sent:\n            if token.lower() == word_lower:\n                pos_counts[pos] += 1\n    \n    total = sum(pos_counts.values())\n    if total == 0:\n        return {}</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_9c0e86d5488db",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>analyze_word_ambiguity_part_2()</code>",
        "<pre><code>    \n    return {pos: count/total for pos, count in pos_counts.items()}</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_6e504ae42b102",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>compare_taggers_part_1()</code>",
        "<pre><code>def compare_taggers(text: str) -> Dict[str, List[Tuple[str, str]]]:\n    \"\"\"Compare different POS taggers on the same text.\"\"\"\n    tokens = nltk.word_tokenize(text)\n    \n    results = {\n        'default': nltk.pos_tag(tokens),\n        'universal': nltk.pos_tag(tokens, tagset='universal')\n    }\n    \n    # Try to use spaCy if available\n    try:\n        import spacy\n        nlp = spacy.load('en_core_web_sm')\n        doc = nlp(text)\n        results['spacy'] = [(token.text, token.pos_) for token in doc]\n    except:\n        pass</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_84969adafbbff",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>compare_taggers_part_2()</code>",
        "<pre><code>    \n    return results</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_62ffcf611f185",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>pos_tag_with_confidence_part_1()</code>",
        "<pre><code>def pos_tag_with_confidence(text: str) -> List[Tuple[str, str, float]]:\n    \"\"\"POS tag with confidence scores (simplified version).\"\"\"\n    # This is a simplified demo - real confidence would come from the model\n    tagged = pos_tag_text(text)\n    \n    # Add mock confidence based on word frequency/ambiguity\n    result = []\n    for word, pos in tagged:\n        # Common unambiguous words get high confidence\n        if pos in ['DT', 'IN', 'CC', '.', ',']:\n            confidence = 0.99\n        # Potentially ambiguous words get lower confidence\n        elif word.lower() in ['run', 'meeting', 'light', 'bank']:\n            confidence = 0.75\n        else:\n            confidence = 0.90</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_602d62323dc22",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>Implement: <code>pos_tag_with_confidence_part_2()</code>",
        "<pre><code>        \n        result.append((word, pos, confidence))\n    \n    return result</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_77dddee9c53ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Regular Expressions for NLP</b><br>What's the key approach?",
        "<b>Approach:</b> Implement regex-based NLP functions:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Regex Nlp",
        "problem_understanding"
      ],
      "guid": "nlp_32e7792e5d9fb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_entities_regex_part_1()</code>",
        "<pre><code>def extract_entities_regex(text: str, \n                          custom_patterns: Optional[Dict[str, str]] = None) -> Dict[str, List[str]]:\n    \"\"\"Extract various entities using regex patterns.\"\"\"\n    entities = defaultdict(list)\n    \n    # Use default patterns plus any custom ones\n    patterns = REGEX_PATTERNS.copy()\n    if custom_patterns:\n        patterns.update(custom_patterns)\n    \n    # Extract emails\n    emails = re.findall(patterns['email'], text, re.IGNORECASE)\n    if emails:\n        entities['EMAIL'] = list(set(emails))\n    </code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_3a3e4099cfacc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_entities_regex_part_2()</code>",
        "<pre><code>    # Extract phone numbers\n    phones = re.findall(patterns['phone_us'], text)\n    phone_formatted = [f\"({area})-{prefix}-{number}\" for area, prefix, number in phones]\n    intl_phones = re.findall(patterns['phone_intl'], text)\n    all_phones = phone_formatted + intl_phones\n    if all_phones:\n        entities['PHONE'] = list(set(all_phones))\n    \n    # Extract URLs\n    urls = re.findall(patterns['url'], text, re.IGNORECASE)\n    if urls:\n        entities['URL'] = list(set(urls))\n    \n    # Extract dates (multiple formats)\n    dates = []\n    for pattern_name in ['date_mdy', 'date_dmy', 'date_ymd', 'date_written']:\n        matches = re.findall(patterns[pattern_name], text, re.IGNORECASE)\n        dates.extend([' '.join(match) if isinstance(match, tuple) else match for match in matches])</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_b4559d2ca8a41",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_entities_regex_part_3()</code>",
        "<pre><code>    \n    if dates:\n        entities['DATE'] = list(set(dates))\n    \n    # Extract times\n    times = []\n    time_12 = re.findall(patterns['time_12'], text, re.IGNORECASE)\n    time_24 = re.findall(patterns['time_24'], text)\n    times.extend([f\"{h}:{m} {ap}\" for h, m, ap in time_12])\n    times.extend(time_24)\n    if times:\n        entities['TIME'] = list(set(times))\n    \n    # Extract money amounts\n    money = []\n    for pattern_name in ['money_dollar', 'money_euro', 'money_pound', 'money_written']:\n        matches = re.findall(patterns[pattern_name], text, re.IGNORECASE)\n        money.extend(matches)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_329b8b9ce85c4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_entities_regex_part_4()</code>",
        "<pre><code>    \n    if money:\n        entities['MONEY'] = list(set(money))\n    \n    # Extract percentages\n    percentages = re.findall(patterns['percentage'], text)\n    if percentages:\n        entities['PERCENTAGE'] = list(set(percentages))\n    \n    # Extract other entities\n    for entity_type, pattern in [\n        ('SSN', 'ssn'),\n        ('CREDIT_CARD', 'credit_card'),\n        ('IP_ADDRESS', 'ip_address'),\n        ('HASHTAG', 'hashtag'),\n        ('MENTION', 'mention')\n    ]:\n        matches = re.findall(patterns[pattern], text)\n        if matches:\n            entities[entity_type] = list(set(matches))</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_e1f19a832a1d9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_entities_regex_part_5()</code>",
        "<pre><code>    \n    return dict(entities)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_af34a044f9f82",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>sentence_segmentation_part_1()</code>",
        "<pre><code>def sentence_segmentation(text: str, preserve_abbreviations: bool = True) -> List[str]:\n    \"\"\"Segment text into sentences using regex, handling abbreviations.\"\"\"\n    \n    # Common abbreviations that don't end sentences\n    abbreviations = {\n        'dr', 'mr', 'mrs', 'ms', 'prof', 'sr', 'jr', 'vs', 'etc', 'eg', 'ie',\n        'inc', 'ltd', 'corp', 'co', 'ave', 'st', 'blvd', 'rd', 'apt', 'dept',\n        'fig', 'vol', 'no', 'pp', 'cf', 'al', 'ca', 'ny', 'tx', 'fl'\n    }\n    \n    if preserve_abbreviations:\n        # Replace abbreviations temporarily\n        temp_text = text\n        abbrev_placeholders = {}\n        counter = 0</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_130e5693467a0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>sentence_segmentation_part_2()</code>",
        "<pre><code>        \n        for abbrev in abbreviations:\n            pattern = fr'\\b{re.escape(abbrev)}\\.(?!\\s*[A-Z])'\n            matches = re.finditer(pattern, temp_text, re.IGNORECASE)\n            \n            for match in reversed(list(matches)):  # Reverse to maintain indices\n                placeholder = f\"__ABBREV_{counter}__\"\n                abbrev_placeholders[placeholder] = match.group()\n                temp_text = temp_text[:match.start()] + placeholder + temp_text[match.end():]\n                counter += 1\n    else:\n        temp_text = text\n        abbrev_placeholders = {}\n    \n    # Handle ellipsis\n    temp_text = re.sub(r'\\.{2,}', '__ELLIPSIS__', temp_text)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_29cce1753d75d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>sentence_segmentation_part_3()</code>",
        "<pre><code>    \n    # Handle decimal numbers\n    temp_text = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: m.group().replace('.', '__DECIMAL__'), temp_text)\n    \n    # Split on sentence-ending punctuation\n    sentences = re.split(r'[.!?]+\\s+', temp_text)\n    \n    # Clean up sentences\n    cleaned_sentences = []\n    for sentence in sentences:\n        # Restore abbreviations\n        for placeholder, original in abbrev_placeholders.items():\n            sentence = sentence.replace(placeholder, original)\n        \n        # Restore ellipsis and decimals\n        sentence = sentence.replace('__ELLIPSIS__', '...')\n        sentence = sentence.replace('__DECIMAL__', '.')</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_f2367304b34fe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>sentence_segmentation_part_4()</code>",
        "<pre><code>        \n        sentence = sentence.strip()\n        if sentence:\n            cleaned_sentences.append(sentence)\n    \n    return cleaned_sentences</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_c5a8c23f06a75",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>pattern_based_ner_part_1()</code>",
        "<pre><code>def pattern_based_ner(text: str, patterns: Dict[str, str]) -> List[Tuple[str, str, int, int]]:\n    \"\"\"Custom named entity recognition using regex patterns.\n    \n    Returns: List of (entity_text, entity_type, start_pos, end_pos)\n    \"\"\"\n    entities = []\n    \n    for entity_type, pattern in patterns.items():\n        matches = re.finditer(pattern, text, re.IGNORECASE)\n        \n        for match in matches:\n            entity_text = match.group()\n            start_pos = match.start()\n            end_pos = match.end()\n            \n            entities.append((entity_text, entity_type, start_pos, end_pos))</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_785faaf705e6c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>pattern_based_ner_part_2()</code>",
        "<pre><code>    \n    # Sort by start position\n    entities.sort(key=lambda x: x[2])\n    \n    return entities</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_f77a2012d4de8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>clean_text_regex()</code>",
        "<pre><code>def clean_text_regex(text: str, rules: List[Tuple[str, str]]) -> str:\n    \"\"\"Apply multiple regex cleaning rules to text.\n    \n    Args:\n        text: Input text to clean\n        rules: List of (pattern, replacement) tuples\n    \"\"\"\n    cleaned_text = text\n    \n    for pattern, replacement in rules:\n        cleaned_text = re.sub(pattern, replacement, cleaned_text)\n    \n    return cleaned_text</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_e90cd5021714e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_structured_data_part_1()</code>",
        "<pre><code>def extract_structured_data(text: str) -> Dict[str, List[Dict]]:\n    \"\"\"Extract structured information like addresses, names, etc.\"\"\"\n    structured_data = defaultdict(list)\n    \n    # Address pattern (simplified US addresses)\n    address_pattern = r'\\d+\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Drive|Dr|Lane|Ln|Way|Court|Ct|Place|Pl)\\.?(?:\\s+(?:Apt|Apartment|Unit|Suite)\\s*\\w+)?'\n    \n    addresses = re.finditer(address_pattern, text, re.IGNORECASE)\n    for match in addresses:\n        structured_data['ADDRESS'].append({\n            'text': match.group(),\n            'start': match.start(),\n            'end': match.end()\n        })\n    </code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_d080776ed41fd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_structured_data_part_2()</code>",
        "<pre><code>    # Person name pattern (Title First Last)\n    name_pattern = r'\\b(?:Mr|Mrs|Ms|Dr|Prof)\\.?\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\b'\n    \n    names = re.finditer(name_pattern, text)\n    for match in names:\n        structured_data['PERSON_NAME'].append({\n            'text': match.group(),\n            'start': match.start(),\n            'end': match.end()\n        })\n    \n    # Product codes (letters and numbers)\n    product_pattern = r'\\b[A-Z]{2,3}-?\\d{3,6}\\b'\n    \n    products = re.finditer(product_pattern, text)\n    for match in products:\n        structured_data['PRODUCT_CODE'].append({\n            'text': match.group(),\n            'start': match.start(),\n            'end': match.end()\n        })</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_75187fd197754",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>extract_structured_data_part_3()</code>",
        "<pre><code>    \n    return dict(structured_data)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_2f8000144eebe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>build_custom_tokenizer_part_1()</code>",
        "<pre><code>def build_custom_tokenizer(patterns: Dict[str, str]) -> callable:\n    \"\"\"Build a custom tokenizer based on regex patterns.\"\"\"\n    \n    def custom_tokenize(text: str) -> List[Tuple[str, str]]:\n        \"\"\"Tokenize text using custom patterns.\n        \n        Returns: List of (token, token_type) tuples\n        \"\"\"\n        tokens = []\n        remaining_text = text\n        offset = 0\n        \n        while remaining_text:\n            matched = False\n            </code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_f090b65790a51",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>build_custom_tokenizer_part_2()</code>",
        "<pre><code>            # Try each pattern\n            for token_type, pattern in patterns.items():\n                match = re.match(pattern, remaining_text)\n                if match:\n                    token = match.group()\n                    tokens.append((token, token_type))\n                    \n                    # Update remaining text and offset\n                    consumed = len(token)\n                    remaining_text = remaining_text[consumed:]\n                    offset += consumed\n                    matched = True\n                    break\n            \n            if not matched:</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_0792755347e99",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>build_custom_tokenizer_part_3()</code>",
        "<pre><code>                # Default: consume one character as unknown\n                if remaining_text:\n                    tokens.append((remaining_text[0], 'UNKNOWN'))\n                    remaining_text = remaining_text[1:]\n                    offset += 1\n        \n        return tokens\n    \n    return custom_tokenize</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_69ee468c277e6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>validate_extracted_data_part_1()</code>",
        "<pre><code>def validate_extracted_data(entities: Dict[str, List[str]]) -> Dict[str, List[str]]:\n    \"\"\"Validate extracted entities and filter out false positives.\"\"\"\n    validated = {}\n    \n    # Validate emails\n    if 'EMAIL' in entities:\n        valid_emails = []\n        for email in entities['EMAIL']:\n            # Additional validation beyond basic regex\n            if '@' in email and '.' in email.split('@')[1]:\n                parts = email.split('@')\n                if len(parts) == 2 and parts[0] and parts[1]:\n                    valid_emails.append(email)\n        \n        if valid_emails:\n            validated['EMAIL'] = valid_emails</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_011cf1313a209",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>validate_extracted_data_part_2()</code>",
        "<pre><code>    \n    # Validate phone numbers\n    if 'PHONE' in entities:\n        valid_phones = []\n        for phone in entities['PHONE']:\n            # Remove all non-digits\n            digits_only = re.sub(r'\\D', '', phone)\n            # US phone should have 10 or 11 digits\n            if len(digits_only) in [10, 11]:\n                valid_phones.append(phone)\n        \n        if valid_phones:\n            validated['PHONE'] = valid_phones\n    \n    # Validate dates\n    if 'DATE' in entities:\n        valid_dates = []\n        for date_str in entities['DATE']:\n            try:</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_ee35f73a75c30",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>validate_extracted_data_part_3()</code>",
        "<pre><code>                # Try to parse the date\n                # This is a simplified validation\n                if re.match(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', date_str):\n                    valid_dates.append(date_str)\n                elif re.match(r'[A-Za-z]{3,9}\\s+\\d{1,2},?\\s+\\d{4}', date_str):\n                    valid_dates.append(date_str)\n            except:\n                continue\n        \n        if valid_dates:\n            validated['DATE'] = valid_dates\n    \n    # Copy other entities as-is\n    for entity_type, entity_list in entities.items():\n        if entity_type not in ['EMAIL', 'PHONE', 'DATE']:\n            validated[entity_type] = entity_list</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_b1c6c28e9a307",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>validate_extracted_data_part_4()</code>",
        "<pre><code>    \n    return validated</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_b6b1847caf9e5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>optimize_regex_performance_part_1()</code>",
        "<pre><code>def optimize_regex_performance(text: str, patterns: Dict[str, str]) -> Dict[str, List[str]]:\n    \"\"\"Optimized regex extraction for large texts.\"\"\"\n    entities = defaultdict(list)\n    \n    # Compile patterns once\n    compiled_patterns = {name: re.compile(pattern, re.IGNORECASE) \n                        for name, pattern in patterns.items()}\n    \n    # Single pass through text\n    for name, compiled_pattern in compiled_patterns.items():\n        matches = compiled_pattern.findall(text)\n        if matches:\n            # Handle tuple matches (from groups)\n            if matches and isinstance(matches[0], tuple):\n                matches = ['-'.join(match) if isinstance(match, tuple) else match \n                          for match in matches]</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_8e3389101fd94",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>optimize_regex_performance_part_2()</code>",
        "<pre><code>            \n            entities[name.upper()] = list(set(matches))\n    \n    return dict(entities)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_72fb8984c7cff",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>custom_tokenize_part_1()</code>",
        "<pre><code>    def custom_tokenize(text: str) -> List[Tuple[str, str]]:\n        \"\"\"Tokenize text using custom patterns.\n        \n        Returns: List of (token, token_type) tuples\n        \"\"\"\n        tokens = []\n        remaining_text = text\n        offset = 0\n        \n        while remaining_text:\n            matched = False\n            \n            # Try each pattern\n            for token_type, pattern in patterns.items():\n                match = re.match(pattern, remaining_text)\n                if match:\n                    token = match.group()\n                    tokens.append((token, token_type))</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_ede65383efcea",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>Implement: <code>custom_tokenize_part_2()</code>",
        "<pre><code>                    \n                    # Update remaining text and offset\n                    consumed = len(token)\n                    remaining_text = remaining_text[consumed:]\n                    offset += consumed\n                    matched = True\n                    break\n            \n            if not matched:\n                # Default: consume one character as unknown\n                if remaining_text:\n                    tokens.append((remaining_text[0], 'UNKNOWN'))\n                    remaining_text = remaining_text[1:]\n                    offset += 1\n        \n        return tokens</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_2940ecff611f2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Rule-based Sentiment Analysis</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 20 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Sentiment Analysis",
        "problem_understanding"
      ],
      "guid": "nlp_cf48dac6dbc83",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>analyze_sentiment_part_1()</code>",
        "<pre><code>def analyze_sentiment(text: str) -> Dict[str, float]:\n    \"\"\"\n    Analyze sentiment using rule-based approach (VADER-style).\n    \n    RULE-BASED SENTIMENT ANALYSIS:\n    - Uses pre-built dictionary of word sentiments\n    - Applies grammatical rules (intensifiers, negations)\n    - Fast and interpretable (good for production)\n    - No training data needed\n    \n    ALGORITHM:\n    1. Tokenize text and look up word sentiments\n    2. Apply intensifier rules (\"very good\" > \"good\")\n    3. Apply negation rules (\"not good\" becomes negative)\n    4. Handle punctuation emphasis (\"great!!!\" > \"great\")\n    5. Normalize scores and return distribution\n    \"\"\"</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_58b15aaf6b7ce",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>analyze_sentiment_part_2()</code>",
        "<pre><code>    \n    # STEP 1: Handle edge cases first\n    if not text or not text.strip():\n        # Return neutral sentiment for empty text\n        return {\"positive\": 0.0, \"negative\": 0.0, \"neutral\": 1.0, \"compound\": 0.0}\n    \n    # STEP 2: Simple tokenization\n    # Split on whitespace and convert to lowercase\n    words = text.lower().split()\n    \n    if not words:\n        return {\"positive\": 0.0, \"negative\": 0.0, \"neutral\": 1.0, \"compound\": 0.0}\n    \n    # STEP 3: Get base sentiment scores for each word\n    # Look up each word in sentiment lexicon\n    scores = []</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_ebec32de23254",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>analyze_sentiment_part_3()</code>",
        "<pre><code>    \n    for i, word in enumerate(words):\n        # Get base sentiment score (0 if not in lexicon)\n        base_score = SENTIMENT_LEXICON.get(word, 0.0)\n        \n        if base_score != 0:  # Only process sentiment-bearing words\n            \n            # RULE 1: Check for intensifiers in previous word\n            # \"very good\" should be more positive than \"good\"\n            if i > 0 and words[i-1] in INTENSIFIERS:\n                # Boost sentiment by 30% (VADER-style boosting)\n                base_score *= 1.3\n                print(f\"  INTENSIFIER: '{words[i-1]} {word}' -> boosted to {base_score:.2f}\")\n            \n            # RULE 2: Check for negations in previous 2 words</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_ef49526350bbe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>analyze_sentiment_part_4()</code>",
        "<pre><code>            # \"not very good\" should be negative despite \"good\" being positive\n            negated = False\n            for j in range(max(0, i-2), i):  # Look back up to 2 words\n                if words[j] in NEGATIONS:\n                    negated = True\n                    print(f\"  NEGATION: '{words[j]}' flips '{word}'\")\n                    break\n            \n            # Apply negation: flip polarity and reduce intensity\n            if negated:\n                base_score *= -0.8  # Flip sign and dampen (VADER approach)\n            \n            scores.append(base_score)\n    \n    # STEP 4: Handle punctuation emphasis</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_0686747f76cee",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>analyze_sentiment_part_5()</code>",
        "<pre><code>    # Multiple exclamation marks add emphasis: \"Great!!!\" > \"Great\"\n    exclamation_count = text.count('!')\n    if exclamation_count > 0 and scores:\n        # Add emphasis but cap the effect\n        emphasis = min(exclamation_count * 0.3, 1.0)\n        print(f\"  EMPHASIS: {exclamation_count} exclamations add {emphasis:.2f}\")\n        \n        # Apply emphasis to existing sentiment\n        scores = [s * (1 + emphasis) if s > 0 else s * (1 + emphasis) for s in scores]\n    \n    # STEP 5: Calculate final sentiment distribution\n    if not scores:\n        # No sentiment words found\n        return {\"positive\": 0.0, \"negative\": 0.0, \"neutral\": 1.0, \"compound\": 0.0}\n    </code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_6d1b5f1791f0f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>analyze_sentiment_part_6()</code>",
        "<pre><code>    # Separate positive and negative scores\n    pos_scores = [s for s in scores if s > 0]\n    neg_scores = [s for s in scores if s < 0]\n    \n    # Calculate proportions\n    pos_sum = sum(pos_scores)\n    neg_sum = abs(sum(neg_scores))  # Make positive for proportion calculation\n    \n    total = pos_sum + neg_sum\n    if total > 0:\n        pos_prop = pos_sum / total\n        neg_prop = neg_sum / total\n        neu_prop = 0.0  # In this simple version, neutral is when no sentiment words\n    else:\n        pos_prop = neg_prop = 0.0\n        neu_prop = 1.0</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_991270cdc760d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>analyze_sentiment_part_7()</code>",
        "<pre><code>    \n    # STEP 6: Calculate compound score (overall sentiment)\n    # Compound score combines all sentiment into single [-1, 1] score\n    # Used for final classification: positive if > 0.05, negative if < -0.05\n    compound = (pos_sum - neg_sum) / (total + 1) if total > 0 else 0.0\n    compound = max(-1, min(1, compound))  # Clamp to [-1, 1] range\n    \n    return {\n        \"positive\": round(pos_prop, 3),\n        \"negative\": round(neg_prop, 3), \n        \"neutral\": round(neu_prop, 3),\n        \"compound\": round(compound, 3)\n    }</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_acc7a541edc30",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>Implement: <code>classify_sentiment_part_1()</code>",
        "<pre><code>def classify_sentiment(sentiment_scores: Dict[str, float]) -> str:\n    \"\"\"\n    Convert sentiment scores to simple classification.\n    \n    CLASSIFICATION THRESHOLDS (VADER standard):\n    - compound >= 0.05: positive\n    - compound <= -0.05: negative  \n    - -0.05 < compound < 0.05: neutral\n    \n    These thresholds are empirically determined from testing.\n    \"\"\"\n    compound = sentiment_scores['compound']\n    \n    if compound >= 0.05:\n        return 'positive'\n    elif compound <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_0129bae6f8eaa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Simple LSTM for Sentiment</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 25 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Sequence Models",
        "problem_understanding"
      ],
      "guid": "nlp_8b1852f332f7c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>Implement: <code>sigmoid()</code>",
        "<pre><code>def sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-max(-500, min(500, x))))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_f3d13a2b60cb1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>Implement: <code>tanh()</code>",
        "<pre><code>def tanh(x: float) -> float:\n    return math.tanh(max(-500, min(500, x)))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_dfdbad60da6bd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>Implement: <code>dot_product()</code>",
        "<pre><code>def dot_product(vec1: List[float], vec2: List[float]) -> float:\n    return sum(a * b for a, b in zip(vec1, vec2))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_af42e4637a33d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>Implement: <code>lstm_cell_part_1()</code>",
        "<pre><code>def lstm_cell(x_t: List[float], h_prev: List[float], c_prev: List[float],\n              weights: Dict) -> Tuple[List[float], List[float]]:\n    \"\"\"Single LSTM cell forward pass.\"\"\"\n    hidden_size = len(h_prev)\n    combined = x_t + h_prev\n    \n    # Gates: forget, input, output  \n    f_t = [sigmoid(dot_product(combined, weights['Wf'][i]) + weights['bf'][i]) for i in range(hidden_size)]\n    i_t = [sigmoid(dot_product(combined, weights['Wi'][i]) + weights['bi'][i]) for i in range(hidden_size)]\n    o_t = [sigmoid(dot_product(combined, weights['Wo'][i]) + weights['bo'][i]) for i in range(hidden_size)]\n    \n    # Candidate cell state\n    c_candidate = [tanh(dot_product(combined, weights['Wc'][i]) + weights['bc'][i]) for i in range(hidden_size)]\n    \n    # Update cell and hidden states\n    c_t = [f_t[i] * c_prev[i] + i_t[i] * c_candidate[i] for i in range(hidden_size)]\n    h_t = [o_t[i] * tanh(c_t[i]) for i in range(hidden_size)]</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_14e351ccf9184",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>Implement: <code>lstm_cell_part_2()</code>",
        "<pre><code>    \n    return h_t, c_t</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_5de8e83d93162",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>Implement: <code>lstm_sentiment_part_1()</code>",
        "<pre><code>def lstm_sentiment(sequence: List[List[float]], weights: Dict) -> float:\n    \"\"\"Run LSTM over sequence and classify sentiment.\"\"\"\n    if not sequence:\n        return 0.5\n    \n    hidden_size = len(weights['bf'])\n    h_t = [0.0] * hidden_size\n    c_t = [0.0] * hidden_size\n    \n    # Process sequence\n    for x_t in sequence:\n        h_t, c_t = lstm_cell(x_t, h_t, c_t, weights)\n    \n    # Final classification\n    logit = dot_product(h_t, weights['W_output']) + weights['b_output']\n    return sigmoid(logit)</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_a0ecfa8c731ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Text Similarity Metrics</b><br>What's the key approach?",
        "<b>Approach:</b> Implement multiple similarity metrics:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Similarity",
        "problem_understanding"
      ],
      "guid": "nlp_cf895c4f35187",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>tokenize()</code>",
        "<pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"Simple tokenization.\"\"\"\n    return text.lower().split()</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_badbcb2f43178",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>cosine_similarity_part_1()</code>",
        "<pre><code>def cosine_similarity(text1: str, text2: str, method='tfidf') -> float:\n    \"\"\"Calculate cosine similarity between two texts.\"\"\"\n    if method == 'tfidf':\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n        return sklearn_cosine(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n    \n    elif method == 'bow':  # Bag of words\n        # Tokenize\n        tokens1 = tokenize(text1)\n        tokens2 = tokenize(text2)\n        \n        # Create vocabulary\n        vocab = set(tokens1 + tokens2)\n        </code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_908e28a7e0d13",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>cosine_similarity_part_2()</code>",
        "<pre><code>        # Create vectors\n        vec1 = np.array([tokens1.count(word) for word in vocab])\n        vec2 = np.array([tokens2.count(word) for word in vocab])\n        \n        # Calculate cosine similarity\n        dot_product = np.dot(vec1, vec2)\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n        \n        if norm1 * norm2 == 0:\n            return 0.0\n        \n        return dot_product / (norm1 * norm2)</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_b76a9535781b6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>jaccard_similarity_part_1()</code>",
        "<pre><code>def jaccard_similarity(text1: str, text2: str, ngram_size: int = 1) -> float:\n    \"\"\"Calculate Jaccard similarity (intersection over union).\"\"\"\n    if ngram_size == 1:\n        # Word-level Jaccard\n        set1 = set(tokenize(text1))\n        set2 = set(tokenize(text2))\n    else:\n        # N-gram Jaccard\n        set1 = set(get_ngrams(text1, ngram_size))\n        set2 = set(get_ngrams(text2, ngram_size))\n    \n    if not set1 and not set2:\n        return 1.0\n    if not set1 or not set2:\n        return 0.0</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_b448af2381343",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>jaccard_similarity_part_2()</code>",
        "<pre><code>    \n    intersection = len(set1 & set2)\n    union = len(set1 | set2)\n    \n    return intersection / union</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_725b6a1a51cb6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>get_ngrams()</code>",
        "<pre><code>def get_ngrams(text: str, n: int) -> List[str]:\n    \"\"\"Extract n-grams from text.\"\"\"\n    tokens = tokenize(text)\n    ngrams = []\n    \n    for i in range(len(tokens) - n + 1):\n        ngram = ' '.join(tokens[i:i+n])\n        ngrams.append(ngram)\n    \n    return ngrams</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_87788b4a4f6df",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>semantic_similarity_part_1()</code>",
        "<pre><code>def semantic_similarity(text1: str, text2: str, use_sentence_transformer: bool = True) -> float:\n    \"\"\"Calculate semantic similarity using embeddings.\"\"\"\n    \n    if use_sentence_transformer:\n        try:\n            from sentence_transformers import SentenceTransformer\n            model = SentenceTransformer('all-MiniLM-L6-v2')\n            \n            # Encode sentences\n            embeddings = model.encode([text1, text2])\n            \n            # Calculate cosine similarity\n            similarity = sklearn_cosine([embeddings[0]], [embeddings[1]])[0][0]\n            return float(similarity)\n            \n        except ImportError:\n            print(\"Install sentence-transformers: pip install sentence-transformers\")\n            use_sentence_transformer = False</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_98f821251ce7b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>semantic_similarity_part_2()</code>",
        "<pre><code>    \n    if not use_sentence_transformer:\n        # Fallback: Simple word embedding average\n        try:\n            import spacy\n            nlp = spacy.load('en_core_web_sm')\n            \n            doc1 = nlp(text1)\n            doc2 = nlp(text2)\n            \n            # Use spaCy's similarity (based on word vectors)\n            return doc1.similarity(doc2)\n            \n        except:\n            # Ultimate fallback: enhanced bag-of-words\n            return cosine_similarity(text1, text2, method='tfidf')</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_ec2652c3bd61d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>levenshtein_distance_part_1()</code>",
        "<pre><code>def levenshtein_distance(text1: str, text2: str, normalize: bool = True) -> float:\n    \"\"\"Calculate Levenshtein (edit) distance between texts.\"\"\"\n    if not text1:\n        return len(text2) if not normalize else 1.0\n    if not text2:\n        return len(text1) if not normalize else 1.0\n    \n    # Create matrix\n    matrix = [[0] * (len(text2) + 1) for _ in range(len(text1) + 1)]\n    \n    # Initialize first row and column\n    for i in range(len(text1) + 1):\n        matrix[i][0] = i\n    for j in range(len(text2) + 1):\n        matrix[0][j] = j</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_5ed4570d39064",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>levenshtein_distance_part_2()</code>",
        "<pre><code>    \n    # Fill matrix\n    for i in range(1, len(text1) + 1):\n        for j in range(1, len(text2) + 1):\n            if text1[i-1] == text2[j-1]:\n                cost = 0\n            else:\n                cost = 1\n            \n            matrix[i][j] = min(\n                matrix[i-1][j] + 1,      # deletion\n                matrix[i][j-1] + 1,      # insertion\n                matrix[i-1][j-1] + cost  # substitution\n            )\n    \n    distance = matrix[len(text1)][len(text2)]</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_8553cd54d0210",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>levenshtein_distance_part_3()</code>",
        "<pre><code>    \n    if normalize:\n        # Normalize by maximum possible distance\n        max_len = max(len(text1), len(text2))\n        return 1 - (distance / max_len)\n    \n    return distance</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_8716f3c7b551a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>find_near_duplicates_part_1()</code>",
        "<pre><code>def find_near_duplicates(texts: List[str], threshold: float = 0.8) -> List[Tuple[int, int, float]]:\n    \"\"\"Find near-duplicate texts using MinHash.\"\"\"\n    minhash = MinHashLSH(num_hashes=64, ngram_size=3)\n    \n    # Compute signatures for all texts\n    signatures = [minhash.compute_minhash(text) for text in texts]\n    \n    # Find similar pairs\n    similar_pairs = []\n    \n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            similarity = minhash.jaccard_similarity_minhash(signatures[i], signatures[j])\n            if similarity >= threshold:\n                similar_pairs.append((i, j, similarity))</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_aa454af7d0180",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>find_near_duplicates_part_2()</code>",
        "<pre><code>    \n    return similar_pairs</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_4365f149ae194",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>similarity_matrix_part_1()</code>",
        "<pre><code>def similarity_matrix(texts: List[str], method: str = 'cosine') -> np.ndarray:\n    \"\"\"Compute pairwise similarity matrix for multiple texts.\"\"\"\n    n = len(texts)\n    matrix = np.zeros((n, n))\n    \n    for i in range(n):\n        for j in range(i, n):\n            if i == j:\n                matrix[i][j] = 1.0\n            else:\n                if method == 'cosine':\n                    sim = cosine_similarity(texts[i], texts[j])\n                elif method == 'jaccard':\n                    sim = jaccard_similarity(texts[i], texts[j])\n                elif method == 'semantic':\n                    sim = semantic_similarity(texts[i], texts[j])\n                else:\n                    raise ValueError(f\"Unknown method: {method}\")</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_19efbdee61813",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>similarity_matrix_part_2()</code>",
        "<pre><code>                \n                matrix[i][j] = sim\n                matrix[j][i] = sim  # Symmetric\n    \n    return matrix</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_47de7d1ab9d9b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>__init__()</code>",
        "<pre><code>    def __init__(self, num_hashes: int = 128, ngram_size: int = 3):\n        self.num_hashes = num_hashes\n        self.ngram_size = ngram_size\n        self.hash_funcs = self._generate_hash_functions()</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_a598130ba43e1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>_generate_hash_functions()</code>",
        "<pre><code>    def _generate_hash_functions(self):\n        \"\"\"Generate hash functions for MinHash.\"\"\"\n        hash_funcs = []\n        for i in range(self.num_hashes):\n            # Use different seeds for different hash functions\n            seed = i\n            hash_funcs.append(lambda x, s=seed: int(hashlib.md5(f\"{s}{x}\".encode()).hexdigest(), 16))\n        return hash_funcs</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_73275183921c7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>get_shingles()</code>",
        "<pre><code>    def get_shingles(self, text: str) -> Set[str]:\n        \"\"\"Convert text to shingles (n-grams).\"\"\"\n        return set(get_ngrams(text, self.ngram_size))</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_39c23eadb98df",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>compute_minhash()</code>",
        "<pre><code>    def compute_minhash(self, text: str) -> List[int]:\n        \"\"\"Compute MinHash signature for text.\"\"\"\n        shingles = self.get_shingles(text)\n        if not shingles:\n            return [0] * self.num_hashes\n        \n        signature = []\n        for hash_func in self.hash_funcs:\n            min_hash = min(hash_func(shingle) for shingle in shingles)\n            signature.append(min_hash)\n        \n        return signature</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_d92dbee8533b5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Implement: <code>jaccard_similarity_minhash()</code>",
        "<pre><code>    def jaccard_similarity_minhash(self, sig1: List[int], sig2: List[int]) -> float:\n        \"\"\"Estimate Jaccard similarity from MinHash signatures.\"\"\"\n        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)\n        return matches / len(sig1)</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_4e971a1aabeaf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Stemming vs Lemmatization</b><br>What's the key approach?",
        "<b>Approach:</b> Implement two functions:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Stemming Lemmatization",
        "problem_understanding"
      ],
      "guid": "nlp_452ce876a7e10",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>Implement: <code>get_wordnet_pos()</code>",
        "<pre><code>def get_wordnet_pos(treebank_tag):\n    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags.\"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN  # default</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_9f658ea26fc45",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>Implement: <code>stem_words()</code>",
        "<pre><code>def stem_words(words: List[str]) -> List[str]:\n    \"\"\"Apply Porter stemming to words.\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word.lower()) for word in words]</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_614878ebd40d6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>Implement: <code>lemmatize_words()</code>",
        "<pre><code>def lemmatize_words(words: List[str], pos_tags: Optional[List[str]] = None) -> List[str]:\n    \"\"\"Lemmatize words with optional POS tags for better accuracy.\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    \n    if pos_tags is None:\n        # Simple lemmatization without POS\n        return [lemmatizer.lemmatize(word.lower()) for word in words]\n    \n    # Lemmatize with POS tags\n    lemmatized = []\n    for word, pos in zip(words, pos_tags):\n        wordnet_pos = get_wordnet_pos(pos)\n        lemmatized.append(lemmatizer.lemmatize(word.lower(), pos=wordnet_pos))\n    \n    return lemmatized</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_5abe29151d1aa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>Implement: <code>compare_methods_part_1()</code>",
        "<pre><code>def compare_methods(words: List[str]) -> dict:\n    \"\"\"Compare stemming vs lemmatization results.\"\"\"\n    import nltk\n    \n    # Get POS tags\n    pos_tags = [pos for _, pos in nltk.pos_tag(words)]\n    \n    stemmed = stem_words(words)\n    lemmatized_simple = lemmatize_words(words)\n    lemmatized_pos = lemmatize_words(words, pos_tags)\n    \n    return {\n        'original': words,\n        'stemmed': stemmed,\n        'lemmatized_simple': lemmatized_simple,\n        'lemmatized_with_pos': lemmatized_pos,\n        'pos_tags': pos_tags\n    }</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_4ada26f389148",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Remove Stopwords (Configurable)</b><br>What's the key approach?",
        "<b>Approach:</b> Implement `remove_stopwords(tokens: List[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]` that removes English stopwords from a token list.<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Stop Word Removal",
        "problem_understanding"
      ],
      "guid": "nlp_6374ee1ee712e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stop_word_removal",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stop Word Removal</b><br>Implement: <code>remove_stopwords()</code>",
        "<pre><code>def remove_stopwords(tokens: Iterable[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]:\n    \"\"\"Remove stopwords, preserving order.\n\n    Case-insensitive membership check, preserves original casing in the output.\n    \"\"\"\n    stop_set = set(ENGLISH_STOPWORDS)\n    if extra_stopwords:\n        stop_set |= {w.lower() for w in extra_stopwords}\n\n    cleaned: List[str] = []\n    for token in tokens:\n        if token and token.lower() not in stop_set:\n            cleaned.append(token)\n    return cleaned</code></pre>",
        "Stop Word Removal",
        "implementation"
      ],
      "guid": "nlp_082f7aabfe84b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stop_word_removal",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: TF-IDF Implementation</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 30 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Tfidf",
        "problem_understanding"
      ],
      "guid": "nlp_a308688e35326",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>compute_tfidf_part_1()</code>",
        "<pre><code>def compute_tfidf(documents: List[str]) -> List[Dict[str, float]]:\n    \"\"\"\n    Compute TF-IDF vectors for documents.\n    \n    TF-IDF = Term Frequency × Inverse Document Frequency\n    - Emphasizes important words that appear frequently in a document\n    - But rarely across the entire collection\n    \"\"\"\n    # STEP 1: Handle edge cases\n    if not documents:\n        return []\n    \n    # STEP 2: Tokenize all documents (simple whitespace splitting)\n    # In real interviews, discuss more sophisticated tokenization\n    tokenized_docs = [doc.lower().split() for doc in documents]</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_b7e013050508b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>compute_tfidf_part_2()</code>",
        "<pre><code>    \n    # STEP 3: Build vocabulary from all unique words\n    # This creates our feature space - each unique word becomes a dimension\n    vocab = set()\n    for doc in tokenized_docs:\n        vocab.update(doc)\n    vocab = sorted(list(vocab))  # Sort for consistency\n    \n    # STEP 4: Calculate Document Frequency (DF) for each term\n    # DF = number of documents containing the term\n    # Used in IDF calculation: IDF = log(total_docs / doc_freq)\n    doc_freq = {}\n    for term in vocab:\n        doc_freq[term] = sum(1 for doc in tokenized_docs if term in doc)\n    </code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_5ad0891d9b134",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>compute_tfidf_part_3()</code>",
        "<pre><code>    # STEP 5: Calculate TF-IDF for each document\n    tfidf_vectors = []\n    num_docs = len(documents)\n    \n    for doc in tokenized_docs:\n        # Count term frequencies in this document\n        term_counts = Counter(doc)\n        doc_length = len(doc)\n        tfidf_vector = {}\n        \n        for term in vocab:\n            # TERM FREQUENCY (TF): How often term appears in this document\n            # Normalized by document length to handle different document sizes\n            tf = term_counts[term] / doc_length if doc_length > 0 else 0\n            </code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_544ce137f6087",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>compute_tfidf_part_4()</code>",
        "<pre><code>            # INVERSE DOCUMENT FREQUENCY (IDF): How rare the term is across collection\n            # log(total_docs / docs_containing_term)\n            # Rare terms get higher IDF scores\n            idf = math.log(num_docs / doc_freq[term])\n            \n            # TF-IDF SCORE: Combines term importance in document (TF) \n            # with term rarity in collection (IDF)\n            tfidf_vector[term] = tf * idf\n        \n        tfidf_vectors.append(tfidf_vector)\n    \n    return tfidf_vectors</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_1f9e7ce1f81b0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>cosine_similarity_part_1()</code>",
        "<pre><code>def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two TF-IDF vectors.\n    \n    Cosine similarity = dot_product(v1, v2) / (||v1|| × ||v2||)\n    - Returns value between 0 and 1 (since TF-IDF values are non-negative)\n    - 1.0 = identical vectors, 0.0 = completely different\n    \"\"\"\n    # STEP 1: Find common terms between the two vectors\n    # Only these contribute to the dot product\n    common_terms = set(vec1.keys()) & set(vec2.keys())\n    \n    if not common_terms:\n        return 0.0  # No overlap = no similarity\n    </code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_a205ec81e6cc2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>cosine_similarity_part_2()</code>",
        "<pre><code>    # STEP 2: Calculate dot product\n    # Sum of element-wise multiplication for common terms\n    dot_product = sum(vec1[term] * vec2[term] for term in common_terms)\n    \n    # STEP 3: Calculate vector norms (magnitudes)\n    # ||v|| = sqrt(sum of squared elements)\n    norm1 = math.sqrt(sum(val**2 for val in vec1.values()))\n    norm2 = math.sqrt(sum(val**2 for val in vec2.values()))\n    \n    # STEP 4: Handle edge case of zero vectors\n    if norm1 == 0 or norm2 == 0:\n        return 0.0  # Can't compute similarity with zero vector\n    \n    # STEP 5: Return cosine similarity\n    # Normalized dot product gives us the cosine of angle between vectors\n    return dot_product / (norm1 * norm2)</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_5ab6ed49e8a38",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>find_similar_documents_part_1()</code>",
        "<pre><code>def find_similar_documents(documents: List[str], query: str) -> int:\n    \"\"\"\n    Find most similar document to query using TF-IDF + cosine similarity.\n    \n    This is the core of many search and recommendation systems.\n    \"\"\"\n    # STEP 1: Create unified document collection\n    # Include query as the last document for TF-IDF calculation\n    all_texts = documents + [query]\n    \n    # STEP 2: Compute TF-IDF for all documents + query\n    # This ensures query and documents are in same vector space\n    tfidf_vectors = compute_tfidf(all_texts)\n    \n    # STEP 3: Extract query vector (last one)\n    query_vector = tfidf_vectors[-1]</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_b9ab9494ba792",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Implement: <code>find_similar_documents_part_2()</code>",
        "<pre><code>    \n    # STEP 4: Compare query with each document\n    max_similarity = -1\n    most_similar_idx = 0\n    \n    # Only compare with documents (exclude query vector)\n    for i, doc_vector in enumerate(tfidf_vectors[:-1]):\n        similarity = cosine_similarity(query_vector, doc_vector)\n        \n        # Track the most similar document\n        if similarity > max_similarity:\n            max_similarity = similarity\n            most_similar_idx = i\n    \n    return most_similar_idx</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_040da2e46c98f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the TF-IDF Formula",
        "<h3>tfidf_vector[term] = tf * idf</h3><br><p><i>Term Frequency × Inverse Document Frequency</i></p><br><details><summary>Context</summary><pre># TF-IDF SCORE: Combines term importance in document (TF)\n# with term rarity in collection (IDF)\ntfidf_vector[term] = tf * idf\ntfidf_vectors.append(tfidf_vector)</pre></details>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_70c73cf37675e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the TF-IDF Formula",
        "<h3>tfidf = tf * idf</h3><br><p><i>Term Frequency × Inverse Document Frequency</i></p><br><details><summary>Context</summary><pre>tf = doc0_counts[word] / doc0_length\nidf = math.log(len(docs) / doc_freq[word])\ntfidf = tf * idf\nprint(f\"   '{word}': TF={tf:.3f}, IDF={idf:.3f}, TF-IDF={tfidf:.3f}\")</pre></details>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_a7e0ade611b08",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>What's the Time Complexity?",
        "<b>O(d×v)</b><br><i>See Big O notation reference</i>",
        "Tfidf",
        "complexity"
      ],
      "guid": "nlp_c50c8b9631175",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>What's the Space Complexity?",
        "<b>O(d×v)</b><br><i>See Big O notation reference</i>",
        "Tfidf",
        "complexity"
      ],
      "guid": "nlp_fe9daeb0fd43b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMO: Show step-by-step execution</strong></p>",
        "Tfidf",
        "interview_insights"
      ],
      "guid": "nlp_a2e75e84f3ff1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Text Classification Pipeline</b><br>What's the key approach?",
        "<b>Approach:</b> Build a complete text classification system:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Text Classification",
        "problem_understanding"
      ],
      "guid": "nlp_70cd353bbe84c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>extract_features_part_1()</code>",
        "<pre><code>def extract_features(texts: List[str], method: str = 'tfidf') -> Tuple[List[List[float]], List[str]]:\n    \"\"\"\n    Extract features from texts for classification.\n    \n    This is the MOST IMPORTANT step in text classification.\n    Feature quality determines model performance more than algorithm choice.\n    \"\"\"\n    \n    # STEP 1: Build vocabulary from all texts\n    # This creates our feature space - each unique word becomes a dimension\n    vocab = set()\n    for text in texts:\n        words = text.lower().split()  # Simple tokenization\n        vocab.update(words)\n    vocab = sorted(list(vocab))  # Sort for consistency</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_ee63369955e68",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>extract_features_part_2()</code>",
        "<pre><code>    \n    if method == 'tfidf':\n        # STEP 2: Calculate document frequencies for IDF computation\n        # DF = number of documents containing each term\n        doc_freq = {}\n        for word in vocab:\n            doc_freq[word] = sum(1 for text in texts if word in text.lower())\n        \n        # STEP 3: Convert each text to TF-IDF vector\n        feature_matrix = []\n        for text in texts:\n            words = text.lower().split()\n            word_counts = Counter(words)\n            doc_length = len(words)\n            </code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_dcb2e7f6de52f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>extract_features_part_3()</code>",
        "<pre><code>            # Calculate TF-IDF for each vocabulary word\n            tfidf_vector = []\n            for word in vocab:\n                # TF: How often word appears in this document (normalized)\n                tf = word_counts[word] / doc_length if doc_length > 0 else 0\n                \n                # IDF: How rare the word is across all documents\n                idf = math.log(len(texts) / doc_freq[word])\n                \n                # TF-IDF: Combines local importance (TF) with global rarity (IDF)\n                tfidf_score = tf * idf\n                tfidf_vector.append(tfidf_score)\n            \n            feature_matrix.append(tfidf_vector)\n        \n        return feature_matrix, vocab</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_31c00014ada82",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>extract_features_part_4()</code>",
        "<pre><code>    \n    else:  # Simple bag-of-words\n        feature_matrix = []\n        for text in texts:\n            word_counts = Counter(text.lower().split())\n            bow_vector = [word_counts[word] for word in vocab]\n            feature_matrix.append(bow_vector)\n        \n        return feature_matrix, vocab</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_94fd16294adb4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>train_logistic_regression_part_1()</code>",
        "<pre><code>def train_logistic_regression(X: List[List[float]], y: List[int]) -> Dict:\n    \"\"\"\n    Train logistic regression classifier from scratch.\n    \n    Logistic regression is linear classifier with sigmoid activation.\n    Good baseline for text classification - simple but effective.\n    \"\"\"\n    # STEP 1: Convert to numpy arrays for easier math\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # STEP 2: Add bias term (intercept)\n    # This allows the decision boundary to not pass through origin\n    X_with_bias = np.column_stack([np.ones(len(X)), X_np])\n    </code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_fb816b6aeb819",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>train_logistic_regression_part_2()</code>",
        "<pre><code>    # STEP 3: Initialize weights randomly (small values)\n    # Small initialization prevents sigmoid saturation early in training\n    n_features = X_with_bias.shape[1]\n    weights = np.random.randn(n_features) * 0.01\n    \n    # STEP 4: Training loop using gradient descent\n    learning_rate = 0.01  # Step size for weight updates\n    epochs = 100          # Number of training iterations\n    \n    for epoch in range(epochs):\n        # FORWARD PASS: Compute predictions\n        # Linear combination followed by sigmoid activation\n        logits = X_with_bias @ weights           # Linear part: Xw + b\n        predictions = 1 / (1 + np.exp(-logits)) # Sigmoid: maps to [0,1]\n        </code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_22f27f9f1d422",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>train_logistic_regression_part_3()</code>",
        "<pre><code>        # BACKWARD PASS: Compute gradients\n        # Gradient of cross-entropy loss w.r.t. weights\n        errors = predictions - y_np              # Prediction errors\n        gradients = (X_with_bias.T @ errors) / len(X)  # Average gradient\n        \n        # UPDATE WEIGHTS: Move in opposite direction of gradient\n        weights -= learning_rate * gradients\n    \n    return {'weights': weights, 'type': 'logistic'}</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_0eec7772bb131",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>predict_logistic()</code>",
        "<pre><code>def predict_logistic(X: List[List[float]], model: Dict) -> List[int]:\n    \"\"\"Make predictions with trained logistic regression model.\"\"\"\n    X_np = np.array(X)\n    \n    # Add bias term (same as training)\n    X_with_bias = np.column_stack([np.ones(len(X)), X_np])\n    \n    # Calculate probabilities\n    logits = X_with_bias @ model['weights']\n    probabilities = 1 / (1 + np.exp(-logits))\n    \n    # Convert to binary predictions (threshold = 0.5)\n    predictions = (probabilities > 0.5).astype(int)\n    \n    return predictions.tolist()</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_9b3c04cffa864",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>evaluate_classifier_part_1()</code>",
        "<pre><code>def evaluate_classifier(y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n    \"\"\"\n    Calculate key evaluation metrics.\n    \n    These are what interviewers ask about most:\n    - Accuracy: Overall correctness\n    - Precision: Of predicted positives, how many are correct?\n    - Recall: Of actual positives, how many did we find?\n    - F1: Balanced measure combining precision and recall\n    \"\"\"\n    # Basic accuracy calculation\n    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n    accuracy = correct / len(y_true) if y_true else 0.0\n    \n    # For binary classification, calculate detailed metrics\n    if set(y_true + y_pred) <= {0, 1}:</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_40b57a43b86ac",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>evaluate_classifier_part_2()</code>",
        "<pre><code>        # Confusion matrix components\n        tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)  # True Positives\n        fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)  # False Positives\n        fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)  # False Negatives\n        \n        # Calculate metrics with zero-division protection\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n        \n        return {\n            'accuracy': accuracy,\n            'precision': precision, \n            'recall': recall,\n            'f1': f1\n        }</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_b19e6c2598473",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Implement: <code>evaluate_classifier_part_3()</code>",
        "<pre><code>    \n    return {'accuracy': accuracy}</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_e06d9d69b88e3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the TF-IDF Formula",
        "<h3>tfidf_score = tf * idf</h3><br><p><i>Term Frequency × Inverse Document Frequency</i></p><br><details><summary>Context</summary><pre># TF-IDF: Combines local importance (TF) with global rarity (IDF)\ntfidf_score = tf * idf\ntfidf_vector.append(tfidf_score)</pre></details>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_991b7332a9ee7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Precision Formula",
        "<h3>precision = tp / (tp + fp)</h3><br><p><i>True Positives / (True Positives + False Positives)</i></p><br><details><summary>Context</summary><pre># Calculate metrics with zero-division protection\nprecision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\nrecall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\nf1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0</pre></details>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_85f5643fac8ed",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Text Tokenization</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 15 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Tokenization",
        "problem_understanding"
      ],
      "guid": "nlp_77a96cae21e8b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>tokenize_part_1()</code>",
        "<pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"\n    Tokenize text into words, preserving contractions and handling punctuation.\n    \n    This is ALWAYS asked in NLP interviews - seems simple but has many edge cases.\n    \n    Key challenges:\n    - Contractions: \"don't\" should stay as one token, not [\"don\", \"'\", \"t\"]\n    - Punctuation: \"Hello!\" should become [\"Hello\", \"!\"]\n    - Empty/None input: Handle gracefully\n    - Unicode characters: Different languages, emojis\n    \"\"\"\n    \n    # STEP 1: Handle edge cases first\n    # Always check for None/empty input in interviews\n    if not text:\n        return []</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_5186a5c87e638",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>tokenize_part_2()</code>",
        "<pre><code>    \n    # STEP 2: Use regex pattern for tokenization\n    # This is the most robust approach for handling complex cases\n    \n    # PATTERN EXPLANATION:\n    # \\w+(?:'\\w+)?  - Matches word characters, optionally followed by apostrophe + more word chars\n    #                 This handles contractions like \"don't\", \"I'm\", \"we'll\"\n    # |             - OR operator\n    # [^\\w\\s]       - Matches any non-word, non-space character (punctuation)\n    #                 This treats each punctuation mark as separate token\n    \n    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n    \n    # re.findall returns all non-overlapping matches\n    tokens = re.findall(pattern, text)</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_7f0b2baa0cd21",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>tokenize_part_3()</code>",
        "<pre><code>    \n    return tokens</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_4e90034c80639",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>tokenize_simple_part_1()</code>",
        "<pre><code>def tokenize_simple(text: str) -> List[str]:\n    \"\"\"\n    Alternative approach: Replace-then-split method.\n    \n    Sometimes interviewers want to see multiple approaches.\n    This is simpler but less robust than regex.\n    \"\"\"\n    if not text:\n        return []\n    \n    # STEP 1: Replace punctuation with spaces (except apostrophes)\n    # This preserves contractions while isolating other punctuation\n    cleaned = re.sub(r\"[^\\w\\s']\", \" \", text)\n    \n    # STEP 2: Split on whitespace</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_8aad01b16e539",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>tokenize_simple_part_2()</code>",
        "<pre><code>    # Simple but effective for basic cases\n    return cleaned.split()</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_e730730495be6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>advanced_tokenize_part_1()</code>",
        "<pre><code>def advanced_tokenize(text: str, preserve_case: bool = False, \n                     handle_urls: bool = True, handle_emails: bool = True) -> List[str]:\n    \"\"\"\n    Advanced tokenization with additional features.\n    \n    Shows awareness of real-world tokenization challenges.\n    Good for follow-up questions about production systems.\n    \"\"\"\n    if not text:\n        return []\n    \n    processed_text = text\n    \n    # STEP 1: Handle special entities before general tokenization\n    if handle_urls:</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_c5fd1be83cfcc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>advanced_tokenize_part_2()</code>",
        "<pre><code>        # Replace URLs with special token\n        # Pattern matches http(s) URLs\n        url_pattern = r'https?://[^\\s]+'\n        processed_text = re.sub(url_pattern, '<URL>', processed_text)\n    \n    if handle_emails:\n        # Replace emails with special token\n        # Basic email pattern for demonstration\n        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        processed_text = re.sub(email_pattern, '<EMAIL>', processed_text)\n    \n    # STEP 2: Case handling\n    if not preserve_case:\n        processed_text = processed_text.lower()\n    </code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_1a9328002fc4e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>advanced_tokenize_part_3()</code>",
        "<pre><code>    # STEP 3: Tokenize using main pattern\n    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n    tokens = re.findall(pattern, processed_text)\n    \n    return tokens</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_24865eeb8332c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>handle_contractions_explicitly_part_1()</code>",
        "<pre><code>def handle_contractions_explicitly(text: str) -> str:\n    \"\"\"\n    Explicit contraction handling - shows deep understanding.\n    \n    Some interviewers want to see you handle contractions manually.\n    This demonstrates knowledge of English language patterns.\n    \"\"\"\n    # Common contractions mapping\n    # In production, you'd have a much larger dictionary\n    contractions = {\n        \"don't\": \"do not\",\n        \"won't\": \"will not\", \n        \"can't\": \"cannot\",\n        \"n't\": \" not\",  # General pattern for negations\n        \"'ll\": \" will\",\n        \"'re\": \" are\", \n        \"'ve\": \" have\",\n        \"'m\": \" am\",\n        \"'d\": \" would\"  # or \"had\" - context dependent\n    }</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_eced298f2437d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>handle_contractions_explicitly_part_2()</code>",
        "<pre><code>    \n    # Apply contractions in order (longest first)\n    expanded = text\n    for contraction, expansion in sorted(contractions.items(), key=len, reverse=True):\n        # Use word boundaries to avoid partial matches\n        pattern = r'\\b' + re.escape(contraction) + r'\\b'\n        expanded = re.sub(pattern, expansion, expanded, flags=re.IGNORECASE)\n    \n    return expanded</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_0252e0278de4b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>test_tokenization_part_1()</code>",
        "<pre><code>def test_tokenization():\n    \"\"\"\n    Comprehensive test cases that cover edge cases interviewers ask about.\n    \n    Practice explaining why each test case is important.\n    \"\"\"\n    test_cases = [\n        # Basic cases\n        (\"Hello world\", [\"Hello\", \"world\"]),\n        (\"Hello world!\", [\"Hello\", \"world\", \"!\"]),\n        \n        # Contractions (most common edge case in interviews)\n        (\"don't go\", [\"don't\", \"go\"]),\n        (\"I'm happy\", [\"I'm\", \"happy\"]),\n        (\"We'll see\", [\"We'll\", \"see\"]),</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_8cff9e36e6c6e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>test_tokenization_part_2()</code>",
        "<pre><code>        \n        # Punctuation handling\n        (\"Hello, world!\", [\"Hello\", \",\", \"world\", \"!\"]),\n        (\"What?!?\", [\"What\", \"?\", \"!\", \"?\"]),\n        \n        # Edge cases (always test these in interviews)\n        (\"\", []),  # Empty string\n        (\"   \", []),  # Only spaces\n        (\"one,two;three\", [\"one\", \",\", \"two\", \";\", \"three\"]),\n        \n        # Complex contractions\n        (\"I've been there\", [\"I've\", \"been\", \"there\"]),\n        (\"You're right\", [\"You're\", \"right\"]),\n        \n        # Numbers and special characters\n        (\"Call 911!\", [\"Call\", \"911\", \"!\"]),\n        (\"Price: $19.99\", [\"Price\", \":\", \"$\", \"19.99\"]),\n    ]</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_645186c2b6fd8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>test_tokenization_part_3()</code>",
        "<pre><code>    \n    print(\"TOKENIZATION TEST SUITE\")\n    print(\"=\" * 30)\n    \n    passed = 0\n    total = len(test_cases)\n    \n    for text, expected in test_cases:\n        result = tokenize(text)\n        \n        # Check if result matches expected\n        if result == expected:\n            status = \"✓ PASS\"\n            passed += 1\n        else:\n            status = \"✗ FAIL\"</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_f3204fd41d15b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Implement: <code>test_tokenization_part_4()</code>",
        "<pre><code>        \n        print(f\"{status} '{text}' -> {result}\")\n        if result != expected:\n            print(f\"     Expected: {expected}\")\n    \n    print(f\"\\nSUCCESS RATE: {passed}/{total} ({100*passed/total:.1f}%)\")\n    \n    return passed == total</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_8b52e9122a61e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>What's the Time Complexity?",
        "<b>O(n)</b><br><i>Linear time - grows proportionally with input size</i>",
        "Tokenization",
        "complexity"
      ],
      "guid": "nlp_51da2c193b99c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>What's the Space Complexity?",
        "<b>O(n)</b><br><i>Linear time - grows proportionally with input size</i>",
        "Tokenization",
        "complexity"
      ],
      "guid": "nlp_19008083fa32e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMO: Show your thinking process</strong></p>",
        "Tokenization",
        "interview_insights"
      ],
      "guid": "nlp_7317ff70b6254",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>Alternative approach: Replace-then-split method</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_ff156324498a5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>Sometimes interviewers want to see multiple approaches</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_2bb91f4141a8a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>This is simpler</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_2ce37fa4b0c0a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>less robust than regex.</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_714342ba60f79",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Byte Pair Encoding (BPE) Tokenizer</b><br>What's the key approach?",
        "<b>Approach:</b> **Time: 30 minutes**<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Tokenization Advanced",
        "problem_understanding"
      ],
      "guid": "nlp_6e8ce2ff880f2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>get_word_frequencies()</code>",
        "<pre><code>def get_word_frequencies(texts: List[str]) -> Dict[str, int]:\n    \"\"\"Get word frequencies with end-of-word marker.\"\"\"\n    word_freqs = Counter()\n    \n    for text in texts:\n        words = text.lower().split()\n        for word in words:\n            # Add end-of-word marker\n            word_with_marker = ' '.join(word) + ' </w>'\n            word_freqs[word_with_marker] += 1\n    \n    return dict(word_freqs)</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_50c0b24d16618",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>get_pairs()</code>",
        "<pre><code>def get_pairs(word_freqs: Dict[str, int]) -> Counter:\n    \"\"\"Get all adjacent character pairs with their frequencies.\"\"\"\n    pairs = Counter()\n    \n    for word, freq in word_freqs.items():\n        chars = word.split()\n        for i in range(len(chars) - 1):\n            pair = (chars[i], chars[i + 1])\n            pairs[pair] += freq\n    \n    return pairs</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_5f32a8757aedf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>merge_vocab()</code>",
        "<pre><code>def merge_vocab(pair: Tuple[str, str], word_freqs: Dict[str, int]) -> Dict[str, int]:\n    \"\"\"Merge most frequent pair in vocabulary.\"\"\"\n    new_word_freqs = {}\n    bigram = re.escape(' '.join(pair))\n    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n    \n    for word, freq in word_freqs.items():\n        # Replace the pair with merged version\n        new_word = pattern.sub(''.join(pair), word)\n        new_word_freqs[new_word] = freq\n    \n    return new_word_freqs</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_ca1b300465491",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>build_bpe_vocab_part_1()</code>",
        "<pre><code>def build_bpe_vocab(texts: List[str], num_merges: int = 10) -> Dict[str, int]:\n    \"\"\"Build BPE vocabulary through iterative merging.\"\"\"\n    if not texts:\n        return {}\n    \n    # Initialize with character-level words\n    word_freqs = get_word_frequencies(texts)\n    \n    # Start with character vocabulary\n    vocab = set()\n    for word in word_freqs:\n        vocab.update(word.split())\n    \n    # Perform merges\n    for i in range(num_merges):\n        pairs = get_pairs(word_freqs)</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_80f5f6e69c8c6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>build_bpe_vocab_part_2()</code>",
        "<pre><code>        \n        if not pairs:\n            break\n        \n        # Get most frequent pair\n        best_pair = pairs.most_common(1)[0][0]\n        \n        # Merge the pair\n        word_freqs = merge_vocab(best_pair, word_freqs)\n        \n        # Add merged token to vocabulary\n        vocab.add(''.join(best_pair))\n        \n        print(f\"Merge {i+1}: {best_pair[0]} + {best_pair[1]} -> {''.join(best_pair)}\")\n    </code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_2785a64438714",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>build_bpe_vocab_part_3()</code>",
        "<pre><code>    # Create token to ID mapping\n    vocab_list = sorted(list(vocab))\n    vocab_dict = {token: idx for idx, token in enumerate(vocab_list)}\n    \n    return vocab_dict</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_311fee21d89fd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>bpe_encode_part_1()</code>",
        "<pre><code>def bpe_encode(text: str, vocab: Dict[str, int]) -> List[int]:\n    \"\"\"Encode text using BPE vocabulary.\"\"\"\n    if not text:\n        return []\n    \n    # Start with character-level splitting\n    words = text.lower().split()\n    encoded = []\n    \n    for word in words:\n        # Convert word to character sequence with end marker\n        word_chars = list(word) + ['</w>']\n        \n        # Greedily apply merges (simplified - just use available tokens)\n        tokens = []\n        i = 0</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_500d99798ed02",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>bpe_encode_part_2()</code>",
        "<pre><code>        \n        while i < len(word_chars):\n            # Try to find longest matching token\n            found = False\n            \n            for length in range(min(len(word_chars) - i, 10), 0, -1):  # Max length 10\n                candidate = ''.join(word_chars[i:i+length])\n                \n                if candidate in vocab:\n                    tokens.append(vocab[candidate])\n                    i += length\n                    found = True\n                    break\n            \n            if not found:</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_8e48e7443bfb0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>bpe_encode_part_3()</code>",
        "<pre><code>                # Fallback to unknown token (use ID 0)\n                tokens.append(0)\n                i += 1\n        \n        encoded.extend(tokens)\n    \n    return encoded</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_e83ab12eb2b33",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>bpe_decode()</code>",
        "<pre><code>def bpe_decode(token_ids: List[int], vocab: Dict[str, int]) -> str:\n    \"\"\"Decode BPE tokens back to text.\"\"\"\n    # Create reverse vocabulary\n    id_to_token = {idx: token for token, idx in vocab.items()}\n    \n    tokens = []\n    for token_id in token_ids:\n        if token_id in id_to_token:\n            tokens.append(id_to_token[token_id])\n    \n    # Join tokens and handle end-of-word markers\n    text = ''.join(tokens)\n    text = text.replace('</w>', ' ')\n    \n    return text.strip()</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_4751c695e35a5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>simulate_wordpiece_part_1()</code>",
        "<pre><code>def simulate_wordpiece(text: str, vocab: Dict[str, int]) -> List[str]:\n    \"\"\"Simulate WordPiece tokenization (greedy longest-match).\"\"\"\n    words = text.lower().split()\n    subwords = []\n    \n    for word in words:\n        # Try to tokenize word using longest matching subwords\n        i = 0\n        word_subwords = []\n        \n        while i < len(word):\n            # Find longest matching subword\n            found = False\n            \n            for end in range(len(word), i, -1):\n                subword = word[i:end]</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_e4a9210d73a0f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>simulate_wordpiece_part_2()</code>",
        "<pre><code>                \n                # Add ## prefix for continuation (except first subword)\n                if i > 0:\n                    subword = '##' + subword\n                \n                if subword in vocab or (i == 0 and subword in vocab):\n                    word_subwords.append(subword)\n                    i = end\n                    found = True\n                    break\n            \n            if not found:\n                # Fallback: use [UNK] token\n                word_subwords.append('[UNK]')\n                i += 1</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_f4a0ca07de280",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>Implement: <code>simulate_wordpiece_part_3()</code>",
        "<pre><code>        \n        subwords.extend(word_subwords)\n    \n    return subwords</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_159655e45f9eb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Topic Modeling with LSA and LDA</b><br>What's the key approach?",
        "<b>Approach:</b> Implement topic modeling algorithms:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Topicmodeling",
        "problem_understanding"
      ],
      "guid": "nlp_110654944d3ed",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>perform_lsa()</code>",
        "<pre><code>def perform_lsa(documents: List[str], num_topics: int = 5) -> LSAModel:\n    \"\"\"Perform Latent Semantic Analysis.\"\"\"\n    model = LSAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_43db57133690b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>perform_lda()</code>",
        "<pre><code>def perform_lda(documents: List[str], num_topics: int = 5) -> LDAModel:\n    \"\"\"Perform Latent Dirichlet Allocation.\"\"\"\n    model = LDAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_767ab3ae9868c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>extract_topics()</code>",
        "<pre><code>def extract_topics(model: Union[LSAModel, LDAModel], \n                  num_words: int = 10) -> List[List[Tuple[str, float]]]:\n    \"\"\"Extract topics from model.\"\"\"\n    return model.get_topics(num_words)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_2c307da18d6c6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>get_document_topics()</code>",
        "<pre><code>def get_document_topics(model: Union[LSAModel, LDAModel], \n                       document: str) -> List[Tuple[int, float]]:\n    \"\"\"Get topic distribution for a document.\"\"\"\n    doc_topics = model.transform([document])[0]\n    \n    # Return as (topic_id, probability) pairs\n    topic_probs = []\n    for topic_idx, prob in enumerate(doc_topics):\n        if prob > 0.01:  # Threshold for relevance\n            topic_probs.append((topic_idx, prob))\n    \n    # Sort by probability\n    topic_probs.sort(key=lambda x: x[1], reverse=True)\n    return topic_probs</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_3fd9294dd784d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>calculate_coherence_score_part_1()</code>",
        "<pre><code>def calculate_coherence_score(model: Union[LSAModel, LDAModel], \n                            documents: List[str], \n                            num_words: int = 10) -> float:\n    \"\"\"Calculate topic coherence score (simplified version).\"\"\"\n    topics = model.get_topics(num_words)\n    \n    # Simple coherence: average pairwise word co-occurrence\n    coherence_scores = []\n    \n    for topic_words in topics:\n        topic_coherence = 0\n        word_list = [word for word, _ in topic_words]\n        \n        # Count co-occurrences\n        for i in range(len(word_list)):\n            for j in range(i + 1, len(word_list)):\n                word1, word2 = word_list[i], word_list[j]\n                co_occur = 0\n                occur1 = 0</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_b3e5e5460f355",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>calculate_coherence_score_part_2()</code>",
        "<pre><code>                \n                for doc in documents:\n                    doc_lower = doc.lower()\n                    if word1 in doc_lower and word2 in doc_lower:\n                        co_occur += 1\n                    if word1 in doc_lower:\n                        occur1 += 1\n                \n                if occur1 > 0:\n                    topic_coherence += co_occur / occur1\n        \n        # Average coherence for topic\n        if len(word_list) > 1:\n            topic_coherence /= (len(word_list) * (len(word_list) - 1) / 2)\n        \n        coherence_scores.append(topic_coherence)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_a2122b5c4a909",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>calculate_coherence_score_part_3()</code>",
        "<pre><code>    \n    return np.mean(coherence_scores)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_d9189a7a03113",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>compare_topic_models_part_1()</code>",
        "<pre><code>def compare_topic_models(documents: List[str], num_topics: int = 5):\n    \"\"\"Compare different topic modeling approaches.\"\"\"\n    results = {}\n    \n    # LSA\n    lsa_model = perform_lsa(documents, num_topics)\n    results['LSA'] = {\n        'topics': extract_topics(lsa_model, num_words=5),\n        'coherence': calculate_coherence_score(lsa_model, documents),\n        'explained_variance': lsa_model.svd.explained_variance_ratio_.sum()\n    }\n    \n    # LDA\n    lda_model = perform_lda(documents, num_topics)\n    results['LDA'] = {\n        'topics': extract_topics(lda_model, num_words=5),\n        'coherence': calculate_coherence_score(lda_model, documents)\n    }</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_13823bc92848e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>compare_topic_models_part_2()</code>",
        "<pre><code>    \n    # NMF (Non-negative Matrix Factorization)\n    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n    doc_term_matrix = vectorizer.fit_transform(documents)\n    \n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(doc_term_matrix)\n    \n    vocabulary = vectorizer.get_feature_names_out()\n    nmf_topics = []\n    for topic_idx in range(num_topics):\n        word_scores = nmf.components_[topic_idx]\n        top_indices = np.argsort(word_scores)[-5:][::-1]\n        topic_words = [(vocabulary[idx], word_scores[idx]) for idx in top_indices]\n        nmf_topics.append(topic_words)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_9ba7bb311c5c2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>compare_topic_models_part_3()</code>",
        "<pre><code>    \n    results['NMF'] = {\n        'topics': nmf_topics,\n        'reconstruction_error': nmf.reconstruction_err_\n    }\n    \n    return results</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_90d1a768941ec",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>__init__()</code>",
        "<pre><code>    def __init__(self, num_topics: int = 5, use_tfidf: bool = True):\n        self.num_topics = num_topics\n        self.use_tfidf = use_tfidf\n        self.vectorizer = None\n        self.svd = None\n        self.document_topic_matrix = None\n        self.topic_word_matrix = None\n        self.vocabulary = None</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_9498163651c11",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>fit_part_1()</code>",
        "<pre><code>    def fit(self, documents: List[str]):\n        \"\"\"Fit LSA model to documents.\"\"\"\n        # Vectorize documents\n        if self.use_tfidf:\n            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n        else:\n            self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n        \n        doc_term_matrix = self.vectorizer.fit_transform(documents)\n        self.vocabulary = self.vectorizer.get_feature_names_out()\n        \n        # Apply SVD\n        self.svd = TruncatedSVD(n_components=self.num_topics, random_state=42)\n        self.document_topic_matrix = self.svd.fit_transform(doc_term_matrix)\n        </code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_1f1ad23d978ce",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>fit_part_2()</code>",
        "<pre><code>        # Topic-word matrix (V^T in SVD)\n        self.topic_word_matrix = self.svd.components_\n        \n        # Normalize for interpretation\n        self.document_topic_matrix = normalize(self.document_topic_matrix, axis=1)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_4c48468db9780",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>get_topics_part_1()</code>",
        "<pre><code>    def get_topics(self, num_words: int = 10) -> List[List[Tuple[str, float]]]:\n        \"\"\"Extract top words for each topic.\"\"\"\n        topics = []\n        \n        for topic_idx in range(self.num_topics):\n            # Get word scores for this topic\n            word_scores = self.topic_word_matrix[topic_idx]\n            \n            # Get top word indices\n            top_indices = np.argsort(word_scores)[-num_words:][::-1]\n            \n            # Create (word, score) pairs\n            topic_words = [(self.vocabulary[idx], word_scores[idx]) \n                          for idx in top_indices]\n            topics.append(topic_words)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_49e3086e74859",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>get_topics_part_2()</code>",
        "<pre><code>        \n        return topics</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_ebf058d943e2e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>transform()</code>",
        "<pre><code>    def transform(self, documents: List[str]) -> np.ndarray:\n        \"\"\"Transform documents to topic space.\"\"\"\n        doc_term_matrix = self.vectorizer.transform(documents)\n        return self.svd.transform(doc_term_matrix)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_2e0b14bebcce7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>__init__()</code>",
        "<pre><code>    def __init__(self, num_topics: int = 5, alpha: float = 0.1, beta: float = 0.01):\n        self.num_topics = num_topics\n        self.alpha = alpha  # Document-topic prior\n        self.beta = beta    # Topic-word prior\n        self.vectorizer = None\n        self.lda = None\n        self.vocabulary = None</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_47b8ac4a8253d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>fit()</code>",
        "<pre><code>    def fit(self, documents: List[str], method: str = 'sklearn'):\n        \"\"\"Fit LDA model to documents.\"\"\"\n        if method == 'sklearn':\n            self._fit_sklearn(documents)\n        else:\n            self._fit_from_scratch(documents)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_3da3beda0be1e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_sklearn_part_1()</code>",
        "<pre><code>    def _fit_sklearn(self, documents: List[str]):\n        \"\"\"Fit using sklearn's LDA.\"\"\"\n        # Use count vectorizer for LDA\n        self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n        doc_term_matrix = self.vectorizer.fit_transform(documents)\n        self.vocabulary = self.vectorizer.get_feature_names_out()\n        \n        # Fit LDA\n        self.lda = LatentDirichletAllocation(\n            n_components=self.num_topics,\n            doc_topic_prior=self.alpha,\n            topic_word_prior=self.beta,\n            random_state=42,\n            max_iter=10\n        )</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_0d0bb22ed5983",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_sklearn_part_2()</code>",
        "<pre><code>        \n        self.lda.fit(doc_term_matrix)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_90d2191e60472",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_from_scratch_part_1()</code>",
        "<pre><code>    def _fit_from_scratch(self, documents: List[str]):\n        \"\"\"Simplified LDA using Gibbs sampling.\"\"\"\n        # Tokenize documents\n        tokenized_docs = []\n        word_to_id = {}\n        id_to_word = {}\n        word_id = 0\n        \n        for doc in documents:\n            tokens = doc.lower().split()\n            doc_tokens = []\n            for token in tokens:\n                if token not in word_to_id:\n                    word_to_id[token] = word_id\n                    id_to_word[word_id] = token\n                    word_id += 1\n                doc_tokens.append(word_to_id[token])\n            tokenized_docs.append(doc_tokens)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_fb61a51682d3f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_from_scratch_part_2()</code>",
        "<pre><code>        \n        self.vocabulary = list(word_to_id.keys())\n        vocab_size = len(self.vocabulary)\n        \n        # Initialize topic assignments randomly\n        doc_topic_counts = np.zeros((len(documents), self.num_topics))\n        topic_word_counts = np.zeros((self.num_topics, vocab_size))\n        topic_counts = np.zeros(self.num_topics)\n        \n        # Topic assignments for each word\n        topic_assignments = []\n        \n        for doc_idx, doc_tokens in enumerate(tokenized_docs):\n            doc_topics = []\n            for token in doc_tokens:</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_516277e4d20f8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_from_scratch_part_3()</code>",
        "<pre><code>                # Random initial assignment\n                topic = np.random.randint(self.num_topics)\n                doc_topics.append(topic)\n                \n                # Update counts\n                doc_topic_counts[doc_idx, topic] += 1\n                topic_word_counts[topic, token] += 1\n                topic_counts[topic] += 1\n            \n            topic_assignments.append(doc_topics)\n        \n        # Gibbs sampling (simplified - just a few iterations)\n        for iteration in range(50):\n            for doc_idx, doc_tokens in enumerate(tokenized_docs):\n                for word_idx, word_id in enumerate(doc_tokens):</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_1441c609aae0b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_from_scratch_part_4()</code>",
        "<pre><code>                    # Current topic\n                    old_topic = topic_assignments[doc_idx][word_idx]\n                    \n                    # Remove from counts\n                    doc_topic_counts[doc_idx, old_topic] -= 1\n                    topic_word_counts[old_topic, word_id] -= 1\n                    topic_counts[old_topic] -= 1\n                    \n                    # Calculate probabilities for each topic\n                    probs = np.zeros(self.num_topics)\n                    for topic in range(self.num_topics):\n                        # P(topic|doc) * P(word|topic)\n                        doc_topic_prob = (doc_topic_counts[doc_idx, topic] + self.alpha) / \\\n                                       (len(doc_tokens) - 1 + self.num_topics * self.alpha)\n                        \n                        topic_word_prob = (topic_word_counts[topic, word_id] + self.beta) / \\\n                                        (topic_counts[topic] + vocab_size * self.beta)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_c40619919c1cd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_from_scratch_part_5()</code>",
        "<pre><code>                        \n                        probs[topic] = doc_topic_prob * topic_word_prob\n                    \n                    # Sample new topic\n                    probs /= probs.sum()\n                    new_topic = np.random.choice(self.num_topics, p=probs)\n                    \n                    # Update assignments and counts\n                    topic_assignments[doc_idx][word_idx] = new_topic\n                    doc_topic_counts[doc_idx, new_topic] += 1\n                    topic_word_counts[new_topic, word_id] += 1\n                    topic_counts[new_topic] += 1\n        \n        # Store final distributions\n        self.doc_topic_dist = doc_topic_counts + self.alpha\n        self.doc_topic_dist /= self.doc_topic_dist.sum(axis=1, keepdims=True)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_3e7f43e6b46ef",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>_fit_from_scratch_part_6()</code>",
        "<pre><code>        \n        self.topic_word_dist = topic_word_counts + self.beta\n        self.topic_word_dist /= self.topic_word_dist.sum(axis=1, keepdims=True)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_27520c64c5885",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>get_topics_part_1()</code>",
        "<pre><code>    def get_topics(self, num_words: int = 10) -> List[List[Tuple[str, float]]]:\n        \"\"\"Extract top words for each topic.\"\"\"\n        topics = []\n        \n        if hasattr(self, 'lda') and self.lda is not None:\n            # sklearn LDA\n            for topic_idx in range(self.num_topics):\n                word_scores = self.lda.components_[topic_idx]\n                top_indices = np.argsort(word_scores)[-num_words:][::-1]\n                \n                topic_words = [(self.vocabulary[idx], word_scores[idx]) \n                              for idx in top_indices]\n                topics.append(topic_words)\n        else:\n            # From scratch implementation\n            for topic_idx in range(self.num_topics):\n                word_scores = self.topic_word_dist[topic_idx]\n                top_indices = np.argsort(word_scores)[-num_words:][::-1]</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_65a4ac11b8c7e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>get_topics_part_2()</code>",
        "<pre><code>                \n                topic_words = [(self.vocabulary[idx], word_scores[idx]) \n                              for idx in top_indices]\n                topics.append(topic_words)\n        \n        return topics</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_ebc3f1dfd1494",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Implement: <code>transform()</code>",
        "<pre><code>    def transform(self, documents: List[str]) -> np.ndarray:\n        \"\"\"Transform documents to topic space.\"\"\"\n        if hasattr(self, 'lda') and self.lda is not None:\n            doc_term_matrix = self.vectorizer.transform(documents)\n            return self.lda.transform(doc_term_matrix)\n        else:\n            # Simplified: return uniform distribution\n            return np.ones((len(documents), self.num_topics)) / self.num_topics</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_476a70319ad85",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Key concept:",
        "<p>Compare different topic modeling approaches.</p>",
        "Topicmodeling",
        "concepts"
      ],
      "guid": "nlp_5a30e433cb5ae",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: BERT Fine-tuning for Sentiment Analysis</b><br>What's the key approach?",
        "<b>Approach:</b> Implement BERT fine-tuning for sentiment classification:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Transformers",
        "problem_understanding"
      ],
      "guid": "nlp_c1605b7141321",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>load_pretrained_bert_part_1()</code>",
        "<pre><code>def load_pretrained_bert(model_name: str = 'bert-base-uncased', \n                        task: str = 'custom') -> Tuple[Union[nn.Module, any], any]:\n    \"\"\"Load pre-trained BERT model and tokenizer.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return None, None\n    \n    if task == 'custom':\n        # Custom model with our classification head\n        model = BERTSentimentClassifier(model_name)\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n    else:\n        # Hugging Face model for sequence classification\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_name, num_labels=2\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_bae4f8a7b0eec",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>load_pretrained_bert_part_2()</code>",
        "<pre><code>    \n    return model, tokenizer</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_d1efe09f3cb13",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>fine_tune_bert_part_1()</code>",
        "<pre><code>def fine_tune_bert(model: nn.Module, texts: List[str], labels: List[int],\n                  tokenizer, epochs: int = 3, batch_size: int = 16,\n                  learning_rate: float = 2e-5, warmup_steps: int = 0) -> nn.Module:\n    \"\"\"Fine-tune BERT for sentiment analysis.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return model\n    \n    # Create dataset and dataloader\n    dataset = SentimentDataset(texts, labels, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Setup optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n    \n    total_steps = len(dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_5178398aaa364",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>fine_tune_bert_part_2()</code>",
        "<pre><code>    \n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # Training loop\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch in dataloader:</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_37fe8b9bca674",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>fine_tune_bert_part_3()</code>",
        "<pre><code>            # Move to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            \n            if isinstance(model, BERTSentimentClassifier):\n                logits, _, _ = model(input_ids, attention_mask)\n            else:\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                logits = outputs.logits\n                loss = outputs.loss\n            \n            if isinstance(model, BERTSentimentClassifier):\n                loss = criterion(logits, labels)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_57ca40fe5c4f8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>fine_tune_bert_part_4()</code>",
        "<pre><code>            \n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            \n            # Track metrics\n            total_loss += loss.item()\n            _, predicted = torch.max(logits, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n        \n        avg_loss = total_loss / len(dataloader)\n        accuracy = correct / total</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_74a878f4c3538",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>fine_tune_bert_part_5()</code>",
        "<pre><code>        \n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"Average Loss: {avg_loss:.4f}\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n    \n    return model</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_934a5b9593319",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>predict_with_bert_part_1()</code>",
        "<pre><code>def predict_with_bert(model: nn.Module, texts: List[str], tokenizer,\n                     return_attention: bool = True) -> List[Tuple[str, float, Dict]]:\n    \"\"\"Make predictions with BERT model.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return []\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n    \n    predictions = []\n    \n    with torch.no_grad():\n        for text in texts:\n            # Tokenize\n            encoding = tokenizer(\n                text,\n                truncation=True,\n                padding='max_length',\n                max_length=128,\n                return_tensors='pt'\n            )</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_b7dd04a7d3481",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>predict_with_bert_part_2()</code>",
        "<pre><code>            \n            input_ids = encoding['input_ids'].to(device)\n            attention_mask = encoding['attention_mask'].to(device)\n            \n            # Get predictions\n            if isinstance(model, BERTSentimentClassifier):\n                logits, attentions, cls_embedding = model(input_ids, attention_mask)\n            else:\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs.logits\n                attentions = outputs.attentions if hasattr(outputs, 'attentions') else None\n                cls_embedding = outputs.hidden_states[-1][:, 0, :] if hasattr(outputs, 'hidden_states') else None\n            \n            # Get probabilities\n            probs = torch.softmax(logits, dim=-1)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_df1cc507fd38e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>predict_with_bert_part_3()</code>",
        "<pre><code>            predicted_class = torch.argmax(probs, dim=-1).item()\n            confidence = probs[0, predicted_class].item()\n            \n            # Sentiment label\n            sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n            \n            # Additional information\n            extra_info = {}\n            \n            if return_attention and attentions is not None:\n                # Average attention across all layers and heads\n                avg_attention = torch.stack(attentions).mean(dim=(0, 1))[0]\n                extra_info['attention_scores'] = avg_attention.cpu().numpy().tolist()\n            \n            if cls_embedding is not None:\n                extra_info['cls_embedding'] = cls_embedding[0].cpu().numpy().tolist()[:10]  # First 10 dims</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_069165a7776e6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>predict_with_bert_part_4()</code>",
        "<pre><code>            \n            predictions.append((sentiment, confidence, extra_info))\n    \n    return predictions</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_6e7a2eda241a5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>compare_transformer_models_part_1()</code>",
        "<pre><code>def compare_transformer_models(texts: List[str], labels: List[int],\n                             models: List[str] = None) -> Dict[str, Dict]:\n    \"\"\"Compare different transformer models.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return {}\n    \n    if models is None:\n        models = ['bert-base-uncased', 'distilbert-base-uncased', 'roberta-base']\n    \n    results = {}\n    \n    for model_name in models:\n        print(f\"\\nEvaluating {model_name}...\")\n        \n        try:</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_c4e5d197de3cb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>compare_transformer_models_part_2()</code>",
        "<pre><code>            # Load model and tokenizer\n            if 'bert' in model_name and 'distil' not in model_name:\n                tokenizer = BertTokenizer.from_pretrained(model_name)\n                model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n            elif 'distilbert' in model_name:\n                tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n                model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n            elif 'roberta' in model_name:\n                tokenizer = RobertaTokenizer.from_pretrained(model_name)\n                model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n            \n            # Count parameters\n            total_params = sum(p.numel() for p in model.parameters())\n            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n            \n            results[model_name] = {\n                'total_parameters': total_params,\n                'trainable_parameters': trainable_params,\n                'model_size_mb': total_params * 4 / 1024 / 1024  # Assuming float32\n            }</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_7a551e7d25ae9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>compare_transformer_models_part_3()</code>",
        "<pre><code>            \n            print(f\"Total parameters: {total_params:,}\")\n            print(f\"Model size: {results[model_name]['model_size_mb']:.2f} MB\")\n            \n        except Exception as e:\n            print(f\"Error loading {model_name}: {e}\")\n            results[model_name] = {'error': str(e)}\n    \n    return results</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_8f87a0ac6ff2d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>few_shot_sentiment_part_1()</code>",
        "<pre><code>def few_shot_sentiment(model, tokenizer, examples: List[Tuple[str, str]], \n                      query: str) -> Tuple[str, float]:\n    \"\"\"Few-shot learning with prompts (for GPT-style models).\"\"\"\n    # Create prompt\n    prompt = \"Classify the sentiment of the following texts:\\n\\n\"\n    \n    for text, label in examples:\n        prompt += f\"Text: {text}\\nSentiment: {label}\\n\\n\"\n    \n    prompt += f\"Text: {query}\\nSentiment:\"\n    \n    # This is a simplified version - in practice, you'd use a generative model\n    # For BERT, we'll just use the fine-tuned classifier\n    predictions = predict_with_bert(model, [query], tokenizer, return_attention=False)\n    \n    if predictions:\n        sentiment, confidence, _ = predictions[0]\n        return sentiment, confidence</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_10e55a4e5ac48",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>few_shot_sentiment_part_2()</code>",
        "<pre><code>    \n    return \"unknown\", 0.0</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_de41f53b6f773",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>visualize_attention()</code>",
        "<pre><code>def visualize_attention(text: str, tokens: List[str], attention_scores: List[float]):\n    \"\"\"Simple text-based attention visualization.\"\"\"\n    print(f\"\\nText: '{text}'\")\n    print(\"\\nToken attention scores:\")\n    \n    max_score = max(attention_scores) if attention_scores else 1\n    \n    for token, score in zip(tokens, attention_scores):\n        # Normalize to 0-20 scale for visualization\n        bar_length = int((score / max_score) * 20)\n        bar = '█' * bar_length\n        print(f\"{token:15} {bar} {score:.3f}\")</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_582524c60e639",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>create_imdb_sample_data_part_1()</code>",
        "<pre><code>def create_imdb_sample_data() -> Tuple[List[str], List[int], List[str], List[int]]:\n    \"\"\"Create sample IMDB-style movie review data.\"\"\"\n    train_texts = [\n        \"This movie is a masterpiece. The acting is superb and the story is captivating.\",\n        \"Terrible film. Waste of time and money. Poor acting and boring plot.\",\n        \"Absolutely loved it! One of the best movies I've seen this year.\",\n        \"Disappointing. Had high expectations but the movie failed to deliver.\",\n        \"Brilliant cinematography and outstanding performances by all actors.\",\n        \"Couldn't even finish watching it. Extremely boring and predictable.\",\n        \"A true work of art. Every scene is beautifully crafted.\",\n        \"Not worth the hype. Mediocre at best.\",\n        \"Exceptional movie that will be remembered for years to come.\",\n        \"One of the worst movies ever made. Avoid at all costs.\"\n    ]\n    \n    train_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_0c35167b25a72",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>create_imdb_sample_data_part_2()</code>",
        "<pre><code>    \n    test_texts = [\n        \"Amazing film with incredible performances!\",\n        \"Boring and poorly executed. Not recommended.\",\n        \"Decent movie but nothing groundbreaking.\"\n    ]\n    \n    test_labels = [1, 0, 1]\n    \n    return train_texts, train_labels, test_texts, test_labels</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_dc07fa388701b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>__init__()</code>",
        "<pre><code>    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_9b61c8f32cf6d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>__len__()</code>",
        "<pre><code>    def __len__(self):\n        return len(self.texts)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_3348343a2fef4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>__getitem___part_1()</code>",
        "<pre><code>    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_acb762fe30779",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>__init___part_1()</code>",
        "<pre><code>    def __init__(self, model_name: str = 'bert-base-uncased', num_classes: int = 2, \n                 dropout: float = 0.3, freeze_bert: bool = False):\n        super().__init__()\n        \n        # Load pre-trained BERT\n        self.bert = BertModel.from_pretrained(model_name, output_attentions=True)\n        \n        # Freeze BERT parameters if specified\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n        # Classification head\n        hidden_size = self.bert.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, num_classes)\n        )</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_e2b5ee42ba5f6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Implement: <code>forward()</code>",
        "<pre><code>    def forward(self, input_ids, attention_mask):\n        # Get BERT outputs\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Use [CLS] token representation\n        pooled_output = outputs.pooler_output\n        \n        # Classification\n        logits = self.classifier(pooled_output)\n        \n        return logits, outputs.attentions, pooled_output</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_3b95fab89d2be",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Transformers replaced RNNs with parallel attention computation</strong></p>",
        "Transformers",
        "interview_insights"
      ],
      "guid": "nlp_1a580358532a2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Multi-head attention captures different types of relationships</strong></p>",
        "Transformers",
        "interview_insights"
      ],
      "guid": "nlp_b8361116a3656",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Positional encoding adds sequence order information</strong></p>",
        "Transformers",
        "interview_insights"
      ],
      "guid": "nlp_84956689c12c1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Problem: Problem: Comprehensive Text Normalization Pipeline</b><br>What's the key approach?",
        "<b>Approach:</b> Build a complete text normalization system:<br><br><i>Think about: What algorithm/data structure fits this problem?</i>",
        "Utilities",
        "problem_understanding"
      ],
      "guid": "nlp_c06f7abe4e7c4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>expand_contractions_part_1()</code>",
        "<pre><code>def expand_contractions(text: str) -> str:\n    \"\"\"Expand contractions in text.\"\"\"\n    # Convert to lowercase for matching\n    text_lower = text.lower()\n    \n    # Sort contractions by length (descending) to match longer ones first\n    sorted_contractions = sorted(CONTRACTIONS.items(), key=lambda x: len(x[0]), reverse=True)\n    \n    for contraction, expansion in sorted_contractions:\n        # Use word boundaries for accurate matching\n        pattern = r'\\b' + re.escape(contraction) + r'\\b'\n        text_lower = re.sub(pattern, expansion, text_lower, flags=re.IGNORECASE)\n    \n    # Preserve original capitalization pattern\n    result = []\n    for i, char in enumerate(text):\n        if i < len(text_lower):\n            if char.isupper():\n                result.append(text_lower[i].upper())\n            else:\n                result.append(text_lower[i])\n        else:\n            result.append(char)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_0daae9c299f58",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>expand_contractions_part_2()</code>",
        "<pre><code>    \n    return ''.join(result)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_e19ed4e995e6b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>expand_abbreviations()</code>",
        "<pre><code>def expand_abbreviations(text: str) -> str:\n    \"\"\"Expand common abbreviations.\"\"\"\n    for abbr, expansion in ABBREVIATIONS.items():\n        # Handle period after abbreviation\n        pattern1 = r'\\b' + re.escape(abbr) + r'\\.\\b'\n        pattern2 = r'\\b' + re.escape(abbr) + r'\\b'\n        \n        text = re.sub(pattern1, expansion, text, flags=re.IGNORECASE)\n        text = re.sub(pattern2, expansion, text, flags=re.IGNORECASE)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_4d2ff735b022d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>clean_html_part_1()</code>",
        "<pre><code>def clean_html(html_text: str) -> str:\n    \"\"\"Remove HTML tags and decode entities.\"\"\"\n    # Remove script and style content\n    html_text = re.sub(r'<script[^>]*>.*?</script>', '', html_text, flags=re.DOTALL | re.IGNORECASE)\n    html_text = re.sub(r'<style[^>]*>.*?</style>', '', html_text, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Remove HTML tags\n    html_text = re.sub(r'<[^>]+>', ' ', html_text)\n    \n    # Decode HTML entities\n    text = html.unescape(html_text)\n    \n    # Clean up whitespace\n    text = ' '.join(text.split())\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_d7986aacf5892",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_unicode_part_1()</code>",
        "<pre><code>def normalize_unicode(text: str) -> str:\n    \"\"\"Normalize Unicode characters.\"\"\"\n    # Normalize to NFKD form\n    text = unicodedata.normalize('NFKD', text)\n    \n    # Replace special quotes and dashes\n    replacements = {\n        '\"': '\"',\n        '\"': '\"',\n        ''': \"'\",\n        ''': \"'\",\n        '–': '-',\n        '—': '-',\n        '…': '...',\n        '•': '*',\n        '®': '(R)',\n        '™': '(TM)',\n        '©': '(C)',\n        '°': ' degrees',\n        '£': 'GBP',\n        '€': 'EUR',\n        '¥': 'JPY',\n        '₹': 'INR'\n    }</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_09035dcfe6d9f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_unicode_part_2()</code>",
        "<pre><code>    \n    for old, new in replacements.items():\n        text = text.replace(old, new)\n    \n    # Remove non-ASCII characters that can't be handled\n    # text = ''.join(char if ord(char) < 128 else ' ' for char in text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_1028068d2a880",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_whitespace()</code>",
        "<pre><code>def normalize_whitespace(text: str) -> str:\n    \"\"\"Normalize whitespace in text.\"\"\"\n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove space before punctuation\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n    \n    # Add space after punctuation if missing\n    text = re.sub(r'([.,!?;:])([A-Za-z])', r'\\1 \\2', text)\n    \n    # Remove leading/trailing whitespace\n    text = text.strip()\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_3fca32e91f629",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_punctuation_part_1()</code>",
        "<pre><code>def normalize_punctuation(text: str) -> str:\n    \"\"\"Normalize punctuation in text.\"\"\"\n    # Replace multiple punctuation with single\n    text = re.sub(r'\\.{2,}', '.', text)  # Multiple dots to single\n    text = re.sub(r'!{2,}', '!', text)   # Multiple exclamations\n    text = re.sub(r'\\?{2,}', '?', text)  # Multiple questions\n    \n    # Normalize ellipsis\n    text = re.sub(r'\\.{3,}', '...', text)\n    \n    # Fix common punctuation errors\n    text = re.sub(r'\\s*,\\s*', ', ', text)  # Space after comma\n    text = re.sub(r'\\s*\\.\\s*', '. ', text)  # Space after period\n    text = re.sub(r'\\s*!\\s*', '! ', text)   # Space after exclamation\n    text = re.sub(r'\\s*\\?\\s*', '? ', text)  # Space after question</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_dddb08ee5d94a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_punctuation_part_2()</code>",
        "<pre><code>    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_f53e54b73fdde",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_urls_part_1()</code>",
        "<pre><code>def handle_urls(text: str, keep_domain: bool = True) -> str:\n    \"\"\"Handle URLs in text.\"\"\"\n    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&/=]*)'\n    \n    if keep_domain:\n        def replace_url(match):\n            url = match.group(0)\n            try:\n                parsed = urlparse(url)\n                domain = parsed.netloc.replace('www.', '')\n                return domain\n            except:\n                return 'URL'\n    else:\n        def replace_url(match):\n            return 'URL'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_2014ce98f26f9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_urls_part_2()</code>",
        "<pre><code>    \n    text = re.sub(url_pattern, replace_url, text)\n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_68279c62ad140",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_emails_part_1()</code>",
        "<pre><code>def handle_emails(text: str, anonymize: bool = True) -> str:\n    \"\"\"Handle email addresses in text.\"\"\"\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    \n    if anonymize:\n        text = re.sub(email_pattern, 'EMAIL', text)\n    else:\n        # Keep domain only\n        def replace_email(match):\n            email = match.group(0)\n            domain = email.split('@')[1]\n            return f'user@{domain}'\n        \n        text = re.sub(email_pattern, replace_email, text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_1afa8cdd5273b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_phone_numbers()</code>",
        "<pre><code>def handle_phone_numbers(text: str) -> str:\n    \"\"\"Handle phone numbers in text.\"\"\"\n    # US phone numbers\n    phone_patterns = [\n        r'\\b\\+?1?\\s*\\(?([0-9]{3})\\)?[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})\\b',\n        r'\\b([0-9]{3})[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})\\b',\n    ]\n    \n    for pattern in phone_patterns:\n        text = re.sub(pattern, 'PHONE', text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_1735bd06e1c74",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_numbers_part_1()</code>",
        "<pre><code>def handle_numbers(text: str, spell_out: bool = False) -> str:\n    \"\"\"Handle numbers in text.\"\"\"\n    if spell_out:\n        # Simple number to word conversion for small numbers\n        num_words = {\n            '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n            '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine',\n            '10': 'ten', '11': 'eleven', '12': 'twelve'\n        }\n        \n        for num, word in num_words.items():\n            text = re.sub(r'\\b' + num + r'\\b', word, text)\n    \n    # Normalize number formats\n    text = re.sub(r'\\b(\\d+),(\\d+)\\b', r'\\1\\2', text)  # Remove commas from numbers</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_40df1d2226d8e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_numbers_part_2()</code>",
        "<pre><code>    \n    # Handle percentages\n    text = re.sub(r'(\\d+)\\s*%', r'\\1 percent', text)\n    \n    # Handle currency\n    text = re.sub(r'\\$\\s*(\\d+)', r'\\1 dollars', text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_47cfdd4761e12",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_social_media_part_1()</code>",
        "<pre><code>def handle_social_media(text: str, preserve_mentions: bool = True, preserve_hashtags: bool = True) -> str:\n    \"\"\"Handle social media specific elements.\"\"\"\n    if not preserve_mentions:\n        # Remove @mentions\n        text = re.sub(r'@\\w+', '', text)\n    else:\n        # Normalize mentions\n        text = re.sub(r'@(\\w+)', r'USER_\\1', text)\n    \n    if not preserve_hashtags:\n        # Remove hashtags\n        text = re.sub(r'#\\w+', '', text)\n    else:\n        # Convert hashtags to normal words\n        text = re.sub(r'#(\\w+)', r'\\1', text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_ab4e5cd435a9b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>handle_social_media_part_2()</code>",
        "<pre><code>    \n    # Handle retweet notation\n    text = re.sub(r'\\bRT\\s+:', '', text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_c716c37dc99f1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>remove_emojis()</code>",
        "<pre><code>def remove_emojis(text: str) -> str:\n    \"\"\"Remove emoji characters from text.\"\"\"\n    emoji_pattern = re.compile(\n        \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        \"]+\", flags=re.UNICODE\n    )\n    \n    return emoji_pattern.sub('', text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_4b9b63573bc6e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_text_part_1()</code>",
        "<pre><code>def normalize_text(text: str, options: Optional[Dict[str, bool]] = None) -> str:\n    \"\"\"Main text normalization function with configurable options.\"\"\"\n    if options is None:\n        options = {\n            'lowercase': False,\n            'expand_contractions': True,\n            'expand_abbreviations': True,\n            'handle_urls': True,\n            'handle_emails': True,\n            'handle_phones': True,\n            'handle_numbers': True,\n            'remove_emojis': True,\n            'normalize_unicode': True,\n            'normalize_whitespace': True,\n            'normalize_punctuation': True,\n            'handle_social_media': True\n        }</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_487f4b4bd9f83",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_text_part_2()</code>",
        "<pre><code>    \n    # Apply normalization steps in order\n    if options.get('normalize_unicode', True):\n        text = normalize_unicode(text)\n    \n    if options.get('expand_contractions', True):\n        text = expand_contractions(text)\n    \n    if options.get('expand_abbreviations', True):\n        text = expand_abbreviations(text)\n    \n    if options.get('handle_urls', True):\n        text = handle_urls(text)\n    \n    if options.get('handle_emails', True):\n        text = handle_emails(text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_8f4026d1a51e2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_text_part_3()</code>",
        "<pre><code>    \n    if options.get('handle_phones', True):\n        text = handle_phone_numbers(text)\n    \n    if options.get('handle_numbers', True):\n        text = handle_numbers(text)\n    \n    if options.get('remove_emojis', True):\n        text = remove_emojis(text)\n    \n    if options.get('handle_social_media', True):\n        text = handle_social_media(text)\n    \n    if options.get('normalize_punctuation', True):\n        text = normalize_punctuation(text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_3f3f9baf02084",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>normalize_text_part_4()</code>",
        "<pre><code>    \n    if options.get('normalize_whitespace', True):\n        text = normalize_whitespace(text)\n    \n    if options.get('lowercase', False):\n        text = text.lower()\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_29ef213d35e0b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>batch_normalize()</code>",
        "<pre><code>def batch_normalize(texts: List[str], options: Optional[Dict[str, bool]] = None) -> List[str]:\n    \"\"\"Normalize multiple texts with the same options.\"\"\"\n    return [normalize_text(text, options) for text in texts]</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_49abeecba72ab",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>create_custom_normalizer_part_1()</code>",
        "<pre><code>def create_custom_normalizer(preserve_entities: List[str]) -> callable:\n    \"\"\"Create a custom normalizer that preserves specific entities.\"\"\"\n    def custom_normalize(text: str) -> str:\n        # Store entities and their positions\n        preserved = []\n        \n        for entity in preserve_entities:\n            pattern = re.escape(entity)\n            for match in re.finditer(pattern, text, re.IGNORECASE):\n                preserved.append((match.start(), match.end(), match.group()))\n        \n        # Sort by position (reverse order for replacement)\n        preserved.sort(reverse=True)\n        \n        # Replace with placeholders\n        for i, (start, end, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text[:start] + placeholder + text[end:]</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_33056192004b2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>create_custom_normalizer_part_2()</code>",
        "<pre><code>        \n        # Normalize\n        text = normalize_text(text)\n        \n        # Restore entities\n        for i, (_, _, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text.replace(placeholder, entity)\n        \n        return text\n    \n    return custom_normalize</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_23c311b4f6b01",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>get_normalization_stats()</code>",
        "<pre><code>def get_normalization_stats(original: str, normalized: str) -> Dict[str, int]:\n    \"\"\"Get statistics about the normalization process.\"\"\"\n    return {\n        'original_length': len(original),\n        'normalized_length': len(normalized),\n        'characters_removed': len(original) - len(normalized),\n        'original_words': len(original.split()),\n        'normalized_words': len(normalized.split()),\n        'reduction_percentage': round((1 - len(normalized) / len(original)) * 100, 2)\n    }</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_5631e522ce274",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>custom_normalize_part_1()</code>",
        "<pre><code>    def custom_normalize(text: str) -> str:\n        # Store entities and their positions\n        preserved = []\n        \n        for entity in preserve_entities:\n            pattern = re.escape(entity)\n            for match in re.finditer(pattern, text, re.IGNORECASE):\n                preserved.append((match.start(), match.end(), match.group()))\n        \n        # Sort by position (reverse order for replacement)\n        preserved.sort(reverse=True)\n        \n        # Replace with placeholders\n        for i, (start, end, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text[:start] + placeholder + text[end:]</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_d2034b8a61713",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>custom_normalize_part_2()</code>",
        "<pre><code>        \n        # Normalize\n        text = normalize_text(text)\n        \n        # Restore entities\n        for i, (_, _, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text.replace(placeholder, entity)\n        \n        return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_00061fd02180a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>replace_url()</code>",
        "<pre><code>        def replace_url(match):\n            url = match.group(0)\n            try:\n                parsed = urlparse(url)\n                domain = parsed.netloc.replace('www.', '')\n                return domain\n            except:\n                return 'URL'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_29d7724541c23",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>replace_url()</code>",
        "<pre><code>        def replace_url(match):\n            return 'URL'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_a09dea4d4dab7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>Implement: <code>replace_email()</code>",
        "<pre><code>        def replace_email(match):\n            email = match.group(0)\n            domain = email.split('@')[1]\n            return f'user@{domain}'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_2387f16e62849",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    }
  ]
}