{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-interview-deck-rules-optimized-2024",
  "deck_config_uuid": "nlp-deck-config-rules-optimized",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-deck-config-rules-optimized",
      "dyn": false,
      "name": "NLP Interview Prep (Rules Optimized)",
      "new": {
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 25
      },
      "rev": {
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "perDay": 100
      }
    }
  ],
  "desc": "Rules-optimized NLP interview cards following research-backed design principles for maximum memorization effectiveness and rapid recall under interview pressure.",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "NLP Interview Prep (Rules Optimized)",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-model-rules-optimized",
      "css": ".card {\n    font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n    font-size: 18px;\n    line-height: 1.6;\n    color: #2c3e50;\n    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n    text-align: left;\n    padding: 20px;\n    border-radius: 12px;\n    box-shadow: 0 4px 20px rgba(0,0,0,0.1);\n    max-width: 100%;\n    margin: 0 auto;\n}\n\n/* Mobile-first responsive design */\n@media (max-width: 480px) {\n    .card {\n        font-size: 16px;\n        padding: 15px;\n        margin: 10px;\n    }\n}\n\n/* Typography hierarchy */\nh1, h2, h3 {\n    color: #34495e;\n    margin-top: 0;\n    font-weight: 600;\n}\n\nh2 {\n    font-size: 20px;\n    color: #3498db;\n    border-bottom: 2px solid #3498db;\n    padding-bottom: 5px;\n}\n\nh3 {\n    font-size: 18px;\n    color: #e74c3c;\n}\n\n/* Color coding system */\n.concept { color: #3498db; font-weight: 600; }\n.implementation { color: #27ae60; }\n.formula { color: #f39c12; font-weight: 700; }\n.edge-case { color: #e74c3c; }\n.interview-tip { color: #9b59b6; font-style: italic; }\n\n/* Code styling for mobile */\npre, code {\n    font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;\n    font-size: 14px;\n    background: #2c3e50;\n    color: #ecf0f1;\n    padding: 12px;\n    border-radius: 8px;\n    overflow-x: auto;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    line-height: 1.4;\n    margin: 10px 0;\n}\n\n@media (max-width: 480px) {\n    pre, code {\n        font-size: 13px;\n        padding: 8px;\n    }\n}\n\n/* Inline code */\ncode {\n    display: inline;\n    padding: 2px 6px;\n    background: #34495e;\n    border-radius: 4px;\n}\n\n/* Metadata styling */\n.metadata {\n    margin-top: 20px;\n    padding-top: 15px;\n    border-top: 1px solid #bdc3c7;\n    font-size: 12px;\n    color: #7f8c8d;\n    text-align: center;\n}\n\n.topic {\n    background: #3498db;\n    color: white;\n    padding: 4px 8px;\n    border-radius: 4px;\n    font-weight: 600;\n}\n\n.type {\n    background: #95a5a6;\n    color: white;\n    padding: 4px 8px;\n    border-radius: 4px;\n    margin-left: 8px;\n}\n\n/* Visual emphasis */\nstrong, b {\n    color: #2c3e50;\n    font-weight: 700;\n}\n\nem, i {\n    color: #7f8c8d;\n    font-style: italic;\n}\n\n/* Interactive elements */\ndetails {\n    margin: 10px 0;\n    padding: 10px;\n    background: rgba(52, 152, 219, 0.1);\n    border-radius: 6px;\n    border-left: 4px solid #3498db;\n}\n\nsummary {\n    cursor: pointer;\n    font-weight: 600;\n    color: #3498db;\n    outline: none;\n}\n\n/* Lists */\nul, ol {\n    padding-left: 20px;\n}\n\nli {\n    margin-bottom: 8px;\n}\n\n/* Focus on readability */\np {\n    margin-bottom: 15px;\n}\n\n/* Answer separator */\nhr#answer {\n    border: none;\n    height: 2px;\n    background: linear-gradient(90deg, #3498db, #9b59b6);\n    margin: 20px 0;\n    border-radius: 1px;\n}",
      "flds": [
        {
          "name": "Front",
          "ord": 0,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Back",
          "ord": 1,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Topic",
          "ord": 2,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 16
        },
        {
          "name": "Type",
          "ord": 3,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 14
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\\special{papersize=3in,5in}\\usepackage{amssymb,amsmath}\\pagestyle{empty}\\setlength{\\parindent}{0in}\\begin{document}",
      "name": "NLP Rules Optimized",
      "req": [
        [
          0,
          "all",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}<hr id=answer>{{Back}}<br><br><div class='metadata'><span class='topic'>{{Topic}}</span> • <span class='type'>{{Type}}</span></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "{{Front}}"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement self-attention from scratch?",
        "<b>Task:</b> Create causal mask to prevent attending to future tokens.<br><br><b>Key Formula/Concept:</b> def self_attention(X: np.ndarray, d_k: int) -> np.ndarray: | 4. Return weighted sum of values: Attention(Q,K,V) = softmax(QK^T/√d_k)V<br><br><b>Steps:</b><br>1. Create Q, K, V matrices from X<br>2. Compute attention scores: QK^T / sqrt(d_k)<br>3. Apply softmax to get attention weights<br>4. Return weighted sum of values: Attention(Q,K,V) = softmax(QK^T/√d_k)V",
        "Attention Mechanisms",
        "problem_understanding"
      ],
      "guid": "nlp_35d9e6c911cff",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>softmax()</code>?",
        "<b>Purpose:</b> Numerically stable softmax implementation<br><br><pre><code>def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"\n    Numerically stable softmax implementation.\n    \n    Why stable? Subtracting max prevents overflow when exponentiating large numbers.\n    This is critical for attention weights which can have large values.\n    \"\"\"\n    # Subtract maximum value for numerical stability\n    # This doesn't change the relative probabilities but prevents exp() overflow\n    x_max = np.max(x, axis=axis, keepdims=True)\n    exp_x = np.exp(x - x_max)\n    \n    # Normalize to get probabilities (sum to 1 along specified axis)\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_7f1263ceb494f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>self_attention_part_1()</code>?",
        "<pre><code>def self_attention(X: np.ndarray, d_k: int) -> np.ndarray:\n    \"\"\"\n    Implement scaled dot-product self-attention mechanism.\n    \n    This is the CORE of all transformer models (BERT, GPT, etc.)\n    \n    Formula: Attention(Q,K,V) = softmax(QK^T / √d_k)V\n    \n    Args:\n        X: Input embeddings (seq_len, d_model)\n        d_k: Dimension of queries and keys (for scaling)\n    \n    Returns:\n        Attention output (seq_len, d_model)\n    \"\"\"\n    seq_len, d_model = X.shape</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_7056d39859d35",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>self_attention_part_2()</code>?",
        "<pre><code>    \n    # STEP 1: Initialize weight matrices\n    # In practice, these are learned parameters\n    # Using small random values for demonstration\n    np.random.seed(42)  # For reproducible results in interview\n    W_q = np.random.randn(d_model, d_k) * 0.1    # Query projection\n    W_k = np.random.randn(d_model, d_k) * 0.1    # Key projection  \n    W_v = np.random.randn(d_model, d_model) * 0.1 # Value projection\n    \n    # STEP 2: Create Query, Key, Value matrices\n    # These are linear transformations of the input\n    # Q: what we're looking for, K: what we're comparing against, V: what we return\n    Q = X @ W_q  # Shape: (seq_len, d_k)\n    K = X @ W_k  # Shape: (seq_len, d_k)  \n    V = X @ W_v  # Shape: (seq_len, d_model)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_1e328a06cee76",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>self_attention_part_3()</code>?",
        "<pre><code>    \n    # STEP 3: Calculate attention scores\n    # QK^T gives us similarity between all pairs of positions\n    scores = Q @ K.T  # Shape: (seq_len, seq_len)\n    \n    # STEP 4: Scale by sqrt(d_k)\n    # Prevents dot products from getting too large (which makes softmax too peaked)\n    # This scaling is CRUCIAL for stable training\n    scores = scores / np.sqrt(d_k)\n    \n    # STEP 5: Apply softmax to get attention weights\n    # Each row sums to 1.0 - these are the attention probabilities\n    # Higher score = more attention to that position\n    attention_weights = softmax(scores, axis=-1)\n    </code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_17b2a5f6fd9f0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>self_attention_part_4()</code>?",
        "<pre><code>    # STEP 6: Apply attention to values\n    # Weighted sum of value vectors based on attention weights\n    # This is where information actually flows between positions\n    output = attention_weights @ V  # Shape: (seq_len, d_model)\n    \n    return output</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_41120a86ef03d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>self_attention_with_mask_part_1()</code>?",
        "<pre><code>def self_attention_with_mask(X: np.ndarray, d_k: int, mask: np.ndarray = None) -> np.ndarray:\n    \"\"\"\n    Self-attention with causal masking (for GPT-style models).\n    \n    Causal mask prevents positions from attending to future positions.\n    Essential for autoregressive generation (predict next token).\n    \"\"\"\n    seq_len, d_model = X.shape\n    \n    # Same weight initialization as before\n    np.random.seed(42)\n    W_q = np.random.randn(d_model, d_k) * 0.1\n    W_k = np.random.randn(d_model, d_k) * 0.1\n    W_v = np.random.randn(d_model, d_model) * 0.1\n    </code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_644b470ef3ef8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>self_attention_with_mask_part_2()</code>?",
        "<pre><code>    # Create Q, K, V matrices\n    Q = X @ W_q\n    K = X @ W_k  \n    V = X @ W_v\n    \n    # Calculate attention scores\n    scores = (Q @ K.T) / np.sqrt(d_k)\n    \n    # STEP: Apply mask BEFORE softmax\n    # Masked positions get large negative values (-inf conceptually)\n    # After softmax, these become ~0 probability\n    if mask is not None:\n        # Add large negative value to masked positions\n        scores = scores + (mask * -1e9)\n    </code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_bbce5b6a14ce6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>self_attention_with_mask_part_3()</code>?",
        "<pre><code>    # Apply softmax and compute output\n    attention_weights = softmax(scores, axis=-1)\n    output = attention_weights @ V\n    \n    return output</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_0f9c166585c5e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>create_causal_mask()</code>?",
        "<b>Purpose:</b> Create causal mask for autoregressive attention<br><br><pre><code>def create_causal_mask(seq_len: int) -> np.ndarray:\n    \"\"\"\n    Create causal mask for autoregressive attention.\n    \n    Used in GPT to prevent \"looking into the future\" during training.\n    \n    Returns:\n        Mask matrix where 1 = mask (don't attend), 0 = allow\n    \"\"\"\n    # Create lower triangular matrix (1s below and on diagonal)\n    # This allows attending to current and previous positions only\n    lower_triangle = np.tril(np.ones((seq_len, seq_len)))\n    \n    # Convert to mask format: 0 = allow attention, 1 = mask\n    return 1 - lower_triangle</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_44a286e013a71",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>multi_head_attention_part_1()</code>?",
        "<pre><code>def multi_head_attention(X: np.ndarray, d_k: int, num_heads: int = 8) -> np.ndarray:\n    \"\"\"\n    Multi-head self-attention (simplified version).\n    \n    Key insight: Multiple attention heads can focus on different aspects\n    - Head 1 might focus on syntax\n    - Head 2 might focus on semantics\n    - Head 3 might focus on long-range dependencies\n    \"\"\"\n    seq_len, d_model = X.shape\n    \n    # REQUIREMENT: d_model must be divisible by num_heads\n    # Each head gets d_model/num_heads dimensions\n    assert d_model % num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n    \n    head_dim = d_model // num_heads\n    outputs = []</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_10883c2434177",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>How do you implement <code>multi_head_attention_part_2()</code>?",
        "<pre><code>    \n    # STEP: Apply attention for each head independently\n    for head in range(num_heads):\n        # Each head operates on a subset of the input dimensions\n        start_idx = head * head_dim\n        end_idx = start_idx + head_dim\n        X_head = X[:, start_idx:end_idx]\n        \n        # Apply self-attention to this head's slice\n        head_output = self_attention(X_head, head_dim)\n        outputs.append(head_output)\n    \n    # STEP: Concatenate all head outputs\n    # This gives us the full d_model dimensional output\n    return np.concatenate(outputs, axis=-1)</code></pre>",
        "Attention Mechanisms",
        "implementation"
      ],
      "guid": "nlp_91c8201a9c480",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Write the Key Formula",
        "<h3>def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_503e3f314b230",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Write the Attention(Q,K,V)",
        "<h3>softmax(QK^T / √d_k)V</h3><br><p><i>Scaled dot-product attention: computes attention weights and applies them to values</i></p><br><details><summary>Context</summary><pre>This is the CORE of all transformer models (BERT, GPT, etc.) | Args:</pre></details>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_bc1fa9c945118",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Write the Key Formula",
        "<h3>attention_weights = softmax(scores, axis=-1)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_4ff78f5b1132a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Write the Key Formula",
        "<h3>attention_weights = softmax(scores, axis=-1)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_4ff78f5b1132a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>Write the Key Formula",
        "<h3>weights = softmax(scores, axis=-1)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_2964decc10b06",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>What's the Time Complexity?",
        "<b>O(n²d)</b><br><i>See Big O notation reference</i>",
        "Attention Mechanisms",
        "complexity"
      ],
      "guid": "nlp_3dcf672bc1874",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>What's the Space Complexity?",
        "<b>O(n²)</b><br><i>Quadratic time - nested loops, can be slow for large inputs</i>",
        "Attention Mechanisms",
        "complexity"
      ],
      "guid": "nlp_7d87fc96709e5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMONSTRATION CODE</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_4d69cfb0fec7e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Formula: Attention(Q,K,V) = softmax(QK^T / √d_k)V</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_c751cb27862e7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Attention allows models to focus on relevant parts of input sequence</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_67144390d8d04",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Attention Mechanisms</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Self-attention computes relationships between all positions simultaneously</strong></p>",
        "Attention Mechanisms",
        "interview_insights"
      ],
      "guid": "nlp_8bafb7ed76301",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "attention_mechanisms",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement bag of words from scratch?",
        "<b>Task:</b> Create bag-of-words representation.",
        "Bow Vectors",
        "problem_understanding"
      ],
      "guid": "nlp_bc81e6c0959c4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>create_bow_vector_part_1()</code>?",
        "<pre><code>def create_bow_vector(documents: List[str]) -> Tuple[List[str], List[List[int]]]:\n    \"\"\"\n    Create bag-of-words representation from scratch.\n    \n    BAG-OF-WORDS CONCEPT:\n    - Represent text as vector of word counts\n    - \"Order doesn't matter, just word presence/frequency\"\n    - Foundation of many NLP systems before embeddings\n    \n    STEPS:\n    1. Build vocabulary (all unique words)\n    2. For each document, count occurrences of each vocab word\n    3. Return vocabulary and count vectors\n    \n    INTERVIEW INSIGHT: Simple but effective. Foundation for TF-IDF.\n    \"\"\"</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_16bc269d1be2e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>create_bow_vector_part_2()</code>?",
        "<pre><code>    \n    # STEP 1: Handle edge case\n    if not documents:\n        return [], []\n    \n    # STEP 2: Build vocabulary from all documents\n    # We need consistent vocabulary across all documents for vector comparison\n    vocab_set = set()\n    \n    for doc in documents:\n        # Simple tokenization: lowercase and split on whitespace\n        # In interviews, mention this could be more sophisticated\n        words = doc.lower().split()\n        vocab_set.update(words)\n    </code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_9828a69e309aa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>create_bow_vector_part_3()</code>?",
        "<pre><code>    # STEP 3: Create ordered vocabulary\n    # Sorting ensures consistent feature ordering across runs\n    vocabulary = sorted(list(vocab_set))\n    \n    # Create word-to-index mapping for efficient lookup\n    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n    \n    # STEP 4: Convert each document to vector\n    vectors = []\n    \n    for doc in documents:\n        # Tokenize document\n        words = doc.lower().split()\n        \n        # Count word occurrences in this document\n        word_counts = Counter(words)</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_fe9390fb4a722",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>create_bow_vector_part_4()</code>?",
        "<pre><code>        \n        # Create vector: count for each vocabulary word\n        # Index i contains count of vocabulary[i] in this document\n        vector = []\n        for word in vocabulary:\n            count = word_counts.get(word, 0)  # 0 if word not in document\n            vector.append(count)\n        \n        vectors.append(vector)\n    \n    return vocabulary, vectors</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_5112d968ded57",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>cosine_similarity_part_1()</code>?",
        "<pre><code>def cosine_similarity(vec1: List[int], vec2: List[int]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two BoW vectors.\n    \n    COSINE SIMILARITY INTUITION:\n    - Measures angle between vectors, not magnitude\n    - Good for text: \"I love cats\" vs \"I really really love cats\"\n    - Both have same direction (similar meaning) despite different lengths\n    \n    FORMULA: cos(θ) = (A·B) / (||A|| × ||B||)\n    \n    INTERVIEW TIP: Always explain why cosine > Euclidean for text\n    \"\"\"\n    \n    # STEP 1: Input validation\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_5b0cd34e14958",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>cosine_similarity_part_2()</code>?",
        "<pre><code>    \n    # STEP 2: Calculate dot product (numerator)\n    # Sum of element-wise products\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    \n    # STEP 3: Calculate vector norms (denominators)\n    # ||v|| = sqrt(sum of squared elements) = vector magnitude\n    norm1 = math.sqrt(sum(x * x for x in vec1))\n    norm2 = math.sqrt(sum(x * x for x in vec2))\n    \n    # STEP 4: Handle zero vectors (edge case)\n    # Zero vector has no direction, so similarity is undefined\n    if norm1 == 0 or norm2 == 0:\n        return 0.0\n    </code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_e52f95879fc57",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>cosine_similarity_part_3()</code>?",
        "<pre><code>    # STEP 5: Return cosine similarity\n    # Result is between -1 (opposite) and 1 (identical)\n    # For BoW with counts, result is between 0 and 1\n    return dot_product / (norm1 * norm2)</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_c260ca867c988",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>find_most_similar_part_1()</code>?",
        "<pre><code>def find_most_similar(documents: List[str], query_idx: int) -> int:\n    \"\"\"\n    Find document most similar to query document.\n    \n    REAL-WORLD APPLICATION:\n    - Document search and retrieval\n    - \"Find documents similar to this one\"\n    - Recommendation systems (\"users who read this also read...\")\n    \n    ALGORITHM:\n    1. Convert all documents to BoW vectors\n    2. Calculate similarity between query and each document\n    3. Return index of most similar document\n    \"\"\"\n    </code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_98904d42bd59a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>find_most_similar_part_2()</code>?",
        "<pre><code>    # STEP 1: Convert documents to BoW representation\n    vocab, vectors = create_bow_vector(documents)\n    \n    # STEP 2: Validate query index\n    if query_idx >= len(vectors) or query_idx < 0:\n        return -1  # Invalid query index\n    \n    # STEP 3: Get query vector\n    query_vector = vectors[query_idx]\n    \n    # STEP 4: Calculate similarity with all other documents\n    max_similarity = -1  # Start with impossible low value\n    most_similar_idx = -1\n    \n    for i, doc_vector in enumerate(vectors):</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_0347e1a81e5d3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>find_most_similar_part_3()</code>?",
        "<pre><code>        # Don't compare document with itself\n        if i != query_idx:\n            similarity = cosine_similarity(query_vector, doc_vector)\n            \n            # Track the highest similarity found\n            if similarity > max_similarity:\n                max_similarity = similarity\n                most_similar_idx = i\n    \n    return most_similar_idx</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_a3a12cd03ef7e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>analyze_vocabulary_distribution_part_1()</code>?",
        "<pre><code>def analyze_vocabulary_distribution(documents: List[str]) -> Dict[str, int]:\n    \"\"\"\n    Analyze vocabulary characteristics - good for follow-up questions.\n    \n    INTERVIEW INSIGHTS:\n    - Most words appear very rarely (Zipf's law)\n    - Top 100 words account for ~50% of text\n    - Vocabulary size grows with more documents\n    \"\"\"\n    vocab, _ = create_bow_vector(documents)\n    \n    # Count how often each word appears across documents\n    word_doc_counts = {}\n    for word in vocab:\n        count = sum(1 for doc in documents if word in doc.lower())\n        word_doc_counts[word] = count</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_1f558c7568c7d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>How do you implement <code>analyze_vocabulary_distribution_part_2()</code>?",
        "<pre><code>    \n    return word_doc_counts</code></pre>",
        "Bow Vectors",
        "implementation"
      ],
      "guid": "nlp_08c0efe5420c4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Write the similarity",
        "<h3>cosine_similarity(query_vector, doc_vector)</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre># Don't compare document with itself | if i != query_idx: | # Track the highest similarity found</pre></details>",
        "Bow Vectors",
        "formula"
      ],
      "guid": "nlp_d55f22e6f1cc8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Write the sim",
        "<h3>cosine_similarity(vectors[i], vectors[j])</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>for i in range(len(documents)): | for j in range(i + 1, len(documents)): | print(f\"  Doc {i} ↔ Doc {j}: {sim:.3f}\")</pre></details>",
        "Bow Vectors",
        "formula"
      ],
      "guid": "nlp_6598fe189167d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>Write the sim_score",
        "<h3>cosine_similarity(vectors[query_idx], vectors[most_similar])</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre># Calculate and show the similarity score | print(f\"Similarity score: {sim_score:.3f}\")</pre></details>",
        "Bow Vectors",
        "formula"
      ],
      "guid": "nlp_82ad82ad69368",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>What's the Time Complexity?",
        "<b>O(d × n × v)</b><br><i>See Big O notation reference</i>",
        "Bow Vectors",
        "complexity"
      ],
      "guid": "nlp_79d0f8097a625",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Bow Vectors</b><br>What's the Space Complexity?",
        "<b>O(d × v)</b><br><i>See Big O notation reference</i>",
        "Bow Vectors",
        "complexity"
      ],
      "guid": "nlp_103f2daf77b26",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "bow_vectors",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement text cnn for classification?",
        "<b>Task:</b> Implement forward pass of a text CNN.",
        "Cnn Text",
        "problem_understanding"
      ],
      "guid": "nlp_59c9efe7c5775",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>text_to_sequence()</code>?",
        "<b>Purpose:</b> Convert text to padded integer sequence<br><br><pre><code>def text_to_sequence(text: str, vocab: Dict[str, int], max_len: int = 10) -> List[int]:\n    \"\"\"Convert text to padded integer sequence.\"\"\"\n    words = text.lower().split()\n    sequence = [vocab.get(word, 0) for word in words]  # 0 for unknown words\n    \n    # Pad or truncate to max_len\n    if len(sequence) < max_len:\n        sequence += [0] * (max_len - len(sequence))\n    else:\n        sequence = sequence[:max_len]\n    \n    return sequence</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_4e6ec9793d557",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>embedding_lookup()</code>?",
        "<b>Purpose:</b> Look up embeddings for sequence<br><br><pre><code>def embedding_lookup(sequence: List[int], embedding_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Look up embeddings for sequence.\"\"\"\n    return np.array([embedding_matrix[idx] for idx in sequence])</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_9aa37ca747106",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>conv1d_part_1()</code>?",
        "<pre><code>def conv1d(embeddings: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n    \"\"\"Apply 1D convolution with kernel size 3.\"\"\"\n    seq_len, embed_dim = embeddings.shape\n    kernel_size = len(kernel)\n    \n    conv_output = []\n    \n    # Slide kernel over sequence\n    for i in range(seq_len - kernel_size + 1):\n        window = embeddings[i:i + kernel_size]  # Shape: (3, embed_dim)\n        \n        # Element-wise multiply and sum\n        conv_value = np.sum(window * kernel[:, np.newaxis])\n        conv_output.append(max(0, conv_value))  # ReLU activation\n    \n    return np.array(conv_output)</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_614d1a1371479",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>max_pool()</code>?",
        "<b>Purpose:</b> Global max pooling<br><br><pre><code>def max_pool(conv_output: np.ndarray) -> float:\n    \"\"\"Global max pooling.\"\"\"\n    return np.max(conv_output) if len(conv_output) > 0 else 0.0</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_2b864803faeb6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>sigmoid()</code>?",
        "<b>Purpose:</b> Sigmoid activation<br><br><pre><code>def sigmoid(x: float) -> float:\n    \"\"\"Sigmoid activation.\"\"\"\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_f9d9bb9d59305",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>text_cnn_predict_part_1()</code>?",
        "<pre><code>def text_cnn_predict(text: str, vocab: Dict[str, int], \n                    weights: Dict, max_len: int = 10) -> float:\n    \"\"\"CNN forward pass for text classification.\"\"\"\n    \n    # 1. Text to sequence\n    sequence = text_to_sequence(text, vocab, max_len)\n    \n    # 2. Embedding lookup\n    embeddings = embedding_lookup(sequence, weights['embedding'])\n    \n    # 3. Convolution\n    conv_output = conv1d(embeddings, weights['conv'])\n    \n    # 4. Max pooling\n    pooled = max_pool(conv_output)</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_fdb9f1770cf12",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>text_cnn_predict_part_2()</code>?",
        "<pre><code>    \n    # 5. Dense layer + sigmoid\n    dense_output = pooled * weights['dense'] + weights['bias']\n    probability = sigmoid(dense_output)\n    \n    return probability</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_cfd9f76fd83d7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>create_sample_weights()</code>?",
        "<b>Purpose:</b> Create sample weights for demonstration<br><br><pre><code>def create_sample_weights(vocab_size: int = 100, embed_dim: int = 50):\n    \"\"\"Create sample weights for demonstration.\"\"\"\n    return {\n        'embedding': np.random.randn(vocab_size, embed_dim) * 0.1,\n        'conv': np.random.randn(3) * 0.1,  # Kernel size 3\n        'dense': np.random.randn() * 0.1,\n        'bias': 0.0\n    }</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_c0aba48ccc368",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>test_cnn_part_1()</code>?",
        "<pre><code>def test_cnn():\n    # Sample vocabulary\n    vocab = {\n        'good': 1, 'bad': 2, 'movie': 3, 'film': 4, \n        'great': 5, 'terrible': 6, 'love': 7, 'hate': 8\n    }\n    \n    # Sample weights\n    weights = create_sample_weights(vocab_size=len(vocab) + 1, embed_dim=10)\n    \n    # Test sentences\n    test_texts = [\n        \"good movie\",\n        \"bad film\", \n        \"great love\",\n        \"terrible hate\"\n    ]</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_48c1b3bdfc354",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Cnn Text</b><br>How do you implement <code>test_cnn_part_2()</code>?",
        "<pre><code>    \n    print(\"CNN Text Classification Results:\")\n    for text in test_texts:\n        prob = text_cnn_predict(text, vocab, weights)\n        prediction = \"positive\" if prob > 0.5 else \"negative\"\n        print(f\"'{text}' -> {prob:.3f} ({prediction})\")</code></pre>",
        "Cnn Text",
        "implementation"
      ],
      "guid": "nlp_5be74f6630a1b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "cnn_text",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement word2vec skip-gram?",
        "<b>Task:</b> Implement the core Skip-gram training step for Word2Vec.",
        "Embeddings",
        "problem_understanding"
      ],
      "guid": "nlp_a2a051886391c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>sigmoid()</code>?",
        "<b>Purpose:</b> Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n\nUsed in Word2Vec to convert dot products to pr...<br><br><pre><code>def sigmoid(x: float) -> float:\n    \"\"\"\n    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n    \n    Used in Word2Vec to convert dot products to probabilities.\n    Clamp input to prevent numerical overflow/underflow.\n    \"\"\"\n    # Clamp x to prevent overflow in exp() function\n    # This is crucial for numerical stability\n    clamped_x = max(-500, min(500, x))\n    return 1 / (1 + math.exp(-clamped_x))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_27fa5482f65f3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>dot_product()</code>?",
        "<b>Purpose:</b> Calculate dot product between two vectors<br><br><pre><code>def dot_product(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"\n    Calculate dot product between two vectors.\n    \n    Dot product measures similarity between vectors:\n    - High positive value = vectors point in same direction (similar)\n    - Zero = vectors are orthogonal (unrelated)\n    - Negative = vectors point in opposite directions (dissimilar)\n    \"\"\"\n    # Element-wise multiplication, then sum\n    return sum(a * b for a, b in zip(vec1, vec2))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_a8ee297f3542e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>skipgram_step_part_1()</code>?",
        "<pre><code>def skipgram_step(center_word: str, context_word: str, \n                  embeddings: Dict[str, List[float]], \n                  learning_rate: float = 0.01) -> None:\n    \"\"\"\n    Perform one training step of Skip-gram Word2Vec.\n    \n    SKIP-GRAM OBJECTIVE: Given center word, predict context words\n    - Makes words that appear in similar contexts have similar embeddings\n    - \"king\" and \"queen\" both appear near \"royal\", \"crown\" -> similar embeddings\n    \n    TRAINING STEP:\n    1. Calculate current similarity between center and context word\n    2. If they co-occur, increase their similarity (gradient ascent)\n    3. Update both embeddings to be more similar\n    \n    Args:\n        center_word: The word we're using to predict context\n        context_word: The word in the context window\n        embeddings: Current word embeddings (will be modified in-place)\n        learning_rate: How big steps to take during learning\n    \"\"\"</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_2b76b956705b7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>skipgram_step_part_2()</code>?",
        "<pre><code>    \n    # STEP 1: Handle missing words\n    # In real implementation, you'd skip unknown words or use subword tokens\n    if center_word not in embeddings or context_word not in embeddings:\n        return  # Skip this training example\n    \n    # STEP 2: Get current embeddings\n    center_vec = embeddings[center_word]     # Current embedding for center word\n    context_vec = embeddings[context_word]   # Current embedding for context word\n    \n    # STEP 3: FORWARD PASS - Calculate current similarity\n    # Dot product measures how similar the embeddings currently are\n    dot_prod = dot_product(center_vec, context_vec)\n    \n    # Convert to probability using sigmoid</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_e331bd4c79849",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>skipgram_step_part_3()</code>?",
        "<pre><code>    # High dot product -> high probability of co-occurrence\n    prob = sigmoid(dot_prod)\n    \n    # STEP 4: BACKWARD PASS - Calculate gradients\n    # We want to MAXIMIZE the probability of this positive pair\n    # Gradient of log(sigmoid(x)) is (1 - sigmoid(x))\n    gradient_scale = (1 - prob) * learning_rate\n    \n    # STEP 5: UPDATE EMBEDDINGS\n    # Move embeddings to increase their similarity\n    \n    # Update center word embedding:\n    # Add context_word's embedding scaled by gradient\n    for i in range(len(center_vec)):\n        embeddings[center_word][i] += gradient_scale * context_vec[i]</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_899c36a0edb1b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>skipgram_step_part_4()</code>?",
        "<pre><code>    \n    # Update context word embedding:\n    # Add center_word's embedding scaled by gradient  \n    for i in range(len(context_vec)):\n        embeddings[context_word][i] += gradient_scale * center_vec[i]</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_a6770df2d0ebf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>word_similarity_part_1()</code>?",
        "<pre><code>def word_similarity(word1: str, word2: str, \n                   embeddings: Dict[str, List[float]]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two word embeddings.\n    \n    Cosine similarity = dot_product(v1, v2) / (||v1|| × ||v2||)\n    - Returns value between -1 and 1\n    - 1.0 = identical direction (very similar words)\n    - 0.0 = orthogonal (unrelated words)\n    - -1.0 = opposite direction (opposite meaning)\n    \"\"\"\n    \n    # STEP 1: Handle missing words\n    if word1 not in embeddings or word2 not in embeddings:\n        return 0.0  # No similarity if words not in vocabulary</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_afdb9d106637a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>word_similarity_part_2()</code>?",
        "<pre><code>    \n    # STEP 2: Get word vectors\n    vec1 = embeddings[word1]\n    vec2 = embeddings[word2]\n    \n    # STEP 3: Calculate dot product (numerator)\n    dot_prod = dot_product(vec1, vec2)\n    \n    # STEP 4: Calculate vector norms (denominators)\n    # ||v|| = sqrt(sum of squared elements)\n    norm1 = math.sqrt(sum(x * x for x in vec1))\n    norm2 = math.sqrt(sum(x * x for x in vec2))\n    \n    # STEP 5: Handle zero vectors (shouldn't happen with proper training)\n    if norm1 == 0 or norm2 == 0:\n        return 0.0</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_897e93f188ca9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>word_similarity_part_3()</code>?",
        "<pre><code>    \n    # STEP 6: Return cosine similarity\n    return dot_prod / (norm1 * norm2)</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_c70e823529089",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>find_most_similar_part_1()</code>?",
        "<pre><code>def find_most_similar(target_word: str, embeddings: Dict[str, List[float]], \n                     top_k: int = 3) -> List[Tuple[str, float]]:\n    \"\"\"\n    Find k most similar words to target word.\n    \n    This is how you'd implement word analogy and similarity queries.\n    Core functionality of Word2Vec for exploration and evaluation.\n    \"\"\"\n    if target_word not in embeddings:\n        return []  # Target word not in vocabulary\n    \n    # Calculate similarity with all other words\n    similarities = []\n    \n    for word in embeddings:\n        if word != target_word:  # Don't compare word with itself\n            sim = word_similarity(target_word, word, embeddings)\n            similarities.append((word, sim))</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_69693ec70ae48",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>How do you implement <code>find_most_similar_part_2()</code>?",
        "<pre><code>    \n    # Sort by similarity (highest first) and return top-k\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    return similarities[:top_k]</code></pre>",
        "Embeddings",
        "implementation"
      ],
      "guid": "nlp_12de0eb5b773e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMONSTRATION</strong></p>",
        "Embeddings",
        "interview_insights"
      ],
      "guid": "nlp_51a2b5d9c2a2f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Embeddings map discrete tokens to dense continuous vectors</strong></p>",
        "Embeddings",
        "interview_insights"
      ],
      "guid": "nlp_105533b9a9fd1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Embeddings</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Similar words have similar embeddings in vector space</strong></p>",
        "Embeddings",
        "interview_insights"
      ],
      "guid": "nlp_838f84b1b5a60",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "embeddings",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>spell_check_approach()</code>?",
        "<b>Purpose:</b> KEY: Use edit distance + frequency ranking\nSTEPS: 1) Find close words 2) Rank by frequency\nINSIGHT: ...<br><br><pre><code>def spell_check_approach():\n    \"\"\"\n    KEY: Use edit distance + frequency ranking\n    STEPS: 1) Find close words 2) Rank by frequency\n    INSIGHT: Most typos are 1-2 edits away\n    \"\"\"\n    pass</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_7c4700b535510",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>edit_distance_recursive_part_1()</code>?",
        "<pre><code>def edit_distance_recursive(s1: str, s2: str) -> int:\n    \"\"\"\n    FORMULA: ED(i,j) = min(\n        ED(i-1,j) + 1,    # deletion\n        ED(i,j-1) + 1,    # insertion  \n        ED(i-1,j-1) + 0/1 # substitution\n    )\n    \"\"\"\n    if not s1: return len(s2)\n    if not s2: return len(s1)\n    \n    if s1[0] == s2[0]:\n        return edit_distance_recursive(s1[1:], s2[1:])\n    \n    return 1 + min(\n        edit_distance_recursive(s1[1:], s2),    # delete\n        edit_distance_recursive(s1, s2[1:]),    # insert\n        edit_distance_recursive(s1[1:], s2[1:]) # replace\n    )</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_e7daafe7c45cb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>edit_distance_dp_part_1()</code>?",
        "<pre><code>def edit_distance_dp(s1: str, s2: str) -> int:\n    \"\"\"\n    COMPLEXITY: O(m*n) time, O(m*n) space\n    OPTIMIZE: Can reduce to O(min(m,n)) space\n    \"\"\"\n    m, n = len(s1), len(s2)\n    dp = [[0] * (n+1) for _ in range(m+1)]\n    \n    # Base cases\n    for i in range(m+1): dp[i][0] = i\n    for j in range(n+1): dp[0][j] = j\n    \n    # Fill matrix\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            if s1[i-1] == s2[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i-1][j],    # delete\n                    dp[i][j-1],    # insert\n                    dp[i-1][j-1]   # replace\n                )</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_e91a97d4d5046",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>edit_distance_dp_part_2()</code>?",
        "<pre><code>    \n    return dp[m][n]</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_f580e607288c7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>find_candidates_part_1()</code>?",
        "<pre><code>def find_candidates(word: str, dictionary: Dict[str, int], \n                   max_distance: int = 2) -> List[Tuple[str, int]]:\n    \"\"\"\n    STRATEGY: Only check words within length ±2\n    EDGE: Empty dictionary returns empty list\n    \"\"\"\n    if not dictionary:\n        return []\n    \n    candidates = []\n    word_len = len(word)\n    \n    for dict_word in dictionary:\n        # Pruning: skip if length difference > max_distance\n        if abs(len(dict_word) - word_len) > max_distance:\n            continue</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_da3b794c1aea0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>find_candidates_part_2()</code>?",
        "<pre><code>            \n        distance = edit_distance_dp(word, dict_word)\n        if distance <= max_distance:\n            candidates.append((dict_word, distance))\n    \n    return candidates</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_4edd834332c08",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>rank_by_frequency_part_1()</code>?",
        "<pre><code>def rank_by_frequency(candidates: List[Tuple[str, int]], \n                     dictionary: Dict[str, int]) -> List[str]:\n    \"\"\"\n    FORMULA: score = frequency / (distance + 1)\n    KEY: Higher frequency, lower distance = better\n    \"\"\"\n    scored = []\n    \n    for word, distance in candidates:\n        freq = dictionary.get(word, 1)\n        score = freq / (distance + 1)\n        scored.append((word, score))\n    \n    # Sort by score descending\n    scored.sort(key=lambda x: x[1], reverse=True)</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_44df519a38618",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>rank_by_frequency_part_2()</code>?",
        "<pre><code>    \n    return [word for word, _ in scored[:3]]</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_e3bf4b5007a09",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>spell_checker_part_1()</code>?",
        "<pre><code>def spell_checker(word: str, dictionary: Dict[str, int]) -> List[str]:\n    \"\"\"\n    INTERVIEW: Mention trade-offs:\n    - Edit distance vs phonetic matching\n    - Memory vs speed (DP vs recursive)\n    - Pruning strategies for large dictionaries\n    \"\"\"\n    # EDGE: Already correct word\n    if word in dictionary:\n        return [word]\n    \n    # EDGE: Empty dictionary\n    if not dictionary:\n        return []\n    </code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_dcb659371ae86",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>spell_checker_part_2()</code>?",
        "<pre><code>    # Find candidates within edit distance 2\n    candidates = find_candidates(word, dictionary, max_distance=2)\n    \n    # EDGE: No candidates found\n    if not candidates:\n        return []\n    \n    # Rank by frequency and return top 3\n    return rank_by_frequency(candidates, dictionary)</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_38eb90b73c437",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>How do you implement <code>example_usage()</code>?",
        "<b>Purpose:</b> EXAMPLE:\ndictionary = {\"hello\": 100, \"help\": 80, \"hell\": 20}\nspell_checker(\"helo\", dictionary) → [\"h...<br><br><pre><code>def example_usage():\n    \"\"\"\n    EXAMPLE:\n    dictionary = {\"hello\": 100, \"help\": 80, \"hell\": 20}\n    spell_checker(\"helo\", dictionary) → [\"hello\", \"help\", \"hell\"]\n    \n    TEST EDGE CASES:\n    - Empty string\n    - Word already in dictionary  \n    - No close matches\n    - Single character words\n    \"\"\"\n    pass</code></pre>",
        "Example Anki Refactor",
        "implementation"
      ],
      "guid": "nlp_cd8aeaa71affd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Write the Key Formula",
        "<h3>- rank_by_frequency: O(k log k) where k=candidates</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Example Anki Refactor",
        "formula"
      ],
      "guid": "nlp_26f2b428c1f29",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Key concept:",
        "<p>from typing import Dict, List, Tuple\n\n# Card 1: Algorithm Overview\ndef spell_check_approach():</p>",
        "Example Anki Refactor",
        "concepts"
      ],
      "guid": "nlp_16246265cf09b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Example Anki Refactor</b><br>Key concept:",
        "<p>from typing import Dict, List, Tuple\n\n# Card 1: Algorithm Overview\ndef spell_check_approach():</p>",
        "Example Anki Refactor",
        "concepts"
      ],
      "guid": "nlp_16246265cf09b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "example_anki_refactor",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement llm fine-tuning for classification?",
        "<b>Task:</b> Implement the key components for fine-tuning a pre-trained LLM for text classification.",
        "Fine Tuning",
        "problem_understanding"
      ],
      "guid": "nlp_9ce9ece4d1cec",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>add_classification_head()</code>?",
        "<b>Purpose:</b> Add classification head to pretrained LLM<br><br><pre><code>def add_classification_head(pretrained_model_dim: int, num_classes: int) -> Dict:\n    \"\"\"Add classification head to pretrained LLM.\"\"\"\n    \n    # Xavier/Glorot initialization for stable training\n    std = np.sqrt(2.0 / (pretrained_model_dim + num_classes))\n    \n    return {\n        'W_cls': np.random.randn(pretrained_model_dim, num_classes) * std,\n        'b_cls': np.zeros(num_classes)\n    }</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_2b10d8a5bbba3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>compute_classification_loss_part_1()</code>?",
        "<pre><code>def compute_classification_loss(logits: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Compute cross-entropy loss with numerical stability.\"\"\"\n    batch_size, num_classes = logits.shape\n    \n    # Numerical stability: subtract max from logits\n    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n    \n    # Softmax probabilities\n    exp_logits = np.exp(logits_stable)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Cross-entropy loss\n    loss = 0.0\n    for i in range(batch_size):\n        true_class = labels[i]\n        loss += -np.log(probs[i, true_class] + 1e-10)  # Add small epsilon</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_17c09d55b238d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>compute_classification_loss_part_2()</code>?",
        "<pre><code>    \n    return loss / batch_size</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_00946a2ea11c6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>freeze_layers_part_1()</code>?",
        "<pre><code>def freeze_layers(model_weights: Dict, freeze_ratio: float = 0.8) -> Dict:\n    \"\"\"Mark layers as frozen (simulate requires_grad=False).\"\"\"\n    frozen_info = {}\n    \n    # Sort layers by name to freeze bottom layers\n    layer_names = sorted([name for name in model_weights.keys() if 'layer_' in name])\n    \n    num_layers = len(layer_names)\n    num_frozen = int(num_layers * freeze_ratio)\n    \n    for i, layer_name in enumerate(layer_names):\n        frozen_info[layer_name] = i < num_frozen  # True if frozen\n    \n    # Never freeze classification head\n    for name in model_weights.keys():\n        if 'cls' in name:\n            frozen_info[name] = False</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_9ff0b0c00a327",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>freeze_layers_part_2()</code>?",
        "<pre><code>    \n    return frozen_info</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_ead291d9a0799",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>fine_tuning_step_part_1()</code>?",
        "<pre><code>def fine_tuning_step(x: np.ndarray, labels: np.ndarray, \n                    pretrained_weights: Dict, cls_head: Dict,\n                    learning_rates: Dict) -> Tuple[float, Dict]:\n    \"\"\"Single fine-tuning step (forward + backward).\"\"\"\n    \n    # Forward pass through pretrained model (simplified)\n    # In practice, this would be the full LLM forward pass\n    pretrained_output = x @ pretrained_weights['final_layer'] + pretrained_weights['final_bias']\n    \n    # Classification head forward pass\n    logits = pretrained_output @ cls_head['W_cls'] + cls_head['b_cls']\n    \n    # Compute loss\n    loss = compute_classification_loss(logits, labels)\n    </code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_eb39c1acf0e38",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>fine_tuning_step_part_2()</code>?",
        "<pre><code>    # Backward pass (simplified gradients)\n    batch_size = x.shape[0]\n    \n    # Softmax for gradient calculation\n    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits_stable)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Gradient w.r.t logits\n    grad_logits = probs.copy()\n    for i in range(batch_size):\n        grad_logits[i, labels[i]] -= 1\n    grad_logits /= batch_size\n    \n    # Gradients for classification head\n    grad_W_cls = pretrained_output.T @ grad_logits\n    grad_b_cls = np.sum(grad_logits, axis=0)</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_acfb1c90c3b9c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>fine_tuning_step_part_3()</code>?",
        "<pre><code>    \n    # Update classification head with higher learning rate\n    cls_head['W_cls'] -= learning_rates['cls_head'] * grad_W_cls\n    cls_head['b_cls'] -= learning_rates['cls_head'] * grad_b_cls\n    \n    # Update pretrained layers with lower learning rate (if not frozen)\n    grad_pretrained = grad_logits @ cls_head['W_cls'].T\n    \n    if not pretrained_weights.get('frozen', True):\n        pretrained_weights['final_layer'] -= learning_rates['pretrained'] * (x.T @ grad_pretrained)\n        pretrained_weights['final_bias'] -= learning_rates['pretrained'] * np.sum(grad_pretrained, axis=0)\n    \n    return loss, {'grad_W_cls': grad_W_cls, 'grad_b_cls': grad_b_cls}</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_d1eb57908d0e4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>compute_accuracy()</code>?",
        "<b>Purpose:</b> Compute classification accuracy<br><br><pre><code>def compute_accuracy(logits: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Compute classification accuracy.\"\"\"\n    predictions = np.argmax(logits, axis=1)\n    return np.mean(predictions == labels)</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_dbcb520e6797f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>lora_approximation()</code>?",
        "<b>Purpose:</b> Simulate LoRA (Low-Rank Adaptation) decomposition<br><br><pre><code>def lora_approximation(weight_matrix: np.ndarray, rank: int = 4) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Simulate LoRA (Low-Rank Adaptation) decomposition.\n    \n    Instead of updating full weight matrix W, update W + A*B where:\n    - A is (d, rank) and B is (rank, d) \n    - Only A and B are trainable (much fewer parameters)\n    \"\"\"\n    d_in, d_out = weight_matrix.shape\n    \n    # Initialize LoRA matrices\n    A = np.random.randn(d_in, rank) * 0.01\n    B = np.zeros((rank, d_out))  # B initialized to zero\n    \n    return A, B</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_4525f0f67f4a4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>test_fine_tuning_part_1()</code>?",
        "<pre><code>def test_fine_tuning():\n    \"\"\"Demonstrate fine-tuning workflow.\"\"\"\n    \n    # Sample data\n    batch_size, seq_len, d_model = 8, 10, 64\n    num_classes = 3\n    \n    # Simulated pretrained model output\n    x = np.random.randn(batch_size, d_model)  # [CLS] token representations\n    labels = np.random.randint(0, num_classes, batch_size)\n    \n    print(\"Fine-tuning Simulation\")\n    print(\"=\" * 30)\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Model dimension: {d_model}\")\n    print(f\"Number of classes: {num_classes}\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_a3f5bd9866d27",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>test_fine_tuning_part_2()</code>?",
        "<pre><code>    \n    # Add classification head\n    cls_head = add_classification_head(d_model, num_classes)\n    print(f\"\\nClassification head shape: {cls_head['W_cls'].shape}\")\n    \n    # Create pretrained weights\n    pretrained_weights = {\n        'final_layer': np.random.randn(d_model, d_model) * 0.02,\n        'final_bias': np.zeros(d_model),\n        'frozen': False  # Will be set by freeze_layers\n    }\n    \n    # Demonstrate freezing\n    freeze_info = freeze_layers({'layer_0': None, 'layer_1': None, 'layer_2': None}, freeze_ratio=0.67)\n    print(f\"\\nLayer freezing (freeze_ratio=0.67): {freeze_info}\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_3e03245e6bc3d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>test_fine_tuning_part_3()</code>?",
        "<pre><code>    \n    # Different learning rates\n    learning_rates = {\n        'pretrained': 1e-5,  # Lower LR for pretrained layers\n        'cls_head': 1e-3     # Higher LR for new classification head\n    }\n    \n    print(f\"\\nLearning rates: {learning_rates}\")\n    \n    # Training loop simulation\n    print(f\"\\nTraining simulation:\")\n    for epoch in range(3):\n        loss, grads = fine_tuning_step(x, labels, pretrained_weights, cls_head, learning_rates)\n        \n        # Calculate accuracy\n        logits = x @ cls_head['W_cls'] + cls_head['b_cls']\n        accuracy = compute_accuracy(logits, labels)</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_f5fb56b2ebe56",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>test_fine_tuning_part_4()</code>?",
        "<pre><code>        \n        print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}, Accuracy = {accuracy:.3f}\")\n    \n    # Demonstrate LoRA\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"LoRA Parameter-Efficient Fine-tuning\")\n    \n    # Original weight matrix\n    W_original = np.random.randn(512, 512)\n    A, B = lora_approximation(W_original, rank=8)\n    \n    original_params = W_original.size\n    lora_params = A.size + B.size\n    reduction = (1 - lora_params / original_params) * 100\n    \n    print(f\"Original parameters: {original_params:,}\")\n    print(f\"LoRA parameters: {lora_params:,}\")\n    print(f\"Parameter reduction: {reduction:.1f}%\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_33fc9278c6157",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>How do you implement <code>test_fine_tuning_part_5()</code>?",
        "<pre><code>    \n    # Show how LoRA update works\n    lora_update = A @ B\n    updated_weight = W_original + lora_update\n    print(f\"LoRA update shape: {lora_update.shape}\")\n    print(\"✓ LoRA allows efficient adaptation with few parameters\")</code></pre>",
        "Fine Tuning",
        "implementation"
      ],
      "guid": "nlp_8073d6f00f4e2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>batch_size, num_classes = logits.shape</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_a741cc0692feb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>logits_stable = logits - np.max(logits, axis=1, keepdims=True)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_91a40d6b86ac7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>exp_logits = np.exp(logits_stable)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_ffa3e2e76281a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_86d4aef9363cf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>Add small epsilon</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_b7bd4c6e30da1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>logits = pretrained_output @ cls_head['W_cls'] + cls_head['b_cls']</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_e7ebabd98d190",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>loss = compute_classification_loss(logits, labels)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_c5d991d3c02fd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>logits_stable = logits - np.max(logits, axis=1, keepdims=True)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_91a40d6b86ac7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>exp_logits = np.exp(logits_stable)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_ffa3e2e76281a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_86d4aef9363cf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>grad_logits = probs.copy()</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_24dcb15502eee",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>grad_logits[i, labels[i]] -= 1</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_5a4f611efd855",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>grad_logits /= batch_size</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_4f94396e6e9e0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>grad_W_cls = pretrained_output.T @ grad_logits</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_5f8846252dc81",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>grad_b_cls = np.sum(grad_logits, axis=0)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_02812982bf37a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>grad_pretrained = grad_logits @ cls_head['W_cls'].T</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_0eb7f9a274346",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>predictions = np.argmax(logits, axis=1)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_71cc54d0bfe6d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>logits = x @ cls_head['W_cls'] + cls_head['b_cls']</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_1dba88e1567fe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Fine Tuning</b><br>Write the Key Formula",
        "<h3>accuracy = compute_accuracy(logits, labels)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Fine Tuning",
        "formula"
      ],
      "guid": "nlp_ca684cd947e8b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "fine_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement gpt transformer block?",
        "<b>Task:</b> Implement a single GPT transformer block with the standard architecture.",
        "Gpt Implementation",
        "problem_understanding"
      ],
      "guid": "nlp_bc4a49d3dc4e2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>gelu()</code>?",
        "<b>Purpose:</b> GELU activation function used in GPT<br><br><pre><code>def gelu(x: np.ndarray) -> np.ndarray:\n    \"\"\"GELU activation function used in GPT.\"\"\"\n    return 0.5 * x * (1 + np.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_17be8914fb2ff",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>layer_norm()</code>?",
        "<b>Purpose:</b> Apply layer normalization<br><br><pre><code>def layer_norm(x: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n    \"\"\"Apply layer normalization.\"\"\"\n    # Calculate mean and variance along last dimension\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    \n    # Normalize\n    normalized = (x - mean) / np.sqrt(variance + eps)\n    \n    # Scale and shift\n    return gamma * normalized + beta</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_9eb75a2665d42",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>causal_self_attention_part_1()</code>?",
        "<pre><code>def causal_self_attention(x: np.ndarray, W_qkv: np.ndarray, W_out: np.ndarray) -> np.ndarray:\n    \"\"\"Causal self-attention for GPT.\"\"\"\n    seq_len, d_model = x.shape\n    \n    # Project to Q, K, V\n    qkv = x @ W_qkv  # (seq_len, 3 * d_model)\n    q, k, v = np.split(qkv, 3, axis=-1)\n    \n    # Attention scores\n    scores = q @ k.T / math.sqrt(d_model)\n    \n    # Apply causal mask (can't attend to future tokens)\n    mask = np.triu(np.ones((seq_len, seq_len)), k=1) * -1e9\n    scores = scores + mask\n    </code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_93b86edacf130",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>causal_self_attention_part_2()</code>?",
        "<pre><code>    # Softmax\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Apply attention to values\n    attended = attention_weights @ v\n    \n    # Output projection\n    return attended @ W_out</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_f9ee13b5687fa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>feed_forward()</code>?",
        "<b>Purpose:</b> Feed-forward network with GELU activation<br><br><pre><code>def feed_forward(x: np.ndarray, W1: np.ndarray, b1: np.ndarray, \n                W2: np.ndarray, b2: np.ndarray) -> np.ndarray:\n    \"\"\"Feed-forward network with GELU activation.\"\"\"\n    # First layer\n    hidden = gelu(x @ W1 + b1)\n    \n    # Second layer  \n    output = hidden @ W2 + b2\n    \n    return output</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_308766f301337",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>gpt_block()</code>?",
        "<b>Purpose:</b> Single GPT transformer block<br><br><pre><code>def gpt_block(x: np.ndarray, weights: Dict) -> np.ndarray:\n    \"\"\"Single GPT transformer block.\"\"\"\n    seq_len, d_model = x.shape\n    \n    # 1. Layer norm + self-attention + residual\n    norm1 = layer_norm(x, weights['ln1_gamma'], weights['ln1_beta'])\n    attn_out = causal_self_attention(norm1, weights['W_qkv'], weights['W_out'])\n    x = x + attn_out  # Residual connection\n    \n    # 2. Layer norm + feed-forward + residual  \n    norm2 = layer_norm(x, weights['ln2_gamma'], weights['ln2_beta'])\n    ffn_out = feed_forward(norm2, weights['W1'], weights['b1'], weights['W2'], weights['b2'])\n    x = x + ffn_out  # Residual connection\n    \n    return x</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_a1afed4442c5c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>create_gpt_weights_part_1()</code>?",
        "<pre><code>def create_gpt_weights(d_model: int = 64, d_ff: int = 256) -> Dict:\n    \"\"\"Create sample weights for GPT block.\"\"\"\n    np.random.seed(42)\n    \n    return {\n        # Layer norm parameters\n        'ln1_gamma': np.ones(d_model),\n        'ln1_beta': np.zeros(d_model),\n        'ln2_gamma': np.ones(d_model), \n        'ln2_beta': np.zeros(d_model),\n        \n        # Attention weights\n        'W_qkv': np.random.randn(d_model, 3 * d_model) * 0.02,\n        'W_out': np.random.randn(d_model, d_model) * 0.02,\n        </code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_5b07ae3e1522e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>create_gpt_weights_part_2()</code>?",
        "<pre><code>        # Feed-forward weights  \n        'W1': np.random.randn(d_model, d_ff) * 0.02,\n        'b1': np.zeros(d_ff),\n        'W2': np.random.randn(d_ff, d_model) * 0.02,\n        'b2': np.zeros(d_model)\n    }</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_1239fc44101fe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>positional_encoding()</code>?",
        "<b>Purpose:</b> Create sinusoidal positional encodings<br><br><pre><code>def positional_encoding(seq_len: int, d_model: int) -> np.ndarray:\n    \"\"\"Create sinusoidal positional encodings.\"\"\"\n    pos_enc = np.zeros((seq_len, d_model))\n    \n    for pos in range(seq_len):\n        for i in range(0, d_model, 2):\n            # Sine for even indices\n            pos_enc[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n            \n            # Cosine for odd indices\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n    \n    return pos_enc</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_02e495830c554",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>simple_gpt_forward_part_1()</code>?",
        "<pre><code>def simple_gpt_forward(token_ids: List[int], vocab_size: int = 1000, \n                      d_model: int = 64, num_layers: int = 2) -> np.ndarray:\n    \"\"\"Forward pass through a simple GPT model.\"\"\"\n    seq_len = len(token_ids)\n    \n    # Token embeddings (random for demo)\n    np.random.seed(42)\n    embedding_matrix = np.random.randn(vocab_size, d_model) * 0.02\n    \n    # Get embeddings for input tokens\n    x = np.array([embedding_matrix[token_id] for token_id in token_ids])\n    \n    # Add positional encodings\n    pos_enc = positional_encoding(seq_len, d_model)\n    x = x + pos_enc</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_3d6c1b10597dd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Gpt Implementation</b><br>How do you implement <code>simple_gpt_forward_part_2()</code>?",
        "<pre><code>    \n    # Pass through transformer blocks\n    for layer in range(num_layers):\n        weights = create_gpt_weights(d_model)\n        x = gpt_block(x, weights)\n    \n    return x</code></pre>",
        "Gpt Implementation",
        "implementation"
      ],
      "guid": "nlp_06853d8c4256e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "gpt_implementation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement instruction following setup?",
        "<b>Task:</b> Implement the data preparation and loss calculation for instruction fine-tuning.",
        "Instruction Tuning",
        "problem_understanding"
      ],
      "guid": "nlp_45529d5d1a721",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>format_instruction_data()</code>?",
        "<b>Purpose:</b> Format instruction-response pair for training<br><br><pre><code>def format_instruction_data(instruction: str, response: str) -> str:\n    \"\"\"Format instruction-response pair for training.\"\"\"\n    return f\"### Instruction:\\n{instruction}\\n### Response:\\n{response}\"</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_0f6ecbc655070",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>tokenize_instruction_response_part_1()</code>?",
        "<pre><code>def tokenize_instruction_response(formatted_text: str, \n                                tokenizer_vocab: Dict[str, int]) -> Tuple[List[int], int]:\n    \"\"\"\n    Tokenize formatted instruction-response and return instruction length.\n    \n    Returns:\n        (token_ids, instruction_length)\n    \"\"\"\n    # Simple tokenization for demo\n    tokens = formatted_text.lower().split()\n    token_ids = [tokenizer_vocab.get(token, 0) for token in tokens]  # 0 = UNK\n    \n    # Find where instruction ends (look for \"### Response:\")\n    instruction_length = 0\n    response_marker = \"### response:\"</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_5252210e22ad9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>tokenize_instruction_response_part_2()</code>?",
        "<pre><code>    \n    # Reconstruct text to find marker\n    text_lower = formatted_text.lower()\n    if response_marker in text_lower:\n        before_response = text_lower[:text_lower.index(response_marker)]\n        instruction_tokens = before_response.split()\n        instruction_length = len(instruction_tokens) + 2  # +2 for \"### response:\"\n    \n    return token_ids, instruction_length</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_22c7dc96d9708",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>compute_instruction_loss_part_1()</code>?",
        "<pre><code>def compute_instruction_loss(model_output: np.ndarray, \n                           target_tokens: List[int],\n                           instruction_length: int) -> float:\n    \"\"\"Compute loss only on response tokens.\"\"\"\n    seq_len, vocab_size = model_output.shape\n    \n    if instruction_length >= seq_len:\n        return 0.0  # No response tokens to train on\n    \n    # Only compute loss on response portion\n    response_logits = model_output[instruction_length:]\n    response_targets = target_tokens[instruction_length:]\n    \n    if len(response_targets) == 0:\n        return 0.0</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_46eb356c67a3e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>compute_instruction_loss_part_2()</code>?",
        "<pre><code>    \n    # Cross-entropy loss on response tokens only\n    loss = 0.0\n    num_response_tokens = len(response_targets)\n    \n    for i, target_token in enumerate(response_targets):\n        if i < len(response_logits):\n            # Softmax\n            logits = response_logits[i]\n            max_logit = np.max(logits)\n            exp_logits = np.exp(logits - max_logit)\n            probs = exp_logits / np.sum(exp_logits)\n            \n            # Cross-entropy for this token\n            loss += -np.log(probs[target_token] + 1e-10)</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_676c9430bff28",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>compute_instruction_loss_part_3()</code>?",
        "<pre><code>    \n    return loss / num_response_tokens if num_response_tokens > 0 else 0.0</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_efb02415f0742",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>create_instruction_dataset_part_1()</code>?",
        "<pre><code>def create_instruction_dataset(examples: List[Tuple[str, str]]) -> List[Dict]:\n    \"\"\"Create instruction tuning dataset.\"\"\"\n    dataset = []\n    \n    for instruction, response in examples:\n        formatted = format_instruction_data(instruction, response)\n        \n        # Simple tokenizer for demo\n        vocab = {word: i for i, word in enumerate(set(formatted.lower().split()))}\n        vocab['<UNK>'] = 0\n        \n        token_ids, inst_len = tokenize_instruction_response(formatted, vocab)\n        \n        dataset.append({\n            'formatted_text': formatted,\n            'token_ids': token_ids,\n            'instruction_length': inst_len,\n            'vocab': vocab\n        })</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_b448953c64dd4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>create_instruction_dataset_part_2()</code>?",
        "<pre><code>    \n    return dataset</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_1b5a377455517",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>evaluate_instruction_following_part_1()</code>?",
        "<pre><code>def evaluate_instruction_following(model_responses: List[str], \n                                 instructions: List[str]) -> Dict[str, float]:\n    \"\"\"Simple evaluation metrics for instruction following.\"\"\"\n    \n    # Instruction following metrics (simplified)\n    scores = {\n        'avg_response_length': np.mean([len(response.split()) for response in model_responses]),\n        'response_rate': sum(1 for response in model_responses if len(response.strip()) > 0) / len(model_responses),\n        'keyword_compliance': 0.0\n    }\n    \n    # Check if response contains key instruction words\n    keyword_matches = 0\n    total_keywords = 0\n    \n    for instruction, response in zip(instructions, model_responses):</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_bd304131050d1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>evaluate_instruction_following_part_2()</code>?",
        "<pre><code>        # Extract important words from instruction (simplified)\n        inst_words = set(instruction.lower().split())\n        resp_words = set(response.lower().split())\n        \n        # Remove common words\n        important_words = inst_words - {'the', 'a', 'an', 'is', 'are', 'and', 'or', 'but'}\n        \n        if important_words:\n            overlap = len(important_words & resp_words) / len(important_words)\n            keyword_matches += overlap\n            total_keywords += 1\n    \n    if total_keywords > 0:\n        scores['keyword_compliance'] = keyword_matches / total_keywords\n    \n    return scores</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_85cd43d698fc4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>sampling_strategies_part_1()</code>?",
        "<pre><code>def sampling_strategies(logits: np.ndarray, temperature: float = 1.0, \n                       top_k: int = None, top_p: float = None) -> int:\n    \"\"\"Implement different sampling strategies for text generation.\"\"\"\n    \n    # Apply temperature scaling\n    if temperature != 1.0:\n        logits = logits / temperature\n    \n    # Convert to probabilities\n    max_logit = np.max(logits)\n    exp_logits = np.exp(logits - max_logit)\n    probs = exp_logits / np.sum(exp_logits)\n    \n    # Top-k sampling\n    if top_k is not None:\n        top_k_indices = np.argsort(probs)[-top_k:]\n        masked_probs = np.zeros_like(probs)\n        masked_probs[top_k_indices] = probs[top_k_indices]\n        probs = masked_probs / np.sum(masked_probs)</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_f6086e25bfdbc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>sampling_strategies_part_2()</code>?",
        "<pre><code>    \n    # Top-p (nucleus) sampling  \n    if top_p is not None:\n        sorted_indices = np.argsort(probs)[::-1]\n        cumsum_probs = np.cumsum(probs[sorted_indices])\n        \n        # Find cutoff where cumulative probability exceeds top_p\n        cutoff_idx = np.argmax(cumsum_probs >= top_p) + 1\n        \n        # Keep only top-p tokens\n        top_p_indices = sorted_indices[:cutoff_idx]\n        masked_probs = np.zeros_like(probs)\n        masked_probs[top_p_indices] = probs[top_p_indices]\n        probs = masked_probs / np.sum(masked_probs)\n    </code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_68b0f2dc90e86",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>How do you implement <code>sampling_strategies_part_3()</code>?",
        "<pre><code>    # Sample from distribution\n    return np.random.choice(len(probs), p=probs)</code></pre>",
        "Instruction Tuning",
        "implementation"
      ],
      "guid": "nlp_74cb30efe9c26",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>response_logits = model_output[instruction_length:]</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_6fc4b56fadaf7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>logits = response_logits[i]</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_ae862aa290a0c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>max_logit = np.max(logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_9415f238f7b1c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>exp_logits = np.exp(logits - max_logit)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_128507bf43be3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>probs = exp_logits / np.sum(exp_logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_44447f56b686c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>loss += -np.log(probs[target_token] + 1e-10)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_ae41e52787fdc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>def sampling_strategies(logits: np.ndarray, temperature: float = 1.0,</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_7a995c0b2f3d8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>logits = logits / temperature</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_b47e440b541d5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>max_logit = np.max(logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_9415f238f7b1c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>exp_logits = np.exp(logits - max_logit)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_128507bf43be3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>probs = exp_logits / np.sum(exp_logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_44447f56b686c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>mock_logits = np.random.randn(seq_len, vocab_size)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_e844761b67f70",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>loss = compute_instruction_loss(mock_logits, mock_targets, instruction_len)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_f99b677940224",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>5 possible tokens</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_f5068f2319b1e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>Write the Key Formula",
        "<h3>token = sampling_strategies(sample_logits, **params)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Instruction Tuning",
        "formula"
      ],
      "guid": "nlp_1c73fb99b2355",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Instruction Tuning</b><br>🔥 Edge case: Empty Collection",
        "<pre><code>response_targets return 0.0</code></pre>",
        "Instruction Tuning",
        "edge_cases"
      ],
      "guid": "nlp_8e82fbeb2e9d2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "instruction_tuning",
        "edge_cases"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement text generation with llms?",
        "<b>Task:</b> Implement beam search for finding high-probability sequences.",
        "Llm Fundamentals",
        "problem_understanding"
      ],
      "guid": "nlp_309811b3439ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>softmax()</code>?",
        "<b>Purpose:</b> Convert logits to probabilities with temperature scaling<br><br><pre><code>def softmax(logits: List[float], temperature: float = 1.0) -> List[float]:\n    \"\"\"Convert logits to probabilities with temperature scaling.\"\"\"\n    if temperature != 1.0:\n        logits = [l / temperature for l in logits]\n    \n    max_logit = max(logits)\n    exp_logits = [math.exp(l - max_logit) for l in logits]\n    sum_exp = sum(exp_logits)\n    \n    return [exp_l / sum_exp for exp_l in exp_logits]</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_0b59073798d28",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>sample_token_part_1()</code>?",
        "<pre><code>def sample_token(probs: List[float], strategy: str = 'greedy', \n                top_k: int = 5, top_p: float = 0.9) -> int:\n    \"\"\"Sample next token using different strategies.\"\"\"\n    \n    if strategy == 'greedy':\n        return probs.index(max(probs))\n    \n    elif strategy == 'random':\n        # Random sampling from full distribution\n        return np.random.choice(len(probs), p=probs)\n    \n    elif strategy == 'top_k':\n        # Sample from top k tokens only\n        top_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:top_k]\n        top_probs = [probs[i] for i in top_indices]</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_e2c90baa7dc9f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>sample_token_part_2()</code>?",
        "<pre><code>        \n        # Renormalize\n        sum_top = sum(top_probs)\n        if sum_top > 0:\n            top_probs = [p / sum_top for p in top_probs]\n            selected_idx = np.random.choice(len(top_probs), p=top_probs)\n            return top_indices[selected_idx]\n        else:\n            return 0\n    \n    elif strategy == 'top_p':\n        # Nucleus sampling\n        sorted_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)\n        \n        cumsum = 0.0\n        nucleus_indices = []</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_1b0d38859d9f0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>sample_token_part_3()</code>?",
        "<pre><code>        \n        for idx in sorted_indices:\n            cumsum += probs[idx]\n            nucleus_indices.append(idx)\n            if cumsum >= top_p:\n                break\n        \n        # Sample from nucleus\n        nucleus_probs = [probs[i] for i in nucleus_indices]\n        sum_nucleus = sum(nucleus_probs)\n        \n        if sum_nucleus > 0:\n            nucleus_probs = [p / sum_nucleus for p in nucleus_probs]\n            selected_idx = np.random.choice(len(nucleus_probs), p=nucleus_probs)\n            return nucleus_indices[selected_idx]\n        else:\n            return 0</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_700b75ceee4ce",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>sample_token_part_4()</code>?",
        "<pre><code>    \n    return 0</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_50d974b4097dc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>generate_text_part_1()</code>?",
        "<pre><code>def generate_text(model_fn: Callable, prompt: str, vocab: Dict, \n                 max_length: int = 20, strategy: str = 'greedy',\n                 temperature: float = 1.0, **kwargs) -> str:\n    \"\"\"Generate text using autoregressive language model.\"\"\"\n    \n    # Convert prompt to tokens\n    prompt_tokens = [vocab.get(word.lower(), vocab.get('<UNK>', 0)) \n                    for word in prompt.split()]\n    \n    if not prompt_tokens:\n        prompt_tokens = [vocab.get('<START>', 0)]\n    \n    # Generate tokens iteratively\n    current_tokens = prompt_tokens.copy()\n    generated_words = prompt.split()</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_7c143f66b3118",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>generate_text_part_2()</code>?",
        "<pre><code>    \n    # Reverse vocabulary for decoding\n    id_to_word = {idx: word for word, idx in vocab.items()}\n    \n    for _ in range(max_length):\n        # Get next token logits from model\n        logits = model_fn(current_tokens)\n        \n        # Convert to probabilities\n        probs = softmax(logits, temperature)\n        \n        # Sample next token\n        next_token_id = sample_token(probs, strategy, \n                                   top_k=kwargs.get('top_k', 5),\n                                   top_p=kwargs.get('top_p', 0.9))</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_bb677fb1365ee",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>generate_text_part_3()</code>?",
        "<pre><code>        \n        # Convert to word\n        next_word = id_to_word.get(next_token_id, '<UNK>')\n        \n        # Stop if end token\n        if next_word in ['<END>', '<EOS>', '</s>']:\n            break\n        \n        # Add to sequence\n        current_tokens.append(next_token_id)\n        generated_words.append(next_word)\n    \n    return ' '.join(generated_words)</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_9202f4a6a6c59",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>beam_search_part_1()</code>?",
        "<pre><code>def beam_search(model_fn: Callable, prompt_tokens: List[int], \n               beam_width: int = 3, max_length: int = 10,\n               vocab: Dict = None) -> List[Tuple[List[int], float]]:\n    \"\"\"Implement beam search for finding high-probability sequences.\"\"\"\n    \n    # Initialize beam with prompt\n    # Each beam entry: (sequence, log_probability)\n    beams = [(prompt_tokens.copy(), 0.0)]\n    \n    for step in range(max_length):\n        new_beams = []\n        \n        for sequence, log_prob in beams:\n            # Get next token logits\n            logits = model_fn(sequence)\n            probs = softmax(logits)</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_b35a17b562728",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>beam_search_part_2()</code>?",
        "<pre><code>            \n            # Consider top beam_width tokens\n            top_indices = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:beam_width]\n            \n            for token_id in top_indices:\n                new_sequence = sequence + [token_id]\n                new_log_prob = log_prob + math.log(probs[token_id] + 1e-10)\n                \n                new_beams.append((new_sequence, new_log_prob))\n        \n        # Keep only top beam_width beams\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:beam_width]\n    \n    return beams</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_fba59d603f366",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>How do you implement <code>mock_language_model()</code>?",
        "<b>Purpose:</b> Mock language model that returns random logits<br><br><pre><code>def mock_language_model(tokens: List[int]) -> List[float]:\n    \"\"\"Mock language model that returns random logits.\"\"\"\n    vocab_size = 50\n    np.random.seed(sum(tokens) % 100)  # Deterministic based on input\n    return np.random.randn(vocab_size).tolist()</code></pre>",
        "Llm Fundamentals",
        "implementation"
      ],
      "guid": "nlp_d2ff042348835",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>def softmax(logits: List[float], temperature: float = 1.0) -> List[float]:</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_0cf1d14d61216",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>logits = [l / temperature for l in logits]</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_bedbf9d1ec8f2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>max_logit = max(logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_765e7f72ba49c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>exp_logits = [math.exp(l - max_logit) for l in logits]</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_4bead9231de72",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>sum_exp = sum(exp_logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_e366c77a9194f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>logits = model_fn(current_tokens)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_09faf8227af7b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>probs = softmax(logits, temperature)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_54b0cdade4934",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>logits = model_fn(sequence)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_aa3a35b9b6604",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>probs = softmax(logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_3c5e8ef055833",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>new_log_prob = log_prob + math.log(probs[token_id] + 1e-10)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_3e08419f8167e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>probability = math.exp(log_prob)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_0de0ac599255a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>Strong preference for token 0</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_0bec9edeefe8c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>probs = softmax(test_logits, temperature=temp)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_eeb5fc6f3d9cf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>Write the Key Formula",
        "<h3>entropy = -sum(p * math.log(p + 1e-10) for p in probs)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Llm Fundamentals",
        "formula"
      ],
      "guid": "nlp_c00d77a04d421",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>🔍 Why is this important in interviews?",
        "<p><strong>LLMs predict next token based on previous context</strong></p>",
        "Llm Fundamentals",
        "interview_insights"
      ],
      "guid": "nlp_e156e41a22b56",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Llm Fundamentals</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Temperature controls randomness in generation</strong></p>",
        "Llm Fundamentals",
        "interview_insights"
      ],
      "guid": "nlp_c6f2501cdcfd2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "llm_fundamentals",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement llm evaluation metrics?",
        "<b>Task:</b> Implement key evaluation metrics for large language models.",
        "Model Evaluation",
        "problem_understanding"
      ],
      "guid": "nlp_54baefca3256e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>calculate_perplexity_part_1()</code>?",
        "<pre><code>def calculate_perplexity(model_probs: List[List[float]], \n                        target_tokens: List[int]) -> float:\n    \"\"\"Calculate perplexity from model probabilities.\"\"\"\n    if not model_probs or not target_tokens:\n        return float('inf')\n    \n    if len(model_probs) != len(target_tokens):\n        return float('inf')\n    \n    log_likelihood = 0.0\n    num_tokens = 0\n    \n    for probs, target_token in zip(model_probs, target_tokens):\n        if target_token < len(probs):\n            # Get probability of target token\n            prob = probs[target_token]</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_bf8bb6aee0e20",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>calculate_perplexity_part_2()</code>?",
        "<pre><code>            \n            # Add log probability (with small epsilon to avoid log(0))\n            log_likelihood += math.log(max(prob, 1e-10))\n            num_tokens += 1\n    \n    if num_tokens == 0:\n        return float('inf')\n    \n    # Perplexity = exp(-avg_log_likelihood)\n    avg_log_likelihood = log_likelihood / num_tokens\n    perplexity = math.exp(-avg_log_likelihood)\n    \n    return perplexity</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_ccce2660c8331",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>get_ngrams()</code>?",
        "<b>Purpose:</b> Extract n-grams from text<br><br><pre><code>def get_ngrams(text: str, n: int) -> List[str]:\n    \"\"\"Extract n-grams from text.\"\"\"\n    words = text.lower().split()\n    ngrams = []\n    \n    for i in range(len(words) - n + 1):\n        ngram = ' '.join(words[i:i + n])\n        ngrams.append(ngram)\n    \n    return ngrams</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_3854a4e02c7bc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>compute_bleu_score_part_1()</code>?",
        "<pre><code>def compute_bleu_score(reference: str, candidate: str, n: int = 4) -> float:\n    \"\"\"Compute BLEU score for text generation evaluation.\"\"\"\n    \n    ref_words = reference.lower().split()\n    cand_words = candidate.lower().split()\n    \n    if not cand_words:\n        return 0.0\n    \n    # Brevity penalty\n    ref_len = len(ref_words)\n    cand_len = len(cand_words)\n    \n    if cand_len > ref_len:\n        bp = 1.0\n    else:\n        bp = math.exp(1 - ref_len / cand_len) if cand_len > 0 else 0.0</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_2cf5730dd1dd8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>compute_bleu_score_part_2()</code>?",
        "<pre><code>    \n    # Calculate n-gram precisions\n    precisions = []\n    \n    for i in range(1, n + 1):\n        ref_ngrams = Counter(get_ngrams(reference, i))\n        cand_ngrams = Counter(get_ngrams(candidate, i))\n        \n        if not cand_ngrams:\n            precisions.append(0.0)\n            continue\n        \n        # Count matches (with clipping for multiple references)\n        matches = 0\n        for ngram, count in cand_ngrams.items():\n            matches += min(count, ref_ngrams.get(ngram, 0))</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_d9f94e422fce3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>compute_bleu_score_part_3()</code>?",
        "<pre><code>        \n        precision = matches / sum(cand_ngrams.values())\n        precisions.append(precision)\n    \n    # Geometric mean of precisions\n    if all(p > 0 for p in precisions):\n        geometric_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n    else:\n        geometric_mean = 0.0\n    \n    return bp * geometric_mean</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_af9f35d51c236",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>calculate_cross_entropy_loss_part_1()</code>?",
        "<pre><code>def calculate_cross_entropy_loss(logits: List[List[float]], \n                               targets: List[int]) -> float:\n    \"\"\"Calculate cross-entropy loss from logits.\"\"\"\n    if not logits or not targets or len(logits) != len(targets):\n        return float('inf')\n    \n    total_loss = 0.0\n    \n    for logit_vec, target in zip(logits, targets):\n        if target < len(logit_vec):\n            # Softmax with numerical stability\n            max_logit = max(logit_vec)\n            exp_logits = [math.exp(logit - max_logit) for logit in logit_vec]\n            sum_exp = sum(exp_logits)\n            </code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_326a99a25197e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>calculate_cross_entropy_loss_part_2()</code>?",
        "<pre><code>            # Probability of target token\n            prob = exp_logits[target] / sum_exp\n            \n            # Cross-entropy\n            total_loss += -math.log(max(prob, 1e-10))\n    \n    return total_loss / len(targets)</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_9d841d2966c3d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>evaluate_generation_quality_part_1()</code>?",
        "<pre><code>def evaluate_generation_quality(references: List[str], \n                               candidates: List[str]) -> Dict[str, float]:\n    \"\"\"Comprehensive evaluation of generated text quality.\"\"\"\n    \n    if len(references) != len(candidates):\n        raise ValueError(\"Number of references and candidates must match\")\n    \n    # Calculate average BLEU scores\n    bleu_scores = []\n    for ref, cand in zip(references, candidates):\n        bleu = compute_bleu_score(ref, cand)\n        bleu_scores.append(bleu)\n    \n    # Calculate other metrics\n    length_ratios = []\n    for ref, cand in zip(references, candidates):\n        ref_len = len(ref.split())\n        cand_len = len(cand.split())\n        ratio = cand_len / ref_len if ref_len > 0 else 0\n        length_ratios.append(ratio)</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_31e30cbd747ae",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>How do you implement <code>evaluate_generation_quality_part_2()</code>?",
        "<pre><code>    \n    return {\n        'avg_bleu': sum(bleu_scores) / len(bleu_scores),\n        'avg_length_ratio': sum(length_ratios) / len(length_ratios),\n        'min_bleu': min(bleu_scores),\n        'max_bleu': max(bleu_scores)\n    }</code></pre>",
        "Model Evaluation",
        "implementation"
      ],
      "guid": "nlp_588ed00015812",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>log_likelihood = 0.0</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_2a9096fc2f56d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>log_likelihood += math.log(max(prob, 1e-10))</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_3fdbfcb3f3773",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>Perplexity = exp(-avg_log_likelihood)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_8fa36acad9fdf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>avg_log_likelihood = log_likelihood / num_tokens</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_d0600d019cbf5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>perplexity = math.exp(-avg_log_likelihood)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_1e235572096dc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>if not logits or not targets or len(logits) != len(targets):</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_ac8326cabacd5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>max_logit = max(logit_vec)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_c495cc44e9ab8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>exp_logits = [math.exp(logit - max_logit) for logit in logit_vec]</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_ca435a1394414",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>sum_exp = sum(exp_logits)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_df7ef7ba5e4f1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>prob = exp_logits[target] / sum_exp</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_9ce61fe1dbb8d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>total_loss += -math.log(max(prob, 1e-10))</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_694eb534e388f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>5 possible next tokens</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_9884514a87ffa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>Nearly deterministic</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_b0e3dd811cac7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>random_sample = sampling_strategies(logits, temperature=1.0)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_2e8eaefe080ea",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>top_k_sample = sampling_strategies(logits, temperature=1.0, top_k=3)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_11e4959157821",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>Write the Key Formula",
        "<h3>top_p_sample = sampling_strategies(logits, temperature=1.0, top_p=0.8)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Model Evaluation",
        "formula"
      ],
      "guid": "nlp_347ecf4e7c141",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Model Evaluation</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Compute BLEU score for text generation evaluation.</strong></p>",
        "Model Evaluation",
        "interview_insights"
      ],
      "guid": "nlp_85e21a2aba637",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "model_evaluation",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement named entity recognition with custom entities?",
        "<b>Task:</b> Implement `extract_entities(text: str) -> Dict[str, List[str]]` that:",
        "Ner",
        "problem_understanding"
      ],
      "guid": "nlp_18600fa216340",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_entities()</code>?",
        "<b>Purpose:</b> Extract named entities using spaCy<br><br><pre><code>def extract_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract named entities using spaCy.\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        entities[ent.label_].append(ent.text)\n    \n    # Remove duplicates while preserving order\n    for label in entities:\n        entities[label] = list(dict.fromkeys(entities[label]))\n    \n    return dict(entities)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_98985fc984ce2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_custom_entities_part_1()</code>?",
        "<pre><code>def extract_custom_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"Extract custom entities like emails, phones, URLs using regex.\"\"\"\n    entities = defaultdict(list)\n    \n    # Email pattern\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    entities['EMAIL'] = re.findall(email_pattern, text)\n    \n    # Phone pattern (US-style)\n    phone_pattern = r'\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b'\n    phones = re.findall(phone_pattern, text)\n    entities['PHONE'] = ['-'.join(groups) for groups in phones]\n    \n    # URL pattern\n    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&/=]*)'\n    entities['URL'] = re.findall(url_pattern, text)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_7027af4972c0c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_custom_entities_part_2()</code>?",
        "<pre><code>    \n    # Money pattern (simple)\n    money_pattern = r'\\$[\\d,]+\\.?\\d*|\\b\\d+\\s*(?:dollars?|cents?|USD|EUR|GBP)\\b'\n    entities['MONEY_CUSTOM'] = re.findall(money_pattern, text, re.IGNORECASE)\n    \n    # Remove empty categories\n    return {k: v for k, v in entities.items() if v}</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_7bc1f1c231b6b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_entities_with_context_part_1()</code>?",
        "<pre><code>def extract_entities_with_context(text: str, context_window: int = 30) -> Dict[str, List[Tuple[str, str]]]:\n    \"\"\"Extract entities with surrounding context.\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        # Get context\n        start = max(0, ent.start_char - context_window)\n        end = min(len(text), ent.end_char + context_window)\n        context = text[start:end].strip()\n        \n        # Mark entity in context\n        entity_start = ent.start_char - start\n        entity_end = ent.end_char - start\n        context_marked = (\n            context[:entity_start] + \n            f\"[{context[entity_start:entity_end]}]\" + \n            context[entity_end:]\n        )</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_ec0e451c8972e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_entities_with_context_part_2()</code>?",
        "<pre><code>        \n        entities[ent.label_].append((ent.text, context_marked))\n    \n    return dict(entities)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_2576938894492",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_entity_relationships_part_1()</code>?",
        "<pre><code>def extract_entity_relationships(text: str) -> List[Tuple[str, str, str]]:\n    \"\"\"Extract simple relationships between entities.\"\"\"\n    doc = nlp(text)\n    relationships = []\n    \n    # Simple pattern: PERSON + verb + ORG\n    for sent in doc.sents:\n        entities_in_sent = [(ent.text, ent.label_, ent.start, ent.end) for ent in sent.ents]\n        \n        # Look for patterns\n        for i, (ent1_text, ent1_label, _, _) in enumerate(entities_in_sent):\n            for j, (ent2_text, ent2_label, _, _) in enumerate(entities_in_sent[i+1:], i+1):\n                # Extract verb between entities\n                if ent1_label == \"PERSON\" and ent2_label == \"ORG\":\n                    # Find verb between entities\n                    between_tokens = sent[entities_in_sent[i][3]:entities_in_sent[j][2]]\n                    verbs = [token.text for token in between_tokens if token.pos_ == \"VERB\"]\n                    if verbs:\n                        relationships.append((ent1_text, verbs[0], ent2_text))</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_f62a1f01c152c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_entity_relationships_part_2()</code>?",
        "<pre><code>    \n    return relationships</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_0914d3f5e04ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>resolve_entity_coreferences_part_1()</code>?",
        "<pre><code>def resolve_entity_coreferences(text: str) -> Dict[str, List[str]]:\n    \"\"\"Simple coreference resolution for entities (e.g., 'Apple' -> 'Apple Inc.')\"\"\"\n    doc = nlp(text)\n    entities = extract_entities(text)\n    \n    # Simple heuristic: map shorter versions to longer versions\n    entity_mapping = {}\n    \n    for label, ent_list in entities.items():\n        # Sort by length\n        sorted_ents = sorted(ent_list, key=len, reverse=True)\n        for i, longer in enumerate(sorted_ents):\n            for shorter in sorted_ents[i+1:]:\n                if shorter.lower() in longer.lower() and shorter != longer:\n                    entity_mapping[shorter] = longer</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_344e277550dcc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>resolve_entity_coreferences_part_2()</code>?",
        "<pre><code>    \n    # Apply mapping\n    resolved = defaultdict(set)\n    for label, ent_list in entities.items():\n        for ent in ent_list:\n            canonical = entity_mapping.get(ent, ent)\n            resolved[label].add(canonical)\n    \n    return {k: list(v) for k, v in resolved.items()}</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_58b991fe0cfdc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_entities_with_confidence_part_1()</code>?",
        "<pre><code>def extract_entities_with_confidence(text: str) -> Dict[str, List[Tuple[str, float]]]:\n    \"\"\"Extract entities with confidence scores (using spaCy's scores if available).\"\"\"\n    doc = nlp(text)\n    entities = defaultdict(list)\n    \n    for ent in doc.ents:\n        # SpaCy doesn't always provide confidence, so we'll simulate\n        # In practice, you'd use a model that provides confidence scores\n        confidence = 0.95 if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"] else 0.85\n        \n        # Lower confidence for single-word entities\n        if len(ent.text.split()) == 1:\n            confidence *= 0.9\n            \n        entities[ent.label_].append((ent.text, confidence))</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_5494aca52503f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>How do you implement <code>extract_entities_with_confidence_part_2()</code>?",
        "<pre><code>    \n    return dict(entities)</code></pre>",
        "Ner",
        "implementation"
      ],
      "guid": "nlp_38ccf2dc29aa1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Simple coreference resolution for entities (e.g., 'Apple' -> 'Apple Inc.')</strong></p>",
        "Ner",
        "interview_insights"
      ],
      "guid": "nlp_e9c4ab0be5066",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ner</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Extract entities with confidence scores (using spaCy's scores if available).</strong></p>",
        "Ner",
        "interview_insights"
      ],
      "guid": "nlp_2f2bb18301fc0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ner",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>How do you implement n-gram language model?",
        "<b>Task:</b> Build bigram language model.",
        "Ngrams",
        "problem_understanding"
      ],
      "guid": "nlp_051456577341f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>How do you implement <code>build_bigram_model_part_1()</code>?",
        "<pre><code>def build_bigram_model(texts: List[str]) -> Dict[str, Dict[str, float]]:\n    \"\"\"Build bigram language model with probabilities.\"\"\"\n    if not texts:\n        return {}\n    \n    # Count bigrams\n    bigram_counts = defaultdict(Counter)\n    \n    for text in texts:\n        words = text.lower().split()\n        \n        # Add start token\n        words = ['<START>'] + words + ['<END>']\n        \n        # Count bigrams\n        for i in range(len(words) - 1):\n            w1, w2 = words[i], words[i + 1]\n            bigram_counts[w1][w2] += 1</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_e44756cdc4693",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>How do you implement <code>build_bigram_model_part_2()</code>?",
        "<pre><code>    \n    # Convert counts to probabilities\n    model = {}\n    for w1, w2_counts in bigram_counts.items():\n        total = sum(w2_counts.values())\n        model[w1] = {w2: count/total for w2, count in w2_counts.items()}\n    \n    return model</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_5560a4816ff72",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>How do you implement <code>generate_text_part_1()</code>?",
        "<pre><code>def generate_text(model: Dict, start_word: str = '<START>', length: int = 5) -> str:\n    \"\"\"Generate text using bigram model (deterministic - pick most probable).\"\"\"\n    if start_word not in model:\n        return \"\"\n    \n    words = []\n    current_word = start_word\n    \n    for _ in range(length):\n        if current_word not in model or not model[current_word]:\n            break\n        \n        # Pick most probable next word\n        next_word = max(model[current_word], key=model[current_word].get)\n        \n        if next_word == '<END>':\n            break</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_e42fffea9da20",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>How do you implement <code>generate_text_part_2()</code>?",
        "<pre><code>        \n        if next_word != '<START>':\n            words.append(next_word)\n        \n        current_word = next_word\n    \n    return ' '.join(words)</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_e83aa1b241caa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Ngrams</b><br>How do you implement <code>calculate_probability()</code>?",
        "<b>Purpose:</b> Calculate probability of text under the model<br><br><pre><code>def calculate_probability(model: Dict, text: str) -> float:\n    \"\"\"Calculate probability of text under the model.\"\"\"\n    words = ['<START>'] + text.lower().split() + ['<END>']\n    \n    prob = 1.0\n    for i in range(len(words) - 1):\n        w1, w2 = words[i], words[i + 1]\n        \n        if w1 in model and w2 in model[w1]:\n            prob *= model[w1][w2]\n        else:\n            return 0.0  # Unseen bigram\n    \n    return prob</code></pre>",
        "Ngrams",
        "implementation"
      ],
      "guid": "nlp_67c27b6405bbf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "ngrams",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement neural network from scratch?",
        "<b>Task:</b> Implement basic neural networks from scratch:",
        "Neural Fundamentals",
        "problem_understanding"
      ],
      "guid": "nlp_143736480e25b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>gradient_check_part_1()</code>?",
        "<pre><code>def gradient_check(network: NeuralNetwork, X: np.ndarray, y: np.ndarray, epsilon: float = 1e-7) -> float:\n    \"\"\"Perform gradient checking to verify backpropagation implementation.\"\"\"\n    # Get gradients from backpropagation\n    y_pred = network.forward(X)\n    network.backward(X, y, y_pred)\n    \n    # Store analytical gradients\n    analytical_gradients = {}\n    for i in range(1, network.num_layers):\n        # Note: This is simplified - in practice you'd store gradients during backward pass\n        pass\n    \n    # Compute numerical gradients\n    numerical_gradients = {}\n    \n    for i in range(1, network.num_layers):\n        w_key = f'W{i}'\n        b_key = f'b{i}'</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_4480c6ee30626",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>gradient_check_part_2()</code>?",
        "<pre><code>        \n        # Check weights\n        w_shape = network.weights[w_key].shape\n        numerical_gradients[w_key] = np.zeros(w_shape)\n        \n        for idx in np.ndindex(w_shape):\n            # Perturb weight\n            network.weights[w_key][idx] += epsilon\n            y_pred_plus = network.forward(X)\n            cost_plus = network.compute_cost(y, y_pred_plus)\n            \n            network.weights[w_key][idx] -= 2 * epsilon\n            y_pred_minus = network.forward(X)\n            cost_minus = network.compute_cost(y, y_pred_minus)\n            </code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_719a0fe67cea1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>gradient_check_part_3()</code>?",
        "<pre><code>            # Restore weight\n            network.weights[w_key][idx] += epsilon\n            \n            # Compute numerical gradient\n            numerical_gradients[w_key][idx] = (cost_plus - cost_minus) / (2 * epsilon)\n    \n    print(\"Gradient checking completed (simplified version)\")\n    return 0.0  # Would return actual difference in real implementation</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_e530fec1da8a4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>generate_xor_data()</code>?",
        "<b>Purpose:</b> Generate XOR problem data<br><br><pre><code>def generate_xor_data() -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate XOR problem data.\"\"\"\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n    y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n    return X, y</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_f43f3a405cc7b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>generate_spiral_data()</code>?",
        "<b>Purpose:</b> Generate spiral classification data<br><br><pre><code>def generate_spiral_data(n_samples: int = 100, n_classes: int = 3) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate spiral classification data.\"\"\"\n    X = np.zeros((n_samples * n_classes, 2))\n    y = np.zeros((n_samples * n_classes, n_classes))\n    \n    for class_idx in range(n_classes):\n        ix = range(n_samples * class_idx, n_samples * (class_idx + 1))\n        r = np.linspace(0.0, 1, n_samples)  # radius\n        t = np.linspace(class_idx * 4, (class_idx + 1) * 4, n_samples) + np.random.randn(n_samples) * 0.2\n        \n        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        y[ix, class_idx] = 1\n    \n    return X, y</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_b4069f41198c8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>plot_decision_boundary_part_1()</code>?",
        "<pre><code>def plot_decision_boundary(network: NeuralNetwork, X: np.ndarray, y: np.ndarray, title: str = \"Decision Boundary\"):\n    \"\"\"Plot decision boundary for 2D data.\"\"\"\n    try:\n        h = 0.01\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        \n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                           np.arange(y_min, y_max, h))\n        \n        mesh_points = np.c_[xx.ravel(), yy.ravel()]\n        Z = network.predict_proba(mesh_points)\n        \n        if Z.shape[1] > 1:\n            Z = np.argmax(Z, axis=1)\n        else:\n            Z = Z.ravel()</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_e8478c3a903f7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>plot_decision_boundary_part_2()</code>?",
        "<pre><code>        \n        Z = Z.reshape(xx.shape)\n        \n        plt.figure(figsize=(10, 8))\n        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n        \n        # Plot data points\n        if y.shape[1] > 1:\n            y_labels = np.argmax(y, axis=1)\n        else:\n            y_labels = y.ravel()\n        \n        scatter = plt.scatter(X[:, 0], X[:, 1], c=y_labels, cmap=plt.cm.RdYlBu, edgecolors='black')\n        plt.colorbar(scatter)\n        plt.title(title)\n        plt.xlabel('X1')\n        plt.ylabel('X2')\n        plt.show()\n    except ImportError:\n        print(\"Matplotlib not available for plotting\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_55d0e16084d39",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>sigmoid()</code>?",
        "<b>Purpose:</b> Sigmoid activation function<br><br><pre><code>    def sigmoid(x: np.ndarray) -> np.ndarray:\n        \"\"\"Sigmoid activation function.\"\"\"\n        # Prevent overflow\n        x = np.clip(x, -500, 500)\n        return 1 / (1 + np.exp(-x))</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_ade95177c8914",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>sigmoid_derivative()</code>?",
        "<b>Purpose:</b> Derivative of sigmoid function<br><br><pre><code>    def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n        \"\"\"Derivative of sigmoid function.\"\"\"\n        s = ActivationFunctions.sigmoid(x)\n        return s * (1 - s)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_25f73f2e15665",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>tanh()</code>?",
        "<b>Purpose:</b> Hyperbolic tangent activation function<br><br><pre><code>    def tanh(x: np.ndarray) -> np.ndarray:\n        \"\"\"Hyperbolic tangent activation function.\"\"\"\n        return np.tanh(x)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_9123d713b8f3d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>tanh_derivative()</code>?",
        "<b>Purpose:</b> Derivative of tanh function<br><br><pre><code>    def tanh_derivative(x: np.ndarray) -> np.ndarray:\n        \"\"\"Derivative of tanh function.\"\"\"\n        return 1 - np.tanh(x) ** 2</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_471b15d0253d0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>relu()</code>?",
        "<b>Purpose:</b> ReLU activation function<br><br><pre><code>    def relu(x: np.ndarray) -> np.ndarray:\n        \"\"\"ReLU activation function.\"\"\"\n        return np.maximum(0, x)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_9c518e40b796e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>relu_derivative()</code>?",
        "<b>Purpose:</b> Derivative of ReLU function<br><br><pre><code>    def relu_derivative(x: np.ndarray) -> np.ndarray:\n        \"\"\"Derivative of ReLU function.\"\"\"\n        return np.where(x > 0, 1, 0)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_c894d022f536c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>leaky_relu()</code>?",
        "<b>Purpose:</b> Leaky ReLU activation function<br><br><pre><code>    def leaky_relu(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n        \"\"\"Leaky ReLU activation function.\"\"\"\n        return np.where(x > 0, x, alpha * x)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_9f624d6f107cf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>leaky_relu_derivative()</code>?",
        "<b>Purpose:</b> Derivative of Leaky ReLU function<br><br><pre><code>    def leaky_relu_derivative(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n        \"\"\"Derivative of Leaky ReLU function.\"\"\"\n        return np.where(x > 0, 1, alpha)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_26c903ec287a6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>softmax()</code>?",
        "<b>Purpose:</b> Softmax activation function<br><br><pre><code>    def softmax(x: np.ndarray) -> np.ndarray:\n        \"\"\"Softmax activation function.\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_b2ba4362bf440",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>__init__()</code>?",
        "<pre><code>    def __init__(self, input_size: int, learning_rate: float = 0.01, \n                 activation: str = 'sigmoid'):\n        self.input_size = input_size\n        self.learning_rate = learning_rate\n        self.activation = activation\n        \n        # Initialize weights and bias\n        self.weights = np.random.randn(input_size) * 0.5\n        self.bias = 0.0\n        \n        # Set activation function\n        self.activation_func, self.activation_derivative = self._get_activation_functions(activation)\n        \n        # Training history\n        self.loss_history = []</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_a7e2539225da2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>_get_activation_functions()</code>?",
        "<b>Purpose:</b> Get activation function and its derivative<br><br><pre><code>    def _get_activation_functions(self, activation: str) -> Tuple[Callable, Callable]:\n        \"\"\"Get activation function and its derivative.\"\"\"\n        if activation == 'sigmoid':\n            return ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative\n        elif activation == 'tanh':\n            return ActivationFunctions.tanh, ActivationFunctions.tanh_derivative\n        elif activation == 'relu':\n            return ActivationFunctions.relu, ActivationFunctions.relu_derivative\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_d91fa8372a903",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>forward()</code>?",
        "<b>Purpose:</b> Forward propagation<br><br><pre><code>    def forward(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Forward propagation.\"\"\"\n        self.z = np.dot(X, self.weights) + self.bias\n        self.a = self.activation_func(self.z)\n        return self.a</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_85a9455c3bd90",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>compute_cost()</code>?",
        "<b>Purpose:</b> Compute binary cross-entropy loss<br><br><pre><code>    def compute_cost(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n        \"\"\"Compute binary cross-entropy loss.\"\"\"\n        # Avoid log(0)\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_6d5277a87c40f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>backward()</code>?",
        "<b>Purpose:</b> Backward propagation<br><br><pre><code>    def backward(self, X: np.ndarray, y_true: np.ndarray, y_pred: np.ndarray):\n        \"\"\"Backward propagation.\"\"\"\n        m = X.shape[0]\n        \n        # Compute gradients\n        dz = y_pred - y_true\n        dw = (1/m) * np.dot(X.T, dz)\n        db = (1/m) * np.sum(dz)\n        \n        # Update weights and bias\n        self.weights -= self.learning_rate * dw\n        self.bias -= self.learning_rate * db</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_2c5b66db472ca",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>train()</code>?",
        "<b>Purpose:</b> Train the perceptron<br><br><pre><code>    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000):\n        \"\"\"Train the perceptron.\"\"\"\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X)\n            \n            # Compute cost\n            cost = self.compute_cost(y, y_pred)\n            self.loss_history.append(cost)\n            \n            # Backward pass\n            self.backward(X, y, y_pred)\n            \n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Cost: {cost:.4f}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_bbd92e45e3ff1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>predict()</code>?",
        "<b>Purpose:</b> Make predictions<br><br><pre><code>    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Make predictions.\"\"\"\n        probabilities = self.forward(X)\n        return (probabilities > 0.5).astype(int)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_aa543c60fb5d7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>__init___part_1()</code>?",
        "<pre><code>    def __init__(self, layers: List[int], activation: str = 'sigmoid', \n                 output_activation: str = 'sigmoid', learning_rate: float = 0.01):\n        self.layers = layers\n        self.num_layers = len(layers)\n        self.learning_rate = learning_rate\n        self.activation = activation\n        self.output_activation = output_activation\n        \n        # Initialize weights and biases\n        self.weights = {}\n        self.biases = {}\n        \n        for i in range(1, self.num_layers):\n            # Xavier/Glorot initialization\n            self.weights[f'W{i}'] = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2.0 / layers[i-1])\n            self.biases[f'b{i}'] = np.zeros((1, layers[i]))</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_2859beda45684",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>__init___part_2()</code>?",
        "<pre><code>        \n        # Get activation functions\n        self.hidden_activation, self.hidden_activation_derivative = self._get_activation_functions(activation)\n        self.out_activation, self.out_activation_derivative = self._get_activation_functions(output_activation)\n        \n        # Training history\n        self.loss_history = []\n        self.accuracy_history = []\n        \n        # Cache for forward propagation\n        self.cache = {}</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_b8e7be5eedcbc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>_get_activation_functions()</code>?",
        "<b>Purpose:</b> Get activation function and its derivative<br><br><pre><code>    def _get_activation_functions(self, activation: str) -> Tuple[Callable, Callable]:\n        \"\"\"Get activation function and its derivative.\"\"\"\n        if activation == 'sigmoid':\n            return ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative\n        elif activation == 'tanh':\n            return ActivationFunctions.tanh, ActivationFunctions.tanh_derivative\n        elif activation == 'relu':\n            return ActivationFunctions.relu, ActivationFunctions.relu_derivative\n        elif activation == 'softmax':\n            return ActivationFunctions.softmax, lambda x: x  # Derivative handled separately\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_3e91a129b3cc9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>forward_part_1()</code>?",
        "<pre><code>    def forward(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Forward propagation through the network.\"\"\"\n        self.cache['A0'] = X\n        \n        # Hidden layers\n        for i in range(1, self.num_layers - 1):\n            Z = np.dot(self.cache[f'A{i-1}'], self.weights[f'W{i}']) + self.biases[f'b{i}']\n            A = self.hidden_activation(Z)\n            \n            self.cache[f'Z{i}'] = Z\n            self.cache[f'A{i}'] = A\n        \n        # Output layer\n        i = self.num_layers - 1\n        Z = np.dot(self.cache[f'A{i-1}'], self.weights[f'W{i}']) + self.biases[f'b{i}']</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_a223ee7813a53",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>forward_part_2()</code>?",
        "<pre><code>        \n        if self.output_activation == 'softmax':\n            A = self.out_activation(Z)\n        else:\n            A = self.out_activation(Z)\n        \n        self.cache[f'Z{i}'] = Z\n        self.cache[f'A{i}'] = A\n        \n        return A</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_4b16f514ddc00",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>compute_cost_part_1()</code>?",
        "<pre><code>    def compute_cost(self, y_true: np.ndarray, y_pred: np.ndarray, cost_type: str = 'cross_entropy') -> float:\n        \"\"\"Compute cost function.\"\"\"\n        m = y_true.shape[0]\n        \n        if cost_type == 'cross_entropy':\n            if y_true.shape[1] > 1:  # Multi-class\n                # Categorical cross-entropy\n                y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n                return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n            else:  # Binary\n                # Binary cross-entropy\n                y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n                return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        \n        elif cost_type == 'mse':\n            return np.mean((y_true - y_pred) ** 2)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_4eb628ca250ba",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>compute_cost_part_2()</code>?",
        "<pre><code>        \n        else:\n            raise ValueError(f\"Unsupported cost type: {cost_type}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_ef0260b8c93f8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>backward_part_1()</code>?",
        "<pre><code>    def backward(self, X: np.ndarray, y_true: np.ndarray, y_pred: np.ndarray):\n        \"\"\"Backward propagation through the network.\"\"\"\n        m = X.shape[0]\n        gradients = {}\n        \n        # Output layer gradient\n        if self.output_activation == 'softmax' and y_true.shape[1] > 1:\n            # Softmax with categorical cross-entropy\n            dZ = y_pred - y_true\n        else:\n            # Other activations\n            dA = -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)\n            dZ = dA * self.out_activation_derivative(self.cache[f'Z{self.num_layers-1}'])\n        \n        # Gradients for output layer\n        i = self.num_layers - 1\n        gradients[f'dW{i}'] = (1/m) * np.dot(self.cache[f'A{i-1}'].T, dZ)\n        gradients[f'db{i}'] = (1/m) * np.sum(dZ, axis=0, keepdims=True)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_f7a9fa7458aa1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>backward_part_2()</code>?",
        "<pre><code>        \n        # Propagate backwards through hidden layers\n        dA_prev = np.dot(dZ, self.weights[f'W{i}'].T)\n        \n        for i in range(self.num_layers - 2, 0, -1):\n            dZ = dA_prev * self.hidden_activation_derivative(self.cache[f'Z{i}'])\n            \n            gradients[f'dW{i}'] = (1/m) * np.dot(self.cache[f'A{i-1}'].T, dZ)\n            gradients[f'db{i}'] = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n            \n            if i > 1:\n                dA_prev = np.dot(dZ, self.weights[f'W{i}'].T)\n        \n        # Update weights and biases\n        for i in range(1, self.num_layers):\n            self.weights[f'W{i}'] -= self.learning_rate * gradients[f'dW{i}']\n            self.biases[f'b{i}'] -= self.learning_rate * gradients[f'db{i}']</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_75c2072359685",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>train_part_1()</code>?",
        "<pre><code>    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, \n              cost_type: str = 'cross_entropy', verbose: bool = True):\n        \"\"\"Train the neural network.\"\"\"\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X)\n            \n            # Compute cost\n            cost = self.compute_cost(y, y_pred, cost_type)\n            self.loss_history.append(cost)\n            \n            # Compute accuracy\n            accuracy = self.compute_accuracy(y, y_pred)\n            self.accuracy_history.append(accuracy)\n            </code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_b435c1a217fb5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>train_part_2()</code>?",
        "<pre><code>            # Backward pass\n            self.backward(X, y, y_pred)\n            \n            if verbose and epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Cost: {cost:.4f}, Accuracy: {accuracy:.4f}\")</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_2eeb6631bcbaf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>predict()</code>?",
        "<b>Purpose:</b> Make predictions<br><br><pre><code>    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Make predictions.\"\"\"\n        y_pred = self.forward(X)\n        \n        if y_pred.shape[1] > 1:  # Multi-class\n            return np.argmax(y_pred, axis=1)\n        else:  # Binary\n            return (y_pred > 0.5).astype(int)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_f38ab97cb2587",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>predict_proba()</code>?",
        "<b>Purpose:</b> Get prediction probabilities<br><br><pre><code>    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Get prediction probabilities.\"\"\"\n        return self.forward(X)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_a1648ab5f5803",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>compute_accuracy()</code>?",
        "<b>Purpose:</b> Compute accuracy<br><br><pre><code>    def compute_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n        \"\"\"Compute accuracy.\"\"\"\n        if y_true.shape[1] > 1:  # Multi-class\n            y_true_labels = np.argmax(y_true, axis=1)\n            y_pred_labels = np.argmax(y_pred, axis=1)\n        else:  # Binary\n            y_true_labels = y_true.flatten()\n            y_pred_labels = (y_pred > 0.5).astype(int).flatten()\n        \n        return np.mean(y_true_labels == y_pred_labels)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_a31d2d815b000",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>__init__()</code>?",
        "<pre><code>    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, \n                 beta2: float = 0.999, epsilon: float = 1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        \n        self.m_weights = {}\n        self.v_weights = {}\n        self.m_biases = {}\n        self.v_biases = {}\n        self.t = 0</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_d4999f4932047",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>update_part_1()</code>?",
        "<pre><code>    def update(self, network: NeuralNetwork, gradients: dict):\n        \"\"\"Update network parameters using Adam optimizer.\"\"\"\n        self.t += 1\n        \n        for i in range(1, network.num_layers):\n            w_key = f'W{i}'\n            b_key = f'b{i}'\n            dw_key = f'dW{i}'\n            db_key = f'db{i}'\n            \n            # Initialize momentum terms if first iteration\n            if w_key not in self.m_weights:\n                self.m_weights[w_key] = np.zeros_like(network.weights[w_key])\n                self.v_weights[w_key] = np.zeros_like(network.weights[w_key])\n                self.m_biases[b_key] = np.zeros_like(network.biases[b_key])\n                self.v_biases[b_key] = np.zeros_like(network.biases[b_key])</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_0ef870225b57e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>update_part_2()</code>?",
        "<pre><code>            \n            # Update momentum terms for weights\n            self.m_weights[w_key] = self.beta1 * self.m_weights[w_key] + (1 - self.beta1) * gradients[dw_key]\n            self.v_weights[w_key] = self.beta2 * self.v_weights[w_key] + (1 - self.beta2) * (gradients[dw_key] ** 2)\n            \n            # Update momentum terms for biases\n            self.m_biases[b_key] = self.beta1 * self.m_biases[b_key] + (1 - self.beta1) * gradients[db_key]\n            self.v_biases[b_key] = self.beta2 * self.v_biases[b_key] + (1 - self.beta2) * (gradients[db_key] ** 2)\n            \n            # Bias correction\n            m_w_corrected = self.m_weights[w_key] / (1 - self.beta1 ** self.t)\n            v_w_corrected = self.v_weights[w_key] / (1 - self.beta2 ** self.t)\n            m_b_corrected = self.m_biases[b_key] / (1 - self.beta1 ** self.t)\n            v_b_corrected = self.v_biases[b_key] / (1 - self.beta2 ** self.t)\n            </code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_9514d9b5ed414",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>How do you implement <code>update_part_3()</code>?",
        "<pre><code>            # Update parameters\n            network.weights[w_key] -= self.learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + self.epsilon)\n            network.biases[b_key] -= self.learning_rate * m_b_corrected / (np.sqrt(v_b_corrected) + self.epsilon)</code></pre>",
        "Neural Fundamentals",
        "implementation"
      ],
      "guid": "nlp_cec6376878d1a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Write the Key Formula",
        "<h3>elif activation == 'softmax':</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Neural Fundamentals",
        "formula"
      ],
      "guid": "nlp_fcf9119d5dbe3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Write the Key Formula",
        "<h3>if self.output_activation == 'softmax':</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Neural Fundamentals",
        "formula"
      ],
      "guid": "nlp_544d340740818",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Write the Key Formula",
        "<h3>return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Neural Fundamentals",
        "formula"
      ],
      "guid": "nlp_3789f7cafacb9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Neural Fundamentals</b><br>Write the Key Formula",
        "<h3>if self.output_activation == 'softmax' and y_true.shape[1] > 1:</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Neural Fundamentals",
        "formula"
      ],
      "guid": "nlp_40f4ee65b5ff2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "neural_fundamentals",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement part-of-speech tagging with accuracy metrics?",
        "<b>Task:</b> Implement `pos_tag_text(text: str) -> List[Tuple[str, str]]` that:",
        "Pos Tagging",
        "problem_understanding"
      ],
      "guid": "nlp_b8945f35008a9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>pos_tag_text()</code>?",
        "<b>Purpose:</b> POS tag text using NLTK's default tagger (Penn Treebank tagset)<br><br><pre><code>def pos_tag_text(text: str) -> List[Tuple[str, str]]:\n    \"\"\"POS tag text using NLTK's default tagger (Penn Treebank tagset).\"\"\"\n    tokens = nltk.word_tokenize(text)\n    return nltk.pos_tag(tokens)</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_6099dfef9c0b0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>extract_pos()</code>?",
        "<b>Purpose:</b> Extract words with specific POS tags<br><br><pre><code>def extract_pos(tagged_text: List[Tuple[str, str]], pos_prefix: str) -> List[str]:\n    \"\"\"Extract words with specific POS tags.\n    \n    Args:\n        tagged_text: List of (word, pos) tuples\n        pos_prefix: POS tag prefix (e.g., 'NN' for nouns, 'VB' for verbs)\n    \"\"\"\n    return [word for word, pos in tagged_text if pos.startswith(pos_prefix)]</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_0878c1d83316a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>find_noun_phrases_part_1()</code>?",
        "<pre><code>def find_noun_phrases(tagged_text: List[Tuple[str, str]]) -> List[str]:\n    \"\"\"Extract simple noun phrases using POS patterns.\"\"\"\n    noun_phrases = []\n    current_phrase = []\n    \n    # Simple pattern: (DT)? (JJ)* (NN)+\n    for word, pos in tagged_text:\n        if pos == 'DT':  # Determiner\n            if current_phrase:\n                noun_phrases.append(' '.join(current_phrase))\n            current_phrase = [word]\n        elif pos.startswith('JJ'):  # Adjective\n            if current_phrase:\n                current_phrase.append(word)\n        elif pos.startswith('NN'):  # Noun\n            current_phrase.append(word)\n        else:\n            if current_phrase and any(pos.startswith('NN') for _, pos in \n                                    [(w, p) for w, p in zip(current_phrase, \n                                     [t[1] for t in tagged_text])]):\n                noun_phrases.append(' '.join(current_phrase))\n            current_phrase = []</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_2b2e591f424ed",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>find_noun_phrases_part_2()</code>?",
        "<pre><code>    \n    if current_phrase:\n        noun_phrases.append(' '.join(current_phrase))\n    \n    return noun_phrases</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_c363f3231ef14",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>analyze_word_ambiguity_part_1()</code>?",
        "<pre><code>def analyze_word_ambiguity(word: str, corpus_name: str = 'brown') -> Dict[str, float]:\n    \"\"\"Analyze POS tag distribution for an ambiguous word.\"\"\"\n    from nltk.corpus import brown\n    \n    # Get all occurrences of the word with their tags\n    word_lower = word.lower()\n    pos_counts = Counter()\n    \n    for sent in brown.tagged_sents(tagset='universal')[:10000]:  # Sample for speed\n        for token, pos in sent:\n            if token.lower() == word_lower:\n                pos_counts[pos] += 1\n    \n    total = sum(pos_counts.values())\n    if total == 0:\n        return {}</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_f66088e36464a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>analyze_word_ambiguity_part_2()</code>?",
        "<pre><code>    \n    return {pos: count/total for pos, count in pos_counts.items()}</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_ccc6d7a10634e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>compare_taggers_part_1()</code>?",
        "<pre><code>def compare_taggers(text: str) -> Dict[str, List[Tuple[str, str]]]:\n    \"\"\"Compare different POS taggers on the same text.\"\"\"\n    tokens = nltk.word_tokenize(text)\n    \n    results = {\n        'default': nltk.pos_tag(tokens),\n        'universal': nltk.pos_tag(tokens, tagset='universal')\n    }\n    \n    # Try to use spaCy if available\n    try:\n        import spacy\n        nlp = spacy.load('en_core_web_sm')\n        doc = nlp(text)\n        results['spacy'] = [(token.text, token.pos_) for token in doc]\n    except:\n        pass</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_415e0da3dcf9c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>compare_taggers_part_2()</code>?",
        "<pre><code>    \n    return results</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_1c5c5bb4f18ff",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>pos_tag_with_confidence_part_1()</code>?",
        "<pre><code>def pos_tag_with_confidence(text: str) -> List[Tuple[str, str, float]]:\n    \"\"\"POS tag with confidence scores (simplified version).\"\"\"\n    # This is a simplified demo - real confidence would come from the model\n    tagged = pos_tag_text(text)\n    \n    # Add mock confidence based on word frequency/ambiguity\n    result = []\n    for word, pos in tagged:\n        # Common unambiguous words get high confidence\n        if pos in ['DT', 'IN', 'CC', '.', ',']:\n            confidence = 0.99\n        # Potentially ambiguous words get lower confidence\n        elif word.lower() in ['run', 'meeting', 'light', 'bank']:\n            confidence = 0.75\n        else:\n            confidence = 0.90</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_a73d9b04fb27d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>How do you implement <code>pos_tag_with_confidence_part_2()</code>?",
        "<pre><code>        \n        result.append((word, pos, confidence))\n    \n    return result</code></pre>",
        "Pos Tagging",
        "implementation"
      ],
      "guid": "nlp_b93132994ad1a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Pos Tagging</b><br>🔍 Why is this important in interviews?",
        "<p><strong>POS tag with confidence scores (simplified version).</strong></p>",
        "Pos Tagging",
        "interview_insights"
      ],
      "guid": "nlp_6d447e4475daf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "pos_tagging",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement regular expressions for nlp?",
        "<b>Task:</b> Implement regex-based NLP functions:",
        "Regex Nlp",
        "problem_understanding"
      ],
      "guid": "nlp_2ab6449687a2c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_entities_regex_part_1()</code>?",
        "<pre><code>def extract_entities_regex(text: str, \n                          custom_patterns: Optional[Dict[str, str]] = None) -> Dict[str, List[str]]:\n    \"\"\"Extract various entities using regex patterns.\"\"\"\n    entities = defaultdict(list)\n    \n    # Use default patterns plus any custom ones\n    patterns = REGEX_PATTERNS.copy()\n    if custom_patterns:\n        patterns.update(custom_patterns)\n    \n    # Extract emails\n    emails = re.findall(patterns['email'], text, re.IGNORECASE)\n    if emails:\n        entities['EMAIL'] = list(set(emails))\n    </code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_e518d10d5151c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_entities_regex_part_2()</code>?",
        "<pre><code>    # Extract phone numbers\n    phones = re.findall(patterns['phone_us'], text)\n    phone_formatted = [f\"({area})-{prefix}-{number}\" for area, prefix, number in phones]\n    intl_phones = re.findall(patterns['phone_intl'], text)\n    all_phones = phone_formatted + intl_phones\n    if all_phones:\n        entities['PHONE'] = list(set(all_phones))\n    \n    # Extract URLs\n    urls = re.findall(patterns['url'], text, re.IGNORECASE)\n    if urls:\n        entities['URL'] = list(set(urls))\n    \n    # Extract dates (multiple formats)\n    dates = []\n    for pattern_name in ['date_mdy', 'date_dmy', 'date_ymd', 'date_written']:\n        matches = re.findall(patterns[pattern_name], text, re.IGNORECASE)\n        dates.extend([' '.join(match) if isinstance(match, tuple) else match for match in matches])</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_b237f4628f411",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_entities_regex_part_3()</code>?",
        "<pre><code>    \n    if dates:\n        entities['DATE'] = list(set(dates))\n    \n    # Extract times\n    times = []\n    time_12 = re.findall(patterns['time_12'], text, re.IGNORECASE)\n    time_24 = re.findall(patterns['time_24'], text)\n    times.extend([f\"{h}:{m} {ap}\" for h, m, ap in time_12])\n    times.extend(time_24)\n    if times:\n        entities['TIME'] = list(set(times))\n    \n    # Extract money amounts\n    money = []\n    for pattern_name in ['money_dollar', 'money_euro', 'money_pound', 'money_written']:\n        matches = re.findall(patterns[pattern_name], text, re.IGNORECASE)\n        money.extend(matches)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_d1d9bb8645f17",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_entities_regex_part_4()</code>?",
        "<pre><code>    \n    if money:\n        entities['MONEY'] = list(set(money))\n    \n    # Extract percentages\n    percentages = re.findall(patterns['percentage'], text)\n    if percentages:\n        entities['PERCENTAGE'] = list(set(percentages))\n    \n    # Extract other entities\n    for entity_type, pattern in [\n        ('SSN', 'ssn'),\n        ('CREDIT_CARD', 'credit_card'),\n        ('IP_ADDRESS', 'ip_address'),\n        ('HASHTAG', 'hashtag'),\n        ('MENTION', 'mention')\n    ]:\n        matches = re.findall(patterns[pattern], text)\n        if matches:\n            entities[entity_type] = list(set(matches))</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_0bafa3d0d6a78",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_entities_regex_part_5()</code>?",
        "<pre><code>    \n    return dict(entities)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_1142a2c165b7b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>sentence_segmentation_part_1()</code>?",
        "<pre><code>def sentence_segmentation(text: str, preserve_abbreviations: bool = True) -> List[str]:\n    \"\"\"Segment text into sentences using regex, handling abbreviations.\"\"\"\n    \n    # Common abbreviations that don't end sentences\n    abbreviations = {\n        'dr', 'mr', 'mrs', 'ms', 'prof', 'sr', 'jr', 'vs', 'etc', 'eg', 'ie',\n        'inc', 'ltd', 'corp', 'co', 'ave', 'st', 'blvd', 'rd', 'apt', 'dept',\n        'fig', 'vol', 'no', 'pp', 'cf', 'al', 'ca', 'ny', 'tx', 'fl'\n    }\n    \n    if preserve_abbreviations:\n        # Replace abbreviations temporarily\n        temp_text = text\n        abbrev_placeholders = {}\n        counter = 0</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_804b0eb6bdc4c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>sentence_segmentation_part_2()</code>?",
        "<pre><code>        \n        for abbrev in abbreviations:\n            pattern = fr'\\b{re.escape(abbrev)}\\.(?!\\s*[A-Z])'\n            matches = re.finditer(pattern, temp_text, re.IGNORECASE)\n            \n            for match in reversed(list(matches)):  # Reverse to maintain indices\n                placeholder = f\"__ABBREV_{counter}__\"\n                abbrev_placeholders[placeholder] = match.group()\n                temp_text = temp_text[:match.start()] + placeholder + temp_text[match.end():]\n                counter += 1\n    else:\n        temp_text = text\n        abbrev_placeholders = {}\n    \n    # Handle ellipsis\n    temp_text = re.sub(r'\\.{2,}', '__ELLIPSIS__', temp_text)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_969ac3f432435",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>sentence_segmentation_part_3()</code>?",
        "<pre><code>    \n    # Handle decimal numbers\n    temp_text = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: m.group().replace('.', '__DECIMAL__'), temp_text)\n    \n    # Split on sentence-ending punctuation\n    sentences = re.split(r'[.!?]+\\s+', temp_text)\n    \n    # Clean up sentences\n    cleaned_sentences = []\n    for sentence in sentences:\n        # Restore abbreviations\n        for placeholder, original in abbrev_placeholders.items():\n            sentence = sentence.replace(placeholder, original)\n        \n        # Restore ellipsis and decimals\n        sentence = sentence.replace('__ELLIPSIS__', '...')\n        sentence = sentence.replace('__DECIMAL__', '.')</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_4684ce96e24dc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>sentence_segmentation_part_4()</code>?",
        "<pre><code>        \n        sentence = sentence.strip()\n        if sentence:\n            cleaned_sentences.append(sentence)\n    \n    return cleaned_sentences</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_e2f6e53faf2cc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>pattern_based_ner_part_1()</code>?",
        "<pre><code>def pattern_based_ner(text: str, patterns: Dict[str, str]) -> List[Tuple[str, str, int, int]]:\n    \"\"\"Custom named entity recognition using regex patterns.\n    \n    Returns: List of (entity_text, entity_type, start_pos, end_pos)\n    \"\"\"\n    entities = []\n    \n    for entity_type, pattern in patterns.items():\n        matches = re.finditer(pattern, text, re.IGNORECASE)\n        \n        for match in matches:\n            entity_text = match.group()\n            start_pos = match.start()\n            end_pos = match.end()\n            \n            entities.append((entity_text, entity_type, start_pos, end_pos))</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_e904b45a4e773",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>pattern_based_ner_part_2()</code>?",
        "<pre><code>    \n    # Sort by start position\n    entities.sort(key=lambda x: x[2])\n    \n    return entities</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_90c22aa2e8e4c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>clean_text_regex()</code>?",
        "<b>Purpose:</b> Apply multiple regex cleaning rules to text<br><br><pre><code>def clean_text_regex(text: str, rules: List[Tuple[str, str]]) -> str:\n    \"\"\"Apply multiple regex cleaning rules to text.\n    \n    Args:\n        text: Input text to clean\n        rules: List of (pattern, replacement) tuples\n    \"\"\"\n    cleaned_text = text\n    \n    for pattern, replacement in rules:\n        cleaned_text = re.sub(pattern, replacement, cleaned_text)\n    \n    return cleaned_text</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_d09fc9c4cf953",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_structured_data_part_1()</code>?",
        "<pre><code>def extract_structured_data(text: str) -> Dict[str, List[Dict]]:\n    \"\"\"Extract structured information like addresses, names, etc.\"\"\"\n    structured_data = defaultdict(list)\n    \n    # Address pattern (simplified US addresses)\n    address_pattern = r'\\d+\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Drive|Dr|Lane|Ln|Way|Court|Ct|Place|Pl)\\.?(?:\\s+(?:Apt|Apartment|Unit|Suite)\\s*\\w+)?'\n    \n    addresses = re.finditer(address_pattern, text, re.IGNORECASE)\n    for match in addresses:\n        structured_data['ADDRESS'].append({\n            'text': match.group(),\n            'start': match.start(),\n            'end': match.end()\n        })\n    </code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_7fdb3655282ec",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_structured_data_part_2()</code>?",
        "<pre><code>    # Person name pattern (Title First Last)\n    name_pattern = r'\\b(?:Mr|Mrs|Ms|Dr|Prof)\\.?\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\b'\n    \n    names = re.finditer(name_pattern, text)\n    for match in names:\n        structured_data['PERSON_NAME'].append({\n            'text': match.group(),\n            'start': match.start(),\n            'end': match.end()\n        })\n    \n    # Product codes (letters and numbers)\n    product_pattern = r'\\b[A-Z]{2,3}-?\\d{3,6}\\b'\n    \n    products = re.finditer(product_pattern, text)\n    for match in products:\n        structured_data['PRODUCT_CODE'].append({\n            'text': match.group(),\n            'start': match.start(),\n            'end': match.end()\n        })</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_dea68d0622ad9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>extract_structured_data_part_3()</code>?",
        "<pre><code>    \n    return dict(structured_data)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_927e19c938472",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>build_custom_tokenizer_part_1()</code>?",
        "<pre><code>def build_custom_tokenizer(patterns: Dict[str, str]) -> callable:\n    \"\"\"Build a custom tokenizer based on regex patterns.\"\"\"\n    \n    def custom_tokenize(text: str) -> List[Tuple[str, str]]:\n        \"\"\"Tokenize text using custom patterns.\n        \n        Returns: List of (token, token_type) tuples\n        \"\"\"\n        tokens = []\n        remaining_text = text\n        offset = 0\n        \n        while remaining_text:\n            matched = False\n            </code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_c02ae429cce0f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>build_custom_tokenizer_part_2()</code>?",
        "<pre><code>            # Try each pattern\n            for token_type, pattern in patterns.items():\n                match = re.match(pattern, remaining_text)\n                if match:\n                    token = match.group()\n                    tokens.append((token, token_type))\n                    \n                    # Update remaining text and offset\n                    consumed = len(token)\n                    remaining_text = remaining_text[consumed:]\n                    offset += consumed\n                    matched = True\n                    break\n            \n            if not matched:</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_5c26ac6f95040",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>build_custom_tokenizer_part_3()</code>?",
        "<pre><code>                # Default: consume one character as unknown\n                if remaining_text:\n                    tokens.append((remaining_text[0], 'UNKNOWN'))\n                    remaining_text = remaining_text[1:]\n                    offset += 1\n        \n        return tokens\n    \n    return custom_tokenize</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_9ea3fab3cad0d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>validate_extracted_data_part_1()</code>?",
        "<pre><code>def validate_extracted_data(entities: Dict[str, List[str]]) -> Dict[str, List[str]]:\n    \"\"\"Validate extracted entities and filter out false positives.\"\"\"\n    validated = {}\n    \n    # Validate emails\n    if 'EMAIL' in entities:\n        valid_emails = []\n        for email in entities['EMAIL']:\n            # Additional validation beyond basic regex\n            if '@' in email and '.' in email.split('@')[1]:\n                parts = email.split('@')\n                if len(parts) == 2 and parts[0] and parts[1]:\n                    valid_emails.append(email)\n        \n        if valid_emails:\n            validated['EMAIL'] = valid_emails</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_22af904e4c55d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>validate_extracted_data_part_2()</code>?",
        "<pre><code>    \n    # Validate phone numbers\n    if 'PHONE' in entities:\n        valid_phones = []\n        for phone in entities['PHONE']:\n            # Remove all non-digits\n            digits_only = re.sub(r'\\D', '', phone)\n            # US phone should have 10 or 11 digits\n            if len(digits_only) in [10, 11]:\n                valid_phones.append(phone)\n        \n        if valid_phones:\n            validated['PHONE'] = valid_phones\n    \n    # Validate dates\n    if 'DATE' in entities:\n        valid_dates = []\n        for date_str in entities['DATE']:\n            try:</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_acc2127380b96",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>validate_extracted_data_part_3()</code>?",
        "<pre><code>                # Try to parse the date\n                # This is a simplified validation\n                if re.match(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', date_str):\n                    valid_dates.append(date_str)\n                elif re.match(r'[A-Za-z]{3,9}\\s+\\d{1,2},?\\s+\\d{4}', date_str):\n                    valid_dates.append(date_str)\n            except:\n                continue\n        \n        if valid_dates:\n            validated['DATE'] = valid_dates\n    \n    # Copy other entities as-is\n    for entity_type, entity_list in entities.items():\n        if entity_type not in ['EMAIL', 'PHONE', 'DATE']:\n            validated[entity_type] = entity_list</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_0d0535f245bc6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>validate_extracted_data_part_4()</code>?",
        "<pre><code>    \n    return validated</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_67e2e60a03c7c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>optimize_regex_performance_part_1()</code>?",
        "<pre><code>def optimize_regex_performance(text: str, patterns: Dict[str, str]) -> Dict[str, List[str]]:\n    \"\"\"Optimized regex extraction for large texts.\"\"\"\n    entities = defaultdict(list)\n    \n    # Compile patterns once\n    compiled_patterns = {name: re.compile(pattern, re.IGNORECASE) \n                        for name, pattern in patterns.items()}\n    \n    # Single pass through text\n    for name, compiled_pattern in compiled_patterns.items():\n        matches = compiled_pattern.findall(text)\n        if matches:\n            # Handle tuple matches (from groups)\n            if matches and isinstance(matches[0], tuple):\n                matches = ['-'.join(match) if isinstance(match, tuple) else match \n                          for match in matches]</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_2e5c6b321409a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>optimize_regex_performance_part_2()</code>?",
        "<pre><code>            \n            entities[name.upper()] = list(set(matches))\n    \n    return dict(entities)</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_6d4bd1e1d0d49",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>custom_tokenize_part_1()</code>?",
        "<pre><code>    def custom_tokenize(text: str) -> List[Tuple[str, str]]:\n        \"\"\"Tokenize text using custom patterns.\n        \n        Returns: List of (token, token_type) tuples\n        \"\"\"\n        tokens = []\n        remaining_text = text\n        offset = 0\n        \n        while remaining_text:\n            matched = False\n            \n            # Try each pattern\n            for token_type, pattern in patterns.items():\n                match = re.match(pattern, remaining_text)\n                if match:\n                    token = match.group()\n                    tokens.append((token, token_type))</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_f4e1997cbc587",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Regex Nlp</b><br>How do you implement <code>custom_tokenize_part_2()</code>?",
        "<pre><code>                    \n                    # Update remaining text and offset\n                    consumed = len(token)\n                    remaining_text = remaining_text[consumed:]\n                    offset += consumed\n                    matched = True\n                    break\n            \n            if not matched:\n                # Default: consume one character as unknown\n                if remaining_text:\n                    tokens.append((remaining_text[0], 'UNKNOWN'))\n                    remaining_text = remaining_text[1:]\n                    offset += 1\n        \n        return tokens</code></pre>",
        "Regex Nlp",
        "implementation"
      ],
      "guid": "nlp_ecf9adce826d4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "regex_nlp",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement rule-based sentiment analysis?",
        "<b>Task:</b> Implement a simple rule-based sentiment analyzer.",
        "Sentiment Analysis",
        "problem_understanding"
      ],
      "guid": "nlp_7df3d7be7bbe5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>analyze_sentiment_part_1()</code>?",
        "<pre><code>def analyze_sentiment(text: str) -> Dict[str, float]:\n    \"\"\"\n    Analyze sentiment using rule-based approach (VADER-style).\n    \n    RULE-BASED SENTIMENT ANALYSIS:\n    - Uses pre-built dictionary of word sentiments\n    - Applies grammatical rules (intensifiers, negations)\n    - Fast and interpretable (good for production)\n    - No training data needed\n    \n    ALGORITHM:\n    1. Tokenize text and look up word sentiments\n    2. Apply intensifier rules (\"very good\" > \"good\")\n    3. Apply negation rules (\"not good\" becomes negative)\n    4. Handle punctuation emphasis (\"great!!!\" > \"great\")\n    5. Normalize scores and return distribution\n    \"\"\"</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_01879b3f9c243",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>analyze_sentiment_part_2()</code>?",
        "<pre><code>    \n    # STEP 1: Handle edge cases first\n    if not text or not text.strip():\n        # Return neutral sentiment for empty text\n        return {\"positive\": 0.0, \"negative\": 0.0, \"neutral\": 1.0, \"compound\": 0.0}\n    \n    # STEP 2: Simple tokenization\n    # Split on whitespace and convert to lowercase\n    words = text.lower().split()\n    \n    if not words:\n        return {\"positive\": 0.0, \"negative\": 0.0, \"neutral\": 1.0, \"compound\": 0.0}\n    \n    # STEP 3: Get base sentiment scores for each word\n    # Look up each word in sentiment lexicon\n    scores = []</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_3cf885ea2859e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>analyze_sentiment_part_3()</code>?",
        "<pre><code>    \n    for i, word in enumerate(words):\n        # Get base sentiment score (0 if not in lexicon)\n        base_score = SENTIMENT_LEXICON.get(word, 0.0)\n        \n        if base_score != 0:  # Only process sentiment-bearing words\n            \n            # RULE 1: Check for intensifiers in previous word\n            # \"very good\" should be more positive than \"good\"\n            if i > 0 and words[i-1] in INTENSIFIERS:\n                # Boost sentiment by 30% (VADER-style boosting)\n                base_score *= 1.3\n                print(f\"  INTENSIFIER: '{words[i-1]} {word}' -> boosted to {base_score:.2f}\")\n            \n            # RULE 2: Check for negations in previous 2 words</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_b44defa9aa85a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>analyze_sentiment_part_4()</code>?",
        "<pre><code>            # \"not very good\" should be negative despite \"good\" being positive\n            negated = False\n            for j in range(max(0, i-2), i):  # Look back up to 2 words\n                if words[j] in NEGATIONS:\n                    negated = True\n                    print(f\"  NEGATION: '{words[j]}' flips '{word}'\")\n                    break\n            \n            # Apply negation: flip polarity and reduce intensity\n            if negated:\n                base_score *= -0.8  # Flip sign and dampen (VADER approach)\n            \n            scores.append(base_score)\n    \n    # STEP 4: Handle punctuation emphasis</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_1bbba90ef6a80",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>analyze_sentiment_part_5()</code>?",
        "<pre><code>    # Multiple exclamation marks add emphasis: \"Great!!!\" > \"Great\"\n    exclamation_count = text.count('!')\n    if exclamation_count > 0 and scores:\n        # Add emphasis but cap the effect\n        emphasis = min(exclamation_count * 0.3, 1.0)\n        print(f\"  EMPHASIS: {exclamation_count} exclamations add {emphasis:.2f}\")\n        \n        # Apply emphasis to existing sentiment\n        scores = [s * (1 + emphasis) if s > 0 else s * (1 + emphasis) for s in scores]\n    \n    # STEP 5: Calculate final sentiment distribution\n    if not scores:\n        # No sentiment words found\n        return {\"positive\": 0.0, \"negative\": 0.0, \"neutral\": 1.0, \"compound\": 0.0}\n    </code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_0a6416b8a21a6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>analyze_sentiment_part_6()</code>?",
        "<pre><code>    # Separate positive and negative scores\n    pos_scores = [s for s in scores if s > 0]\n    neg_scores = [s for s in scores if s < 0]\n    \n    # Calculate proportions\n    pos_sum = sum(pos_scores)\n    neg_sum = abs(sum(neg_scores))  # Make positive for proportion calculation\n    \n    total = pos_sum + neg_sum\n    if total > 0:\n        pos_prop = pos_sum / total\n        neg_prop = neg_sum / total\n        neu_prop = 0.0  # In this simple version, neutral is when no sentiment words\n    else:\n        pos_prop = neg_prop = 0.0\n        neu_prop = 1.0</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_148eb9d12007c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>analyze_sentiment_part_7()</code>?",
        "<pre><code>    \n    # STEP 6: Calculate compound score (overall sentiment)\n    # Compound score combines all sentiment into single [-1, 1] score\n    # Used for final classification: positive if > 0.05, negative if < -0.05\n    compound = (pos_sum - neg_sum) / (total + 1) if total > 0 else 0.0\n    compound = max(-1, min(1, compound))  # Clamp to [-1, 1] range\n    \n    return {\n        \"positive\": round(pos_prop, 3),\n        \"negative\": round(neg_prop, 3), \n        \"neutral\": round(neu_prop, 3),\n        \"compound\": round(compound, 3)\n    }</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_472c0d650e8ed",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sentiment Analysis</b><br>How do you implement <code>classify_sentiment_part_1()</code>?",
        "<pre><code>def classify_sentiment(sentiment_scores: Dict[str, float]) -> str:\n    \"\"\"\n    Convert sentiment scores to simple classification.\n    \n    CLASSIFICATION THRESHOLDS (VADER standard):\n    - compound >= 0.05: positive\n    - compound <= -0.05: negative  \n    - -0.05 < compound < 0.05: neutral\n    \n    These thresholds are empirically determined from testing.\n    \"\"\"\n    compound = sentiment_scores['compound']\n    \n    if compound >= 0.05:\n        return 'positive'\n    elif compound <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'</code></pre>",
        "Sentiment Analysis",
        "implementation"
      ],
      "guid": "nlp_b3f383a5d3bae",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sentiment_analysis",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>How do you implement simple lstm for sentiment?",
        "<b>Task:</b> Implement a basic LSTM cell for sentiment classification.",
        "Sequence Models",
        "problem_understanding"
      ],
      "guid": "nlp_c4f83cba869f0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>How do you implement <code>sigmoid()</code>?",
        "<pre><code>def sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-max(-500, min(500, x))))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_f7559d6b79c79",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>How do you implement <code>tanh()</code>?",
        "<pre><code>def tanh(x: float) -> float:\n    return math.tanh(max(-500, min(500, x)))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_4407a46462d66",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>How do you implement <code>dot_product()</code>?",
        "<pre><code>def dot_product(vec1: List[float], vec2: List[float]) -> float:\n    return sum(a * b for a, b in zip(vec1, vec2))</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_ec341b2011664",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>How do you implement <code>lstm_cell_part_1()</code>?",
        "<pre><code>def lstm_cell(x_t: List[float], h_prev: List[float], c_prev: List[float],\n              weights: Dict) -> Tuple[List[float], List[float]]:\n    \"\"\"Single LSTM cell forward pass.\"\"\"\n    hidden_size = len(h_prev)\n    combined = x_t + h_prev\n    \n    # Gates: forget, input, output  \n    f_t = [sigmoid(dot_product(combined, weights['Wf'][i]) + weights['bf'][i]) for i in range(hidden_size)]\n    i_t = [sigmoid(dot_product(combined, weights['Wi'][i]) + weights['bi'][i]) for i in range(hidden_size)]\n    o_t = [sigmoid(dot_product(combined, weights['Wo'][i]) + weights['bo'][i]) for i in range(hidden_size)]\n    \n    # Candidate cell state\n    c_candidate = [tanh(dot_product(combined, weights['Wc'][i]) + weights['bc'][i]) for i in range(hidden_size)]\n    \n    # Update cell and hidden states\n    c_t = [f_t[i] * c_prev[i] + i_t[i] * c_candidate[i] for i in range(hidden_size)]\n    h_t = [o_t[i] * tanh(c_t[i]) for i in range(hidden_size)]</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_14ea80ef2219b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>How do you implement <code>lstm_cell_part_2()</code>?",
        "<pre><code>    \n    return h_t, c_t</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_996da0bdb657a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>How do you implement <code>lstm_sentiment_part_1()</code>?",
        "<pre><code>def lstm_sentiment(sequence: List[List[float]], weights: Dict) -> float:\n    \"\"\"Run LSTM over sequence and classify sentiment.\"\"\"\n    if not sequence:\n        return 0.5\n    \n    hidden_size = len(weights['bf'])\n    h_t = [0.0] * hidden_size\n    c_t = [0.0] * hidden_size\n    \n    # Process sequence\n    for x_t in sequence:\n        h_t, c_t = lstm_cell(x_t, h_t, c_t, weights)\n    \n    # Final classification\n    logit = dot_product(h_t, weights['W_output']) + weights['b_output']\n    return sigmoid(logit)</code></pre>",
        "Sequence Models",
        "implementation"
      ],
      "guid": "nlp_6a6d39b39ebc2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Sequence Models</b><br>Write the Key Formula",
        "<h3>logit = dot_product(h_t, weights['W_output']) + weights['b_output']</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Sequence Models",
        "formula"
      ],
      "guid": "nlp_7886e8219d896",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "sequence_models",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement text similarity metrics?",
        "<b>Task:</b> Implement multiple similarity metrics:",
        "Similarity",
        "problem_understanding"
      ],
      "guid": "nlp_2f93ca4cc679d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>tokenize()</code>?",
        "<b>Purpose:</b> Simple tokenization<br><br><pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"Simple tokenization.\"\"\"\n    return text.lower().split()</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_86a63d2305afa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>cosine_similarity_part_1()</code>?",
        "<pre><code>def cosine_similarity(text1: str, text2: str, method='tfidf') -> float:\n    \"\"\"Calculate cosine similarity between two texts.\"\"\"\n    if method == 'tfidf':\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n        return sklearn_cosine(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n    \n    elif method == 'bow':  # Bag of words\n        # Tokenize\n        tokens1 = tokenize(text1)\n        tokens2 = tokenize(text2)\n        \n        # Create vocabulary\n        vocab = set(tokens1 + tokens2)\n        </code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_1aff2eb553f9c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>cosine_similarity_part_2()</code>?",
        "<pre><code>        # Create vectors\n        vec1 = np.array([tokens1.count(word) for word in vocab])\n        vec2 = np.array([tokens2.count(word) for word in vocab])\n        \n        # Calculate cosine similarity\n        dot_product = np.dot(vec1, vec2)\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n        \n        if norm1 * norm2 == 0:\n            return 0.0\n        \n        return dot_product / (norm1 * norm2)</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_0c9ac59d744f3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>jaccard_similarity_part_1()</code>?",
        "<pre><code>def jaccard_similarity(text1: str, text2: str, ngram_size: int = 1) -> float:\n    \"\"\"Calculate Jaccard similarity (intersection over union).\"\"\"\n    if ngram_size == 1:\n        # Word-level Jaccard\n        set1 = set(tokenize(text1))\n        set2 = set(tokenize(text2))\n    else:\n        # N-gram Jaccard\n        set1 = set(get_ngrams(text1, ngram_size))\n        set2 = set(get_ngrams(text2, ngram_size))\n    \n    if not set1 and not set2:\n        return 1.0\n    if not set1 or not set2:\n        return 0.0</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_4bf2538ee34af",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>jaccard_similarity_part_2()</code>?",
        "<pre><code>    \n    intersection = len(set1 & set2)\n    union = len(set1 | set2)\n    \n    return intersection / union</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_e838be11cf72c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>get_ngrams()</code>?",
        "<b>Purpose:</b> Extract n-grams from text<br><br><pre><code>def get_ngrams(text: str, n: int) -> List[str]:\n    \"\"\"Extract n-grams from text.\"\"\"\n    tokens = tokenize(text)\n    ngrams = []\n    \n    for i in range(len(tokens) - n + 1):\n        ngram = ' '.join(tokens[i:i+n])\n        ngrams.append(ngram)\n    \n    return ngrams</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_49ec40e5d415a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>semantic_similarity_part_1()</code>?",
        "<pre><code>def semantic_similarity(text1: str, text2: str, use_sentence_transformer: bool = True) -> float:\n    \"\"\"Calculate semantic similarity using embeddings.\"\"\"\n    \n    if use_sentence_transformer:\n        try:\n            from sentence_transformers import SentenceTransformer\n            model = SentenceTransformer('all-MiniLM-L6-v2')\n            \n            # Encode sentences\n            embeddings = model.encode([text1, text2])\n            \n            # Calculate cosine similarity\n            similarity = sklearn_cosine([embeddings[0]], [embeddings[1]])[0][0]\n            return float(similarity)\n            \n        except ImportError:\n            print(\"Install sentence-transformers: pip install sentence-transformers\")\n            use_sentence_transformer = False</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_7c471cd9162d6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>semantic_similarity_part_2()</code>?",
        "<pre><code>    \n    if not use_sentence_transformer:\n        # Fallback: Simple word embedding average\n        try:\n            import spacy\n            nlp = spacy.load('en_core_web_sm')\n            \n            doc1 = nlp(text1)\n            doc2 = nlp(text2)\n            \n            # Use spaCy's similarity (based on word vectors)\n            return doc1.similarity(doc2)\n            \n        except:\n            # Ultimate fallback: enhanced bag-of-words\n            return cosine_similarity(text1, text2, method='tfidf')</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_4672339635974",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>levenshtein_distance_part_1()</code>?",
        "<pre><code>def levenshtein_distance(text1: str, text2: str, normalize: bool = True) -> float:\n    \"\"\"Calculate Levenshtein (edit) distance between texts.\"\"\"\n    if not text1:\n        return len(text2) if not normalize else 1.0\n    if not text2:\n        return len(text1) if not normalize else 1.0\n    \n    # Create matrix\n    matrix = [[0] * (len(text2) + 1) for _ in range(len(text1) + 1)]\n    \n    # Initialize first row and column\n    for i in range(len(text1) + 1):\n        matrix[i][0] = i\n    for j in range(len(text2) + 1):\n        matrix[0][j] = j</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_4c1622481ff12",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>levenshtein_distance_part_2()</code>?",
        "<pre><code>    \n    # Fill matrix\n    for i in range(1, len(text1) + 1):\n        for j in range(1, len(text2) + 1):\n            if text1[i-1] == text2[j-1]:\n                cost = 0\n            else:\n                cost = 1\n            \n            matrix[i][j] = min(\n                matrix[i-1][j] + 1,      # deletion\n                matrix[i][j-1] + 1,      # insertion\n                matrix[i-1][j-1] + cost  # substitution\n            )\n    \n    distance = matrix[len(text1)][len(text2)]</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_e635daa73ea80",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>levenshtein_distance_part_3()</code>?",
        "<pre><code>    \n    if normalize:\n        # Normalize by maximum possible distance\n        max_len = max(len(text1), len(text2))\n        return 1 - (distance / max_len)\n    \n    return distance</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_f2eea981d8c4d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>find_near_duplicates_part_1()</code>?",
        "<pre><code>def find_near_duplicates(texts: List[str], threshold: float = 0.8) -> List[Tuple[int, int, float]]:\n    \"\"\"Find near-duplicate texts using MinHash.\"\"\"\n    minhash = MinHashLSH(num_hashes=64, ngram_size=3)\n    \n    # Compute signatures for all texts\n    signatures = [minhash.compute_minhash(text) for text in texts]\n    \n    # Find similar pairs\n    similar_pairs = []\n    \n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            similarity = minhash.jaccard_similarity_minhash(signatures[i], signatures[j])\n            if similarity >= threshold:\n                similar_pairs.append((i, j, similarity))</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_b46994dde3ce2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>find_near_duplicates_part_2()</code>?",
        "<pre><code>    \n    return similar_pairs</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_69fd2a24dcfe0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>similarity_matrix_part_1()</code>?",
        "<pre><code>def similarity_matrix(texts: List[str], method: str = 'cosine') -> np.ndarray:\n    \"\"\"Compute pairwise similarity matrix for multiple texts.\"\"\"\n    n = len(texts)\n    matrix = np.zeros((n, n))\n    \n    for i in range(n):\n        for j in range(i, n):\n            if i == j:\n                matrix[i][j] = 1.0\n            else:\n                if method == 'cosine':\n                    sim = cosine_similarity(texts[i], texts[j])\n                elif method == 'jaccard':\n                    sim = jaccard_similarity(texts[i], texts[j])\n                elif method == 'semantic':\n                    sim = semantic_similarity(texts[i], texts[j])\n                else:\n                    raise ValueError(f\"Unknown method: {method}\")</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_16e7637e1133f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>similarity_matrix_part_2()</code>?",
        "<pre><code>                \n                matrix[i][j] = sim\n                matrix[j][i] = sim  # Symmetric\n    \n    return matrix</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_12e54afd08abd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>__init__()</code>?",
        "<pre><code>    def __init__(self, num_hashes: int = 128, ngram_size: int = 3):\n        self.num_hashes = num_hashes\n        self.ngram_size = ngram_size\n        self.hash_funcs = self._generate_hash_functions()</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_7c75f38eff7ec",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>_generate_hash_functions()</code>?",
        "<b>Purpose:</b> Generate hash functions for MinHash<br><br><pre><code>    def _generate_hash_functions(self):\n        \"\"\"Generate hash functions for MinHash.\"\"\"\n        hash_funcs = []\n        for i in range(self.num_hashes):\n            # Use different seeds for different hash functions\n            seed = i\n            hash_funcs.append(lambda x, s=seed: int(hashlib.md5(f\"{s}{x}\".encode()).hexdigest(), 16))\n        return hash_funcs</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_52fc36fb16259",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>get_shingles()</code>?",
        "<b>Purpose:</b> Convert text to shingles (n-grams)<br><br><pre><code>    def get_shingles(self, text: str) -> Set[str]:\n        \"\"\"Convert text to shingles (n-grams).\"\"\"\n        return set(get_ngrams(text, self.ngram_size))</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_e2cd613dac460",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>compute_minhash()</code>?",
        "<b>Purpose:</b> Compute MinHash signature for text<br><br><pre><code>    def compute_minhash(self, text: str) -> List[int]:\n        \"\"\"Compute MinHash signature for text.\"\"\"\n        shingles = self.get_shingles(text)\n        if not shingles:\n            return [0] * self.num_hashes\n        \n        signature = []\n        for hash_func in self.hash_funcs:\n            min_hash = min(hash_func(shingle) for shingle in shingles)\n            signature.append(min_hash)\n        \n        return signature</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_07733baf065f6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>How do you implement <code>jaccard_similarity_minhash()</code>?",
        "<b>Purpose:</b> Estimate Jaccard similarity from MinHash signatures<br><br><pre><code>    def jaccard_similarity_minhash(self, sig1: List[int], sig2: List[int]) -> float:\n        \"\"\"Estimate Jaccard similarity from MinHash signatures.\"\"\"\n        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)\n        return matches / len(sig1)</code></pre>",
        "Similarity",
        "implementation"
      ],
      "guid": "nlp_40328103e8069",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the def cosine_similarity(text1: str, text2: str, method",
        "<h3>'tfidf') -> float:</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>\"\"\"Calculate cosine similarity between two texts.\"\"\" | if method == 'tfidf':</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_f9f4f8942e526",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the similarity",
        "<h3>sklearn_cosine([embeddings[0]], [embeddings[1]])[0][0]</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre># Calculate cosine similarity | return float(similarity)</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_3e4f3e52a67a8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the return cosine_similarity(text1, text2, method",
        "<h3>'tfidf')</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>except: | # Ultimate fallback: enhanced bag-of-words</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_5f3bd699f73da",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the def similarity_matrix(texts: List[str], method: str",
        "<h3>'cosine') -> np.ndarray:</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>\"\"\"Compute pairwise similarity matrix for multiple texts.\"\"\" | n = len(texts)</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_0bc4632919cf3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the if method",
        "<h3>= 'cosine':</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>matrix[i][j] = 1.0 | else: | sim = cosine_similarity(texts[i], texts[j])</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_5287ac4a0c30a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the sim",
        "<h3>cosine_similarity(texts[i], texts[j])</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>else: | if method == 'cosine': | elif method == 'jaccard':</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_7073ec594b1e5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the print(f\"Cosine (BoW):     {cosine_similarity(text1, text2, method",
        "<h3>'bow'):.3f}\")</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre># Different similarity metrics | print(f\"Cosine (TF-IDF):  {cosine_similarity(text1, text2, method='tfidf'):.3f}\") | print(f\"Jaccard (words):  {jaccard_similarity(text1, text2, ngram_size=1):.3f}\")</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_e4f2c299123d3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the print(f\"Cosine (TF-IDF):  {cosine_similarity(text1, text2, method",
        "<h3>'tfidf'):.3f}\")</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre># Different similarity metrics | print(f\"Cosine (BoW):     {cosine_similarity(text1, text2, method='bow'):.3f}\") | print(f\"Jaccard (words):  {jaccard_similarity(text1, text2, ngram_size=1):.3f}\")</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_bb6068602a4bd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Similarity</b><br>Write the sim_matrix",
        "<h3>similarity_matrix(texts, method='cosine')</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>] | print(\"\\nTexts:\") | for i, text in enumerate(texts):</pre></details>",
        "Similarity",
        "formula"
      ],
      "guid": "nlp_f3059a4e8943a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "similarity",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>How do you implement stemming vs lemmatization?",
        "<b>Task:</b> Implement two functions:",
        "Stemming Lemmatization",
        "problem_understanding"
      ],
      "guid": "nlp_c8a0a65eb2d9d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>How do you implement <code>get_wordnet_pos()</code>?",
        "<b>Purpose:</b> Convert Penn Treebank POS tags to WordNet POS tags<br><br><pre><code>def get_wordnet_pos(treebank_tag):\n    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags.\"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN  # default</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_22472b2a6f5a0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>How do you implement <code>stem_words()</code>?",
        "<b>Purpose:</b> Apply Porter stemming to words<br><br><pre><code>def stem_words(words: List[str]) -> List[str]:\n    \"\"\"Apply Porter stemming to words.\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word.lower()) for word in words]</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_436d940607840",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>How do you implement <code>lemmatize_words()</code>?",
        "<b>Purpose:</b> Lemmatize words with optional POS tags for better accuracy<br><br><pre><code>def lemmatize_words(words: List[str], pos_tags: Optional[List[str]] = None) -> List[str]:\n    \"\"\"Lemmatize words with optional POS tags for better accuracy.\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    \n    if pos_tags is None:\n        # Simple lemmatization without POS\n        return [lemmatizer.lemmatize(word.lower()) for word in words]\n    \n    # Lemmatize with POS tags\n    lemmatized = []\n    for word, pos in zip(words, pos_tags):\n        wordnet_pos = get_wordnet_pos(pos)\n        lemmatized.append(lemmatizer.lemmatize(word.lower(), pos=wordnet_pos))\n    \n    return lemmatized</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_93b2a616a8c70",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stemming Lemmatization</b><br>How do you implement <code>compare_methods_part_1()</code>?",
        "<pre><code>def compare_methods(words: List[str]) -> dict:\n    \"\"\"Compare stemming vs lemmatization results.\"\"\"\n    import nltk\n    \n    # Get POS tags\n    pos_tags = [pos for _, pos in nltk.pos_tag(words)]\n    \n    stemmed = stem_words(words)\n    lemmatized_simple = lemmatize_words(words)\n    lemmatized_pos = lemmatize_words(words, pos_tags)\n    \n    return {\n        'original': words,\n        'stemmed': stemmed,\n        'lemmatized_simple': lemmatized_simple,\n        'lemmatized_with_pos': lemmatized_pos,\n        'pos_tags': pos_tags\n    }</code></pre>",
        "Stemming Lemmatization",
        "implementation"
      ],
      "guid": "nlp_7b927b98ed979",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stemming_lemmatization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stop Word Removal</b><br>How do you implement remove stopwords (configurable)?",
        "<b>Task:</b> Implement `remove_stopwords(tokens: List[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]` that removes English stopwords from a token list.",
        "Stop Word Removal",
        "problem_understanding"
      ],
      "guid": "nlp_388eb1bc01365",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stop_word_removal",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Stop Word Removal</b><br>How do you implement <code>remove_stopwords()</code>?",
        "<b>Purpose:</b> Remove stopwords, preserving order<br><br><pre><code>def remove_stopwords(tokens: Iterable[str], extra_stopwords: Optional[Set[str]] = None) -> List[str]:\n    \"\"\"Remove stopwords, preserving order.\n\n    Case-insensitive membership check, preserves original casing in the output.\n    \"\"\"\n    stop_set = set(ENGLISH_STOPWORDS)\n    if extra_stopwords:\n        stop_set |= {w.lower() for w in extra_stopwords}\n\n    cleaned: List[str] = []\n    for token in tokens:\n        if token and token.lower() not in stop_set:\n            cleaned.append(token)\n    return cleaned</code></pre>",
        "Stop Word Removal",
        "implementation"
      ],
      "guid": "nlp_1f3c51564c2a4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "stop_word_removal",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement tf-idf implementation?",
        "<b>Task:</b> Implement TF-IDF from scratch to find document similarity.",
        "Tfidf",
        "problem_understanding"
      ],
      "guid": "nlp_0374e3784c64f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>compute_tfidf_part_1()</code>?",
        "<pre><code>def compute_tfidf(documents: List[str]) -> List[Dict[str, float]]:\n    \"\"\"\n    Compute TF-IDF vectors for documents.\n    \n    TF-IDF = Term Frequency × Inverse Document Frequency\n    - Emphasizes important words that appear frequently in a document\n    - But rarely across the entire collection\n    \"\"\"\n    # STEP 1: Handle edge cases\n    if not documents:\n        return []\n    \n    # STEP 2: Tokenize all documents (simple whitespace splitting)\n    # In real interviews, discuss more sophisticated tokenization\n    tokenized_docs = [doc.lower().split() for doc in documents]</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_af156c60033b6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>compute_tfidf_part_2()</code>?",
        "<pre><code>    \n    # STEP 3: Build vocabulary from all unique words\n    # This creates our feature space - each unique word becomes a dimension\n    vocab = set()\n    for doc in tokenized_docs:\n        vocab.update(doc)\n    vocab = sorted(list(vocab))  # Sort for consistency\n    \n    # STEP 4: Calculate Document Frequency (DF) for each term\n    # DF = number of documents containing the term\n    # Used in IDF calculation: IDF = log(total_docs / doc_freq)\n    doc_freq = {}\n    for term in vocab:\n        doc_freq[term] = sum(1 for doc in tokenized_docs if term in doc)\n    </code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_19962185f8f42",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>compute_tfidf_part_3()</code>?",
        "<pre><code>    # STEP 5: Calculate TF-IDF for each document\n    tfidf_vectors = []\n    num_docs = len(documents)\n    \n    for doc in tokenized_docs:\n        # Count term frequencies in this document\n        term_counts = Counter(doc)\n        doc_length = len(doc)\n        tfidf_vector = {}\n        \n        for term in vocab:\n            # TERM FREQUENCY (TF): How often term appears in this document\n            # Normalized by document length to handle different document sizes\n            tf = term_counts[term] / doc_length if doc_length > 0 else 0\n            </code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_bb2f3069781ca",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>compute_tfidf_part_4()</code>?",
        "<pre><code>            # INVERSE DOCUMENT FREQUENCY (IDF): How rare the term is across collection\n            # log(total_docs / docs_containing_term)\n            # Rare terms get higher IDF scores\n            idf = math.log(num_docs / doc_freq[term])\n            \n            # TF-IDF SCORE: Combines term importance in document (TF) \n            # with term rarity in collection (IDF)\n            tfidf_vector[term] = tf * idf\n        \n        tfidf_vectors.append(tfidf_vector)\n    \n    return tfidf_vectors</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_5e17877c8e7fb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>cosine_similarity_part_1()</code>?",
        "<pre><code>def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:\n    \"\"\"\n    Calculate cosine similarity between two TF-IDF vectors.\n    \n    Cosine similarity = dot_product(v1, v2) / (||v1|| × ||v2||)\n    - Returns value between 0 and 1 (since TF-IDF values are non-negative)\n    - 1.0 = identical vectors, 0.0 = completely different\n    \"\"\"\n    # STEP 1: Find common terms between the two vectors\n    # Only these contribute to the dot product\n    common_terms = set(vec1.keys()) & set(vec2.keys())\n    \n    if not common_terms:\n        return 0.0  # No overlap = no similarity\n    </code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_38de49ead67ea",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>cosine_similarity_part_2()</code>?",
        "<pre><code>    # STEP 2: Calculate dot product\n    # Sum of element-wise multiplication for common terms\n    dot_product = sum(vec1[term] * vec2[term] for term in common_terms)\n    \n    # STEP 3: Calculate vector norms (magnitudes)\n    # ||v|| = sqrt(sum of squared elements)\n    norm1 = math.sqrt(sum(val**2 for val in vec1.values()))\n    norm2 = math.sqrt(sum(val**2 for val in vec2.values()))\n    \n    # STEP 4: Handle edge case of zero vectors\n    if norm1 == 0 or norm2 == 0:\n        return 0.0  # Can't compute similarity with zero vector\n    \n    # STEP 5: Return cosine similarity\n    # Normalized dot product gives us the cosine of angle between vectors\n    return dot_product / (norm1 * norm2)</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_5f5a3ffbcdd89",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>find_similar_documents_part_1()</code>?",
        "<pre><code>def find_similar_documents(documents: List[str], query: str) -> int:\n    \"\"\"\n    Find most similar document to query using TF-IDF + cosine similarity.\n    \n    This is the core of many search and recommendation systems.\n    \"\"\"\n    # STEP 1: Create unified document collection\n    # Include query as the last document for TF-IDF calculation\n    all_texts = documents + [query]\n    \n    # STEP 2: Compute TF-IDF for all documents + query\n    # This ensures query and documents are in same vector space\n    tfidf_vectors = compute_tfidf(all_texts)\n    \n    # STEP 3: Extract query vector (last one)\n    query_vector = tfidf_vectors[-1]</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_5dd34f76c42ea",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>How do you implement <code>find_similar_documents_part_2()</code>?",
        "<pre><code>    \n    # STEP 4: Compare query with each document\n    max_similarity = -1\n    most_similar_idx = 0\n    \n    # Only compare with documents (exclude query vector)\n    for i, doc_vector in enumerate(tfidf_vectors[:-1]):\n        similarity = cosine_similarity(query_vector, doc_vector)\n        \n        # Track the most similar document\n        if similarity > max_similarity:\n            max_similarity = similarity\n            most_similar_idx = i\n    \n    return most_similar_idx</code></pre>",
        "Tfidf",
        "implementation"
      ],
      "guid": "nlp_c69747b0884c7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the TF-IDF",
        "<h3>Term Frequency × Inverse Document Frequency</h3><br><p><i>TF-IDF: weights terms by frequency and rarity across documents</i></p><br><details><summary>Context</summary><pre>Compute TF-IDF vectors for documents. | - Emphasizes important words that appear frequently in a document | - But rarely across the entire collection</pre></details>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_3c7957520d556",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the Key Formula",
        "<h3>Used in IDF calculation: IDF = log(total_docs / doc_freq)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_46c0722bcb1bb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the Key Formula",
        "<h3>idf = math.log(num_docs / doc_freq[term])</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_7a456d172ceec",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the similarity",
        "<h3>cosine_similarity(query_vector, doc_vector)</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre># Only compare with documents (exclude query vector) | for i, doc_vector in enumerate(tfidf_vectors[:-1]): | # Track the most similar document</pre></details>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_ec0a788135eac",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the Key Formula",
        "<h3>idf = math.log(len(docs) / df)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_2323d4f20fe38",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the Key Formula",
        "<h3>idf = math.log(len(docs) / doc_freq[word])</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_0cfd723829daf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the print(f\"   '{word}': TF",
        "<h3>{tf:.3f}, IDF={idf:.3f}, TF-IDF={tfidf:.3f}\")</h3><br><p><i>TF-IDF: weights terms by frequency and rarity across documents</i></p><br><details><summary>Context</summary><pre>idf = math.log(len(docs) / doc_freq[word]) | tfidf = tf * idf | # Full computation</pre></details>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_5fcb93ec4bd9b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>Write the sim",
        "<h3>cosine_similarity(query_vec, doc_vec)</h3><br><p><i>Cosine similarity: measures angle between vectors (0=orthogonal, 1=identical)</i></p><br><details><summary>Context</summary><pre>for i, doc in enumerate(docs): | doc_vec = tfidf_vectors[i] | print(f\"   Similarity with doc {i}: {sim:.3f}\")</pre></details>",
        "Tfidf",
        "formula"
      ],
      "guid": "nlp_fe82e052c56f8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>What's the Time Complexity?",
        "<b>O(d×v)</b><br><i>See Big O notation reference</i>",
        "Tfidf",
        "complexity"
      ],
      "guid": "nlp_c50c8b9631175",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>What's the Space Complexity?",
        "<b>O(d×v)</b><br><i>See Big O notation reference</i>",
        "Tfidf",
        "complexity"
      ],
      "guid": "nlp_fe9daeb0fd43b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tfidf</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMO: Show step-by-step execution</strong></p>",
        "Tfidf",
        "interview_insights"
      ],
      "guid": "nlp_a2e75e84f3ff1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tfidf",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement text classification pipeline?",
        "<b>Task:</b> Build a complete text classification system:",
        "Text Classification",
        "problem_understanding"
      ],
      "guid": "nlp_553b33b4dc0be",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>extract_features_part_1()</code>?",
        "<pre><code>def extract_features(texts: List[str], method: str = 'tfidf') -> Tuple[List[List[float]], List[str]]:\n    \"\"\"\n    Extract features from texts for classification.\n    \n    This is the MOST IMPORTANT step in text classification.\n    Feature quality determines model performance more than algorithm choice.\n    \"\"\"\n    \n    # STEP 1: Build vocabulary from all texts\n    # This creates our feature space - each unique word becomes a dimension\n    vocab = set()\n    for text in texts:\n        words = text.lower().split()  # Simple tokenization\n        vocab.update(words)\n    vocab = sorted(list(vocab))  # Sort for consistency</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_84441501f98dd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>extract_features_part_2()</code>?",
        "<pre><code>    \n    if method == 'tfidf':\n        # STEP 2: Calculate document frequencies for IDF computation\n        # DF = number of documents containing each term\n        doc_freq = {}\n        for word in vocab:\n            doc_freq[word] = sum(1 for text in texts if word in text.lower())\n        \n        # STEP 3: Convert each text to TF-IDF vector\n        feature_matrix = []\n        for text in texts:\n            words = text.lower().split()\n            word_counts = Counter(words)\n            doc_length = len(words)\n            </code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_2930ffa77cf60",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>extract_features_part_3()</code>?",
        "<pre><code>            # Calculate TF-IDF for each vocabulary word\n            tfidf_vector = []\n            for word in vocab:\n                # TF: How often word appears in this document (normalized)\n                tf = word_counts[word] / doc_length if doc_length > 0 else 0\n                \n                # IDF: How rare the word is across all documents\n                idf = math.log(len(texts) / doc_freq[word])\n                \n                # TF-IDF: Combines local importance (TF) with global rarity (IDF)\n                tfidf_score = tf * idf\n                tfidf_vector.append(tfidf_score)\n            \n            feature_matrix.append(tfidf_vector)\n        \n        return feature_matrix, vocab</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_d2c1554cc8bd4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>extract_features_part_4()</code>?",
        "<pre><code>    \n    else:  # Simple bag-of-words\n        feature_matrix = []\n        for text in texts:\n            word_counts = Counter(text.lower().split())\n            bow_vector = [word_counts[word] for word in vocab]\n            feature_matrix.append(bow_vector)\n        \n        return feature_matrix, vocab</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_a1a1ba91c9499",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>train_logistic_regression_part_1()</code>?",
        "<pre><code>def train_logistic_regression(X: List[List[float]], y: List[int]) -> Dict:\n    \"\"\"\n    Train logistic regression classifier from scratch.\n    \n    Logistic regression is linear classifier with sigmoid activation.\n    Good baseline for text classification - simple but effective.\n    \"\"\"\n    # STEP 1: Convert to numpy arrays for easier math\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # STEP 2: Add bias term (intercept)\n    # This allows the decision boundary to not pass through origin\n    X_with_bias = np.column_stack([np.ones(len(X)), X_np])\n    </code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_48b68fc0feb4f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>train_logistic_regression_part_2()</code>?",
        "<pre><code>    # STEP 3: Initialize weights randomly (small values)\n    # Small initialization prevents sigmoid saturation early in training\n    n_features = X_with_bias.shape[1]\n    weights = np.random.randn(n_features) * 0.01\n    \n    # STEP 4: Training loop using gradient descent\n    learning_rate = 0.01  # Step size for weight updates\n    epochs = 100          # Number of training iterations\n    \n    for epoch in range(epochs):\n        # FORWARD PASS: Compute predictions\n        # Linear combination followed by sigmoid activation\n        logits = X_with_bias @ weights           # Linear part: Xw + b\n        predictions = 1 / (1 + np.exp(-logits)) # Sigmoid: maps to [0,1]\n        </code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_404b54e700181",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>train_logistic_regression_part_3()</code>?",
        "<pre><code>        # BACKWARD PASS: Compute gradients\n        # Gradient of cross-entropy loss w.r.t. weights\n        errors = predictions - y_np              # Prediction errors\n        gradients = (X_with_bias.T @ errors) / len(X)  # Average gradient\n        \n        # UPDATE WEIGHTS: Move in opposite direction of gradient\n        weights -= learning_rate * gradients\n    \n    return {'weights': weights, 'type': 'logistic'}</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_5691df682e52d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>predict_logistic()</code>?",
        "<b>Purpose:</b> Make predictions with trained logistic regression model<br><br><pre><code>def predict_logistic(X: List[List[float]], model: Dict) -> List[int]:\n    \"\"\"Make predictions with trained logistic regression model.\"\"\"\n    X_np = np.array(X)\n    \n    # Add bias term (same as training)\n    X_with_bias = np.column_stack([np.ones(len(X)), X_np])\n    \n    # Calculate probabilities\n    logits = X_with_bias @ model['weights']\n    probabilities = 1 / (1 + np.exp(-logits))\n    \n    # Convert to binary predictions (threshold = 0.5)\n    predictions = (probabilities > 0.5).astype(int)\n    \n    return predictions.tolist()</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_96279391b05da",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>evaluate_classifier_part_1()</code>?",
        "<pre><code>def evaluate_classifier(y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n    \"\"\"\n    Calculate key evaluation metrics.\n    \n    These are what interviewers ask about most:\n    - Accuracy: Overall correctness\n    - Precision: Of predicted positives, how many are correct?\n    - Recall: Of actual positives, how many did we find?\n    - F1: Balanced measure combining precision and recall\n    \"\"\"\n    # Basic accuracy calculation\n    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n    accuracy = correct / len(y_true) if y_true else 0.0\n    \n    # For binary classification, calculate detailed metrics\n    if set(y_true + y_pred) <= {0, 1}:</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_64dd42a802d71",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>evaluate_classifier_part_2()</code>?",
        "<pre><code>        # Confusion matrix components\n        tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)  # True Positives\n        fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)  # False Positives\n        fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)  # False Negatives\n        \n        # Calculate metrics with zero-division protection\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n        \n        return {\n            'accuracy': accuracy,\n            'precision': precision, \n            'recall': recall,\n            'f1': f1\n        }</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_f74760b1d6039",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>How do you implement <code>evaluate_classifier_part_3()</code>?",
        "<pre><code>    \n    return {'accuracy': accuracy}</code></pre>",
        "Text Classification",
        "implementation"
      ],
      "guid": "nlp_ffa5d9109edbe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>idf = math.log(len(texts) / doc_freq[word])</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_c8a9394a082a5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>Linear part: Xw + b</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_2f6f862de95c1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>Sigmoid: maps to [0,1]</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_ece487aa74462",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>logits = X_with_bias @ model['weights']</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_078be6fe32897",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>probabilities = 1 / (1 + np.exp(-logits))</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_7967224f941ff",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>model = train_logistic_regression(X_train, train_labels)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_2e4c559348605",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>predictions = predict_logistic(X_test, model)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_16d21c1280e1d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Text Classification</b><br>Write the Key Formula",
        "<h3>train_predictions = predict_logistic(X_train, model)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Text Classification",
        "formula"
      ],
      "guid": "nlp_74de93297f5c1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "text_classification",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement text tokenization?",
        "<b>Task:</b> Implement a function that tokenizes text into words while handling edge cases.",
        "Tokenization",
        "problem_understanding"
      ],
      "guid": "nlp_04c8b9bdb7b44",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>tokenize_part_1()</code>?",
        "<pre><code>def tokenize(text: str) -> List[str]:\n    \"\"\"\n    Tokenize text into words, preserving contractions and handling punctuation.\n    \n    This is ALWAYS asked in NLP interviews - seems simple but has many edge cases.\n    \n    Key challenges:\n    - Contractions: \"don't\" should stay as one token, not [\"don\", \"'\", \"t\"]\n    - Punctuation: \"Hello!\" should become [\"Hello\", \"!\"]\n    - Empty/None input: Handle gracefully\n    - Unicode characters: Different languages, emojis\n    \"\"\"\n    \n    # STEP 1: Handle edge cases first\n    # Always check for None/empty input in interviews\n    if not text:\n        return []</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_6cc75bd51c675",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>tokenize_part_2()</code>?",
        "<pre><code>    \n    # STEP 2: Use regex pattern for tokenization\n    # This is the most robust approach for handling complex cases\n    \n    # PATTERN EXPLANATION:\n    # \\w+(?:'\\w+)?  - Matches word characters, optionally followed by apostrophe + more word chars\n    #                 This handles contractions like \"don't\", \"I'm\", \"we'll\"\n    # |             - OR operator\n    # [^\\w\\s]       - Matches any non-word, non-space character (punctuation)\n    #                 This treats each punctuation mark as separate token\n    \n    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n    \n    # re.findall returns all non-overlapping matches\n    tokens = re.findall(pattern, text)</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_cadb2c6cb9e96",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>tokenize_part_3()</code>?",
        "<pre><code>    \n    return tokens</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_0b9d25ab297f5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>tokenize_simple_part_1()</code>?",
        "<pre><code>def tokenize_simple(text: str) -> List[str]:\n    \"\"\"\n    Alternative approach: Replace-then-split method.\n    \n    Sometimes interviewers want to see multiple approaches.\n    This is simpler but less robust than regex.\n    \"\"\"\n    if not text:\n        return []\n    \n    # STEP 1: Replace punctuation with spaces (except apostrophes)\n    # This preserves contractions while isolating other punctuation\n    cleaned = re.sub(r\"[^\\w\\s']\", \" \", text)\n    \n    # STEP 2: Split on whitespace</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_0d71988320135",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>tokenize_simple_part_2()</code>?",
        "<pre><code>    # Simple but effective for basic cases\n    return cleaned.split()</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_c8ef0b4e074d5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>advanced_tokenize_part_1()</code>?",
        "<pre><code>def advanced_tokenize(text: str, preserve_case: bool = False, \n                     handle_urls: bool = True, handle_emails: bool = True) -> List[str]:\n    \"\"\"\n    Advanced tokenization with additional features.\n    \n    Shows awareness of real-world tokenization challenges.\n    Good for follow-up questions about production systems.\n    \"\"\"\n    if not text:\n        return []\n    \n    processed_text = text\n    \n    # STEP 1: Handle special entities before general tokenization\n    if handle_urls:</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_b6afa653a0ba0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>advanced_tokenize_part_2()</code>?",
        "<pre><code>        # Replace URLs with special token\n        # Pattern matches http(s) URLs\n        url_pattern = r'https?://[^\\s]+'\n        processed_text = re.sub(url_pattern, '<URL>', processed_text)\n    \n    if handle_emails:\n        # Replace emails with special token\n        # Basic email pattern for demonstration\n        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        processed_text = re.sub(email_pattern, '<EMAIL>', processed_text)\n    \n    # STEP 2: Case handling\n    if not preserve_case:\n        processed_text = processed_text.lower()\n    </code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_59c1b05b39533",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>advanced_tokenize_part_3()</code>?",
        "<pre><code>    # STEP 3: Tokenize using main pattern\n    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n    tokens = re.findall(pattern, processed_text)\n    \n    return tokens</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_8b2625fb822bb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>handle_contractions_explicitly_part_1()</code>?",
        "<pre><code>def handle_contractions_explicitly(text: str) -> str:\n    \"\"\"\n    Explicit contraction handling - shows deep understanding.\n    \n    Some interviewers want to see you handle contractions manually.\n    This demonstrates knowledge of English language patterns.\n    \"\"\"\n    # Common contractions mapping\n    # In production, you'd have a much larger dictionary\n    contractions = {\n        \"don't\": \"do not\",\n        \"won't\": \"will not\", \n        \"can't\": \"cannot\",\n        \"n't\": \" not\",  # General pattern for negations\n        \"'ll\": \" will\",\n        \"'re\": \" are\", \n        \"'ve\": \" have\",\n        \"'m\": \" am\",\n        \"'d\": \" would\"  # or \"had\" - context dependent\n    }</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_2730e45743b00",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>handle_contractions_explicitly_part_2()</code>?",
        "<pre><code>    \n    # Apply contractions in order (longest first)\n    expanded = text\n    for contraction, expansion in sorted(contractions.items(), key=len, reverse=True):\n        # Use word boundaries to avoid partial matches\n        pattern = r'\\b' + re.escape(contraction) + r'\\b'\n        expanded = re.sub(pattern, expansion, expanded, flags=re.IGNORECASE)\n    \n    return expanded</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_fef3c2fc16ec5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>test_tokenization_part_1()</code>?",
        "<pre><code>def test_tokenization():\n    \"\"\"\n    Comprehensive test cases that cover edge cases interviewers ask about.\n    \n    Practice explaining why each test case is important.\n    \"\"\"\n    test_cases = [\n        # Basic cases\n        (\"Hello world\", [\"Hello\", \"world\"]),\n        (\"Hello world!\", [\"Hello\", \"world\", \"!\"]),\n        \n        # Contractions (most common edge case in interviews)\n        (\"don't go\", [\"don't\", \"go\"]),\n        (\"I'm happy\", [\"I'm\", \"happy\"]),\n        (\"We'll see\", [\"We'll\", \"see\"]),</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_766783bdb4d05",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>test_tokenization_part_2()</code>?",
        "<pre><code>        \n        # Punctuation handling\n        (\"Hello, world!\", [\"Hello\", \",\", \"world\", \"!\"]),\n        (\"What?!?\", [\"What\", \"?\", \"!\", \"?\"]),\n        \n        # Edge cases (always test these in interviews)\n        (\"\", []),  # Empty string\n        (\"   \", []),  # Only spaces\n        (\"one,two;three\", [\"one\", \",\", \"two\", \";\", \"three\"]),\n        \n        # Complex contractions\n        (\"I've been there\", [\"I've\", \"been\", \"there\"]),\n        (\"You're right\", [\"You're\", \"right\"]),\n        \n        # Numbers and special characters\n        (\"Call 911!\", [\"Call\", \"911\", \"!\"]),\n        (\"Price: $19.99\", [\"Price\", \":\", \"$\", \"19.99\"]),\n    ]</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_e53cc8cbf6513",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>test_tokenization_part_3()</code>?",
        "<pre><code>    \n    print(\"TOKENIZATION TEST SUITE\")\n    print(\"=\" * 30)\n    \n    passed = 0\n    total = len(test_cases)\n    \n    for text, expected in test_cases:\n        result = tokenize(text)\n        \n        # Check if result matches expected\n        if result == expected:\n            status = \"✓ PASS\"\n            passed += 1\n        else:\n            status = \"✗ FAIL\"</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_3bd67eed28f93",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>How do you implement <code>test_tokenization_part_4()</code>?",
        "<pre><code>        \n        print(f\"{status} '{text}' -> {result}\")\n        if result != expected:\n            print(f\"     Expected: {expected}\")\n    \n    print(f\"\\nSUCCESS RATE: {passed}/{total} ({100*passed/total:.1f}%)\")\n    \n    return passed == total</code></pre>",
        "Tokenization",
        "implementation"
      ],
      "guid": "nlp_d83c59fd2461e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>What's the Time Complexity?",
        "<b>O(n)</b><br><i>Linear time - grows proportionally with input size</i>",
        "Tokenization",
        "complexity"
      ],
      "guid": "nlp_51da2c193b99c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>What's the Space Complexity?",
        "<b>O(n)</b><br><i>Linear time - grows proportionally with input size</i>",
        "Tokenization",
        "complexity"
      ],
      "guid": "nlp_19008083fa32e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "complexity"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>💡 Why is this important in interviews?",
        "<p><strong>DEMO: Show your thinking process</strong></p>",
        "Tokenization",
        "interview_insights"
      ],
      "guid": "nlp_7317ff70b6254",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>Alternative approach: Replace-then-split method</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_ff156324498a5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>Sometimes interviewers want to see multiple approaches</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_2bb91f4141a8a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>This is simpler</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_2ce37fa4b0c0a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization</b><br>Key concept:",
        "<p>less robust than regex.</p>",
        "Tokenization",
        "concepts"
      ],
      "guid": "nlp_714342ba60f79",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement byte pair encoding (bpe) tokenizer?",
        "<b>Task:</b> Build BPE vocabulary by iteratively merging most frequent pairs.",
        "Tokenization Advanced",
        "problem_understanding"
      ],
      "guid": "nlp_fad33c33f4b8b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>get_word_frequencies()</code>?",
        "<b>Purpose:</b> Get word frequencies with end-of-word marker<br><br><pre><code>def get_word_frequencies(texts: List[str]) -> Dict[str, int]:\n    \"\"\"Get word frequencies with end-of-word marker.\"\"\"\n    word_freqs = Counter()\n    \n    for text in texts:\n        words = text.lower().split()\n        for word in words:\n            # Add end-of-word marker\n            word_with_marker = ' '.join(word) + ' </w>'\n            word_freqs[word_with_marker] += 1\n    \n    return dict(word_freqs)</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_e7d1897228b2e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>get_pairs()</code>?",
        "<b>Purpose:</b> Get all adjacent character pairs with their frequencies<br><br><pre><code>def get_pairs(word_freqs: Dict[str, int]) -> Counter:\n    \"\"\"Get all adjacent character pairs with their frequencies.\"\"\"\n    pairs = Counter()\n    \n    for word, freq in word_freqs.items():\n        chars = word.split()\n        for i in range(len(chars) - 1):\n            pair = (chars[i], chars[i + 1])\n            pairs[pair] += freq\n    \n    return pairs</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_9dda63ccc2dca",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>merge_vocab()</code>?",
        "<b>Purpose:</b> Merge most frequent pair in vocabulary<br><br><pre><code>def merge_vocab(pair: Tuple[str, str], word_freqs: Dict[str, int]) -> Dict[str, int]:\n    \"\"\"Merge most frequent pair in vocabulary.\"\"\"\n    new_word_freqs = {}\n    bigram = re.escape(' '.join(pair))\n    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n    \n    for word, freq in word_freqs.items():\n        # Replace the pair with merged version\n        new_word = pattern.sub(''.join(pair), word)\n        new_word_freqs[new_word] = freq\n    \n    return new_word_freqs</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_480aaaa8060ce",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>build_bpe_vocab_part_1()</code>?",
        "<pre><code>def build_bpe_vocab(texts: List[str], num_merges: int = 10) -> Dict[str, int]:\n    \"\"\"Build BPE vocabulary through iterative merging.\"\"\"\n    if not texts:\n        return {}\n    \n    # Initialize with character-level words\n    word_freqs = get_word_frequencies(texts)\n    \n    # Start with character vocabulary\n    vocab = set()\n    for word in word_freqs:\n        vocab.update(word.split())\n    \n    # Perform merges\n    for i in range(num_merges):\n        pairs = get_pairs(word_freqs)</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_5b3879553a80d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>build_bpe_vocab_part_2()</code>?",
        "<pre><code>        \n        if not pairs:\n            break\n        \n        # Get most frequent pair\n        best_pair = pairs.most_common(1)[0][0]\n        \n        # Merge the pair\n        word_freqs = merge_vocab(best_pair, word_freqs)\n        \n        # Add merged token to vocabulary\n        vocab.add(''.join(best_pair))\n        \n        print(f\"Merge {i+1}: {best_pair[0]} + {best_pair[1]} -> {''.join(best_pair)}\")\n    </code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_65948ae83454a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>build_bpe_vocab_part_3()</code>?",
        "<pre><code>    # Create token to ID mapping\n    vocab_list = sorted(list(vocab))\n    vocab_dict = {token: idx for idx, token in enumerate(vocab_list)}\n    \n    return vocab_dict</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_cb838fd7d3591",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>bpe_encode_part_1()</code>?",
        "<pre><code>def bpe_encode(text: str, vocab: Dict[str, int]) -> List[int]:\n    \"\"\"Encode text using BPE vocabulary.\"\"\"\n    if not text:\n        return []\n    \n    # Start with character-level splitting\n    words = text.lower().split()\n    encoded = []\n    \n    for word in words:\n        # Convert word to character sequence with end marker\n        word_chars = list(word) + ['</w>']\n        \n        # Greedily apply merges (simplified - just use available tokens)\n        tokens = []\n        i = 0</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_e1d5231ef637f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>bpe_encode_part_2()</code>?",
        "<pre><code>        \n        while i < len(word_chars):\n            # Try to find longest matching token\n            found = False\n            \n            for length in range(min(len(word_chars) - i, 10), 0, -1):  # Max length 10\n                candidate = ''.join(word_chars[i:i+length])\n                \n                if candidate in vocab:\n                    tokens.append(vocab[candidate])\n                    i += length\n                    found = True\n                    break\n            \n            if not found:</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_9abeb117b82e7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>bpe_encode_part_3()</code>?",
        "<pre><code>                # Fallback to unknown token (use ID 0)\n                tokens.append(0)\n                i += 1\n        \n        encoded.extend(tokens)\n    \n    return encoded</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_20889208a52fa",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>bpe_decode()</code>?",
        "<b>Purpose:</b> Decode BPE tokens back to text<br><br><pre><code>def bpe_decode(token_ids: List[int], vocab: Dict[str, int]) -> str:\n    \"\"\"Decode BPE tokens back to text.\"\"\"\n    # Create reverse vocabulary\n    id_to_token = {idx: token for token, idx in vocab.items()}\n    \n    tokens = []\n    for token_id in token_ids:\n        if token_id in id_to_token:\n            tokens.append(id_to_token[token_id])\n    \n    # Join tokens and handle end-of-word markers\n    text = ''.join(tokens)\n    text = text.replace('</w>', ' ')\n    \n    return text.strip()</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_b3f0a69ea3ce8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>simulate_wordpiece_part_1()</code>?",
        "<pre><code>def simulate_wordpiece(text: str, vocab: Dict[str, int]) -> List[str]:\n    \"\"\"Simulate WordPiece tokenization (greedy longest-match).\"\"\"\n    words = text.lower().split()\n    subwords = []\n    \n    for word in words:\n        # Try to tokenize word using longest matching subwords\n        i = 0\n        word_subwords = []\n        \n        while i < len(word):\n            # Find longest matching subword\n            found = False\n            \n            for end in range(len(word), i, -1):\n                subword = word[i:end]</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_b4bdea7163cd9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>simulate_wordpiece_part_2()</code>?",
        "<pre><code>                \n                # Add ## prefix for continuation (except first subword)\n                if i > 0:\n                    subword = '##' + subword\n                \n                if subword in vocab or (i == 0 and subword in vocab):\n                    word_subwords.append(subword)\n                    i = end\n                    found = True\n                    break\n            \n            if not found:\n                # Fallback: use [UNK] token\n                word_subwords.append('[UNK]')\n                i += 1</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_00e719471bdd6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Tokenization Advanced</b><br>How do you implement <code>simulate_wordpiece_part_3()</code>?",
        "<pre><code>        \n        subwords.extend(word_subwords)\n    \n    return subwords</code></pre>",
        "Tokenization Advanced",
        "implementation"
      ],
      "guid": "nlp_5418947b7345b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "tokenization_advanced",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement topic modeling with lsa and lda?",
        "<b>Task:</b> Implement topic modeling algorithms:",
        "Topicmodeling",
        "problem_understanding"
      ],
      "guid": "nlp_ef8a949028de2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>perform_lsa()</code>?",
        "<b>Purpose:</b> Perform Latent Semantic Analysis<br><br><pre><code>def perform_lsa(documents: List[str], num_topics: int = 5) -> LSAModel:\n    \"\"\"Perform Latent Semantic Analysis.\"\"\"\n    model = LSAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_82f7749343ff2",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>perform_lda()</code>?",
        "<b>Purpose:</b> Perform Latent Dirichlet Allocation<br><br><pre><code>def perform_lda(documents: List[str], num_topics: int = 5) -> LDAModel:\n    \"\"\"Perform Latent Dirichlet Allocation.\"\"\"\n    model = LDAModel(num_topics=num_topics)\n    model.fit(documents)\n    return model</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_97f22b8056aec",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>extract_topics()</code>?",
        "<b>Purpose:</b> Extract topics from model<br><br><pre><code>def extract_topics(model: Union[LSAModel, LDAModel], \n                  num_words: int = 10) -> List[List[Tuple[str, float]]]:\n    \"\"\"Extract topics from model.\"\"\"\n    return model.get_topics(num_words)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_bfe33f7a106e4",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>get_document_topics()</code>?",
        "<b>Purpose:</b> Get topic distribution for a document<br><br><pre><code>def get_document_topics(model: Union[LSAModel, LDAModel], \n                       document: str) -> List[Tuple[int, float]]:\n    \"\"\"Get topic distribution for a document.\"\"\"\n    doc_topics = model.transform([document])[0]\n    \n    # Return as (topic_id, probability) pairs\n    topic_probs = []\n    for topic_idx, prob in enumerate(doc_topics):\n        if prob > 0.01:  # Threshold for relevance\n            topic_probs.append((topic_idx, prob))\n    \n    # Sort by probability\n    topic_probs.sort(key=lambda x: x[1], reverse=True)\n    return topic_probs</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_3495fb42448ed",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>calculate_coherence_score_part_1()</code>?",
        "<pre><code>def calculate_coherence_score(model: Union[LSAModel, LDAModel], \n                            documents: List[str], \n                            num_words: int = 10) -> float:\n    \"\"\"Calculate topic coherence score (simplified version).\"\"\"\n    topics = model.get_topics(num_words)\n    \n    # Simple coherence: average pairwise word co-occurrence\n    coherence_scores = []\n    \n    for topic_words in topics:\n        topic_coherence = 0\n        word_list = [word for word, _ in topic_words]\n        \n        # Count co-occurrences\n        for i in range(len(word_list)):\n            for j in range(i + 1, len(word_list)):\n                word1, word2 = word_list[i], word_list[j]\n                co_occur = 0\n                occur1 = 0</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_0845fbea588dd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>calculate_coherence_score_part_2()</code>?",
        "<pre><code>                \n                for doc in documents:\n                    doc_lower = doc.lower()\n                    if word1 in doc_lower and word2 in doc_lower:\n                        co_occur += 1\n                    if word1 in doc_lower:\n                        occur1 += 1\n                \n                if occur1 > 0:\n                    topic_coherence += co_occur / occur1\n        \n        # Average coherence for topic\n        if len(word_list) > 1:\n            topic_coherence /= (len(word_list) * (len(word_list) - 1) / 2)\n        \n        coherence_scores.append(topic_coherence)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_a041e00d2fe4a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>calculate_coherence_score_part_3()</code>?",
        "<pre><code>    \n    return np.mean(coherence_scores)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_4bb460a815f5a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>compare_topic_models_part_1()</code>?",
        "<pre><code>def compare_topic_models(documents: List[str], num_topics: int = 5):\n    \"\"\"Compare different topic modeling approaches.\"\"\"\n    results = {}\n    \n    # LSA\n    lsa_model = perform_lsa(documents, num_topics)\n    results['LSA'] = {\n        'topics': extract_topics(lsa_model, num_words=5),\n        'coherence': calculate_coherence_score(lsa_model, documents),\n        'explained_variance': lsa_model.svd.explained_variance_ratio_.sum()\n    }\n    \n    # LDA\n    lda_model = perform_lda(documents, num_topics)\n    results['LDA'] = {\n        'topics': extract_topics(lda_model, num_words=5),\n        'coherence': calculate_coherence_score(lda_model, documents)\n    }</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_671ce2ec726d0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>compare_topic_models_part_2()</code>?",
        "<pre><code>    \n    # NMF (Non-negative Matrix Factorization)\n    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n    doc_term_matrix = vectorizer.fit_transform(documents)\n    \n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(doc_term_matrix)\n    \n    vocabulary = vectorizer.get_feature_names_out()\n    nmf_topics = []\n    for topic_idx in range(num_topics):\n        word_scores = nmf.components_[topic_idx]\n        top_indices = np.argsort(word_scores)[-5:][::-1]\n        topic_words = [(vocabulary[idx], word_scores[idx]) for idx in top_indices]\n        nmf_topics.append(topic_words)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_f9332fcfd43fd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>compare_topic_models_part_3()</code>?",
        "<pre><code>    \n    results['NMF'] = {\n        'topics': nmf_topics,\n        'reconstruction_error': nmf.reconstruction_err_\n    }\n    \n    return results</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_fc4ad1236a721",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>__init__()</code>?",
        "<pre><code>    def __init__(self, num_topics: int = 5, use_tfidf: bool = True):\n        self.num_topics = num_topics\n        self.use_tfidf = use_tfidf\n        self.vectorizer = None\n        self.svd = None\n        self.document_topic_matrix = None\n        self.topic_word_matrix = None\n        self.vocabulary = None</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_8dbaadef407dd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>fit_part_1()</code>?",
        "<pre><code>    def fit(self, documents: List[str]):\n        \"\"\"Fit LSA model to documents.\"\"\"\n        # Vectorize documents\n        if self.use_tfidf:\n            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n        else:\n            self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n        \n        doc_term_matrix = self.vectorizer.fit_transform(documents)\n        self.vocabulary = self.vectorizer.get_feature_names_out()\n        \n        # Apply SVD\n        self.svd = TruncatedSVD(n_components=self.num_topics, random_state=42)\n        self.document_topic_matrix = self.svd.fit_transform(doc_term_matrix)\n        </code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_55334faba4c2f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>fit_part_2()</code>?",
        "<pre><code>        # Topic-word matrix (V^T in SVD)\n        self.topic_word_matrix = self.svd.components_\n        \n        # Normalize for interpretation\n        self.document_topic_matrix = normalize(self.document_topic_matrix, axis=1)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_6dc07477fd6f0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>get_topics_part_1()</code>?",
        "<pre><code>    def get_topics(self, num_words: int = 10) -> List[List[Tuple[str, float]]]:\n        \"\"\"Extract top words for each topic.\"\"\"\n        topics = []\n        \n        for topic_idx in range(self.num_topics):\n            # Get word scores for this topic\n            word_scores = self.topic_word_matrix[topic_idx]\n            \n            # Get top word indices\n            top_indices = np.argsort(word_scores)[-num_words:][::-1]\n            \n            # Create (word, score) pairs\n            topic_words = [(self.vocabulary[idx], word_scores[idx]) \n                          for idx in top_indices]\n            topics.append(topic_words)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_49a72653f697b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>get_topics_part_2()</code>?",
        "<pre><code>        \n        return topics</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_9501332893b39",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>transform()</code>?",
        "<b>Purpose:</b> Transform documents to topic space<br><br><pre><code>    def transform(self, documents: List[str]) -> np.ndarray:\n        \"\"\"Transform documents to topic space.\"\"\"\n        doc_term_matrix = self.vectorizer.transform(documents)\n        return self.svd.transform(doc_term_matrix)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_1bbd8bd142da1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>__init__()</code>?",
        "<pre><code>    def __init__(self, num_topics: int = 5, alpha: float = 0.1, beta: float = 0.01):\n        self.num_topics = num_topics\n        self.alpha = alpha  # Document-topic prior\n        self.beta = beta    # Topic-word prior\n        self.vectorizer = None\n        self.lda = None\n        self.vocabulary = None</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_bf981396680fc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>fit()</code>?",
        "<b>Purpose:</b> Fit LDA model to documents<br><br><pre><code>    def fit(self, documents: List[str], method: str = 'sklearn'):\n        \"\"\"Fit LDA model to documents.\"\"\"\n        if method == 'sklearn':\n            self._fit_sklearn(documents)\n        else:\n            self._fit_from_scratch(documents)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_b54a30c56b60b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_sklearn_part_1()</code>?",
        "<pre><code>    def _fit_sklearn(self, documents: List[str]):\n        \"\"\"Fit using sklearn's LDA.\"\"\"\n        # Use count vectorizer for LDA\n        self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n        doc_term_matrix = self.vectorizer.fit_transform(documents)\n        self.vocabulary = self.vectorizer.get_feature_names_out()\n        \n        # Fit LDA\n        self.lda = LatentDirichletAllocation(\n            n_components=self.num_topics,\n            doc_topic_prior=self.alpha,\n            topic_word_prior=self.beta,\n            random_state=42,\n            max_iter=10\n        )</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_32dad656fe97d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_sklearn_part_2()</code>?",
        "<pre><code>        \n        self.lda.fit(doc_term_matrix)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_43745000c50ad",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_from_scratch_part_1()</code>?",
        "<pre><code>    def _fit_from_scratch(self, documents: List[str]):\n        \"\"\"Simplified LDA using Gibbs sampling.\"\"\"\n        # Tokenize documents\n        tokenized_docs = []\n        word_to_id = {}\n        id_to_word = {}\n        word_id = 0\n        \n        for doc in documents:\n            tokens = doc.lower().split()\n            doc_tokens = []\n            for token in tokens:\n                if token not in word_to_id:\n                    word_to_id[token] = word_id\n                    id_to_word[word_id] = token\n                    word_id += 1\n                doc_tokens.append(word_to_id[token])\n            tokenized_docs.append(doc_tokens)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_e3a05fc51aea6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_from_scratch_part_2()</code>?",
        "<pre><code>        \n        self.vocabulary = list(word_to_id.keys())\n        vocab_size = len(self.vocabulary)\n        \n        # Initialize topic assignments randomly\n        doc_topic_counts = np.zeros((len(documents), self.num_topics))\n        topic_word_counts = np.zeros((self.num_topics, vocab_size))\n        topic_counts = np.zeros(self.num_topics)\n        \n        # Topic assignments for each word\n        topic_assignments = []\n        \n        for doc_idx, doc_tokens in enumerate(tokenized_docs):\n            doc_topics = []\n            for token in doc_tokens:</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_bbd025b3a0e4f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_from_scratch_part_3()</code>?",
        "<pre><code>                # Random initial assignment\n                topic = np.random.randint(self.num_topics)\n                doc_topics.append(topic)\n                \n                # Update counts\n                doc_topic_counts[doc_idx, topic] += 1\n                topic_word_counts[topic, token] += 1\n                topic_counts[topic] += 1\n            \n            topic_assignments.append(doc_topics)\n        \n        # Gibbs sampling (simplified - just a few iterations)\n        for iteration in range(50):\n            for doc_idx, doc_tokens in enumerate(tokenized_docs):\n                for word_idx, word_id in enumerate(doc_tokens):</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_ec67b538e90fe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_from_scratch_part_4()</code>?",
        "<pre><code>                    # Current topic\n                    old_topic = topic_assignments[doc_idx][word_idx]\n                    \n                    # Remove from counts\n                    doc_topic_counts[doc_idx, old_topic] -= 1\n                    topic_word_counts[old_topic, word_id] -= 1\n                    topic_counts[old_topic] -= 1\n                    \n                    # Calculate probabilities for each topic\n                    probs = np.zeros(self.num_topics)\n                    for topic in range(self.num_topics):\n                        # P(topic|doc) * P(word|topic)\n                        doc_topic_prob = (doc_topic_counts[doc_idx, topic] + self.alpha) / \\\n                                       (len(doc_tokens) - 1 + self.num_topics * self.alpha)\n                        \n                        topic_word_prob = (topic_word_counts[topic, word_id] + self.beta) / \\\n                                        (topic_counts[topic] + vocab_size * self.beta)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_09ac4bf099c8a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_from_scratch_part_5()</code>?",
        "<pre><code>                        \n                        probs[topic] = doc_topic_prob * topic_word_prob\n                    \n                    # Sample new topic\n                    probs /= probs.sum()\n                    new_topic = np.random.choice(self.num_topics, p=probs)\n                    \n                    # Update assignments and counts\n                    topic_assignments[doc_idx][word_idx] = new_topic\n                    doc_topic_counts[doc_idx, new_topic] += 1\n                    topic_word_counts[new_topic, word_id] += 1\n                    topic_counts[new_topic] += 1\n        \n        # Store final distributions\n        self.doc_topic_dist = doc_topic_counts + self.alpha\n        self.doc_topic_dist /= self.doc_topic_dist.sum(axis=1, keepdims=True)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_01fb18f53beda",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>_fit_from_scratch_part_6()</code>?",
        "<pre><code>        \n        self.topic_word_dist = topic_word_counts + self.beta\n        self.topic_word_dist /= self.topic_word_dist.sum(axis=1, keepdims=True)</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_62e5285d14bf0",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>get_topics_part_1()</code>?",
        "<pre><code>    def get_topics(self, num_words: int = 10) -> List[List[Tuple[str, float]]]:\n        \"\"\"Extract top words for each topic.\"\"\"\n        topics = []\n        \n        if hasattr(self, 'lda') and self.lda is not None:\n            # sklearn LDA\n            for topic_idx in range(self.num_topics):\n                word_scores = self.lda.components_[topic_idx]\n                top_indices = np.argsort(word_scores)[-num_words:][::-1]\n                \n                topic_words = [(self.vocabulary[idx], word_scores[idx]) \n                              for idx in top_indices]\n                topics.append(topic_words)\n        else:\n            # From scratch implementation\n            for topic_idx in range(self.num_topics):\n                word_scores = self.topic_word_dist[topic_idx]\n                top_indices = np.argsort(word_scores)[-num_words:][::-1]</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_e210f8bb6917e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>get_topics_part_2()</code>?",
        "<pre><code>                \n                topic_words = [(self.vocabulary[idx], word_scores[idx]) \n                              for idx in top_indices]\n                topics.append(topic_words)\n        \n        return topics</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_691e49eeb00fb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>How do you implement <code>transform()</code>?",
        "<b>Purpose:</b> Transform documents to topic space<br><br><pre><code>    def transform(self, documents: List[str]) -> np.ndarray:\n        \"\"\"Transform documents to topic space.\"\"\"\n        if hasattr(self, 'lda') and self.lda is not None:\n            doc_term_matrix = self.vectorizer.transform(documents)\n            return self.lda.transform(doc_term_matrix)\n        else:\n            # Simplified: return uniform distribution\n            return np.ones((len(documents), self.num_topics)) / self.num_topics</code></pre>",
        "Topicmodeling",
        "implementation"
      ],
      "guid": "nlp_af9b1fcc49839",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Calculate topic coherence score (simplified version).</strong></p>",
        "Topicmodeling",
        "interview_insights"
      ],
      "guid": "nlp_8503b0eca314b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Topicmodeling</b><br>Key concept:",
        "<p>Compare different topic modeling approaches.</p>",
        "Topicmodeling",
        "concepts"
      ],
      "guid": "nlp_5a30e433cb5ae",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "topicmodeling",
        "concepts"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement bert fine-tuning for sentiment analysis?",
        "<b>Task:</b> Implement BERT fine-tuning for sentiment classification:",
        "Transformers",
        "problem_understanding"
      ],
      "guid": "nlp_e614c02a4d965",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>load_pretrained_bert_part_1()</code>?",
        "<pre><code>def load_pretrained_bert(model_name: str = 'bert-base-uncased', \n                        task: str = 'custom') -> Tuple[Union[nn.Module, any], any]:\n    \"\"\"Load pre-trained BERT model and tokenizer.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return None, None\n    \n    if task == 'custom':\n        # Custom model with our classification head\n        model = BERTSentimentClassifier(model_name)\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n    else:\n        # Hugging Face model for sequence classification\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_name, num_labels=2\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_8098d753f880d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>load_pretrained_bert_part_2()</code>?",
        "<pre><code>    \n    return model, tokenizer</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_d5e27cadc6e33",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>fine_tune_bert_part_1()</code>?",
        "<pre><code>def fine_tune_bert(model: nn.Module, texts: List[str], labels: List[int],\n                  tokenizer, epochs: int = 3, batch_size: int = 16,\n                  learning_rate: float = 2e-5, warmup_steps: int = 0) -> nn.Module:\n    \"\"\"Fine-tune BERT for sentiment analysis.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return model\n    \n    # Create dataset and dataloader\n    dataset = SentimentDataset(texts, labels, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Setup optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n    \n    total_steps = len(dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_45a1842ff0c1e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>fine_tune_bert_part_2()</code>?",
        "<pre><code>    \n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # Training loop\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch in dataloader:</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_bc407417d8aa3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>fine_tune_bert_part_3()</code>?",
        "<pre><code>            # Move to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            \n            if isinstance(model, BERTSentimentClassifier):\n                logits, _, _ = model(input_ids, attention_mask)\n            else:\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                logits = outputs.logits\n                loss = outputs.loss\n            \n            if isinstance(model, BERTSentimentClassifier):\n                loss = criterion(logits, labels)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_04152c54d7646",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>fine_tune_bert_part_4()</code>?",
        "<pre><code>            \n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            \n            # Track metrics\n            total_loss += loss.item()\n            _, predicted = torch.max(logits, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n        \n        avg_loss = total_loss / len(dataloader)\n        accuracy = correct / total</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_cef4a43d8c9cf",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>fine_tune_bert_part_5()</code>?",
        "<pre><code>        \n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"Average Loss: {avg_loss:.4f}\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n    \n    return model</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_0d611463be4c8",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>predict_with_bert_part_1()</code>?",
        "<pre><code>def predict_with_bert(model: nn.Module, texts: List[str], tokenizer,\n                     return_attention: bool = True) -> List[Tuple[str, float, Dict]]:\n    \"\"\"Make predictions with BERT model.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return []\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n    \n    predictions = []\n    \n    with torch.no_grad():\n        for text in texts:\n            # Tokenize\n            encoding = tokenizer(\n                text,\n                truncation=True,\n                padding='max_length',\n                max_length=128,\n                return_tensors='pt'\n            )</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_fc5060e53e054",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>predict_with_bert_part_2()</code>?",
        "<pre><code>            \n            input_ids = encoding['input_ids'].to(device)\n            attention_mask = encoding['attention_mask'].to(device)\n            \n            # Get predictions\n            if isinstance(model, BERTSentimentClassifier):\n                logits, attentions, cls_embedding = model(input_ids, attention_mask)\n            else:\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs.logits\n                attentions = outputs.attentions if hasattr(outputs, 'attentions') else None\n                cls_embedding = outputs.hidden_states[-1][:, 0, :] if hasattr(outputs, 'hidden_states') else None\n            \n            # Get probabilities\n            probs = torch.softmax(logits, dim=-1)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_65fa07745b3c7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>predict_with_bert_part_3()</code>?",
        "<pre><code>            predicted_class = torch.argmax(probs, dim=-1).item()\n            confidence = probs[0, predicted_class].item()\n            \n            # Sentiment label\n            sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n            \n            # Additional information\n            extra_info = {}\n            \n            if return_attention and attentions is not None:\n                # Average attention across all layers and heads\n                avg_attention = torch.stack(attentions).mean(dim=(0, 1))[0]\n                extra_info['attention_scores'] = avg_attention.cpu().numpy().tolist()\n            \n            if cls_embedding is not None:\n                extra_info['cls_embedding'] = cls_embedding[0].cpu().numpy().tolist()[:10]  # First 10 dims</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_853c61cd16715",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>predict_with_bert_part_4()</code>?",
        "<pre><code>            \n            predictions.append((sentiment, confidence, extra_info))\n    \n    return predictions</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_9a9b1204a329b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>compare_transformer_models_part_1()</code>?",
        "<pre><code>def compare_transformer_models(texts: List[str], labels: List[int],\n                             models: List[str] = None) -> Dict[str, Dict]:\n    \"\"\"Compare different transformer models.\"\"\"\n    if not TRANSFORMERS_AVAILABLE:\n        return {}\n    \n    if models is None:\n        models = ['bert-base-uncased', 'distilbert-base-uncased', 'roberta-base']\n    \n    results = {}\n    \n    for model_name in models:\n        print(f\"\\nEvaluating {model_name}...\")\n        \n        try:</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_3b48f2480f14b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>compare_transformer_models_part_2()</code>?",
        "<pre><code>            # Load model and tokenizer\n            if 'bert' in model_name and 'distil' not in model_name:\n                tokenizer = BertTokenizer.from_pretrained(model_name)\n                model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n            elif 'distilbert' in model_name:\n                tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n                model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n            elif 'roberta' in model_name:\n                tokenizer = RobertaTokenizer.from_pretrained(model_name)\n                model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n            \n            # Count parameters\n            total_params = sum(p.numel() for p in model.parameters())\n            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n            \n            results[model_name] = {\n                'total_parameters': total_params,\n                'trainable_parameters': trainable_params,\n                'model_size_mb': total_params * 4 / 1024 / 1024  # Assuming float32\n            }</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_3b48360158cd9",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>compare_transformer_models_part_3()</code>?",
        "<pre><code>            \n            print(f\"Total parameters: {total_params:,}\")\n            print(f\"Model size: {results[model_name]['model_size_mb']:.2f} MB\")\n            \n        except Exception as e:\n            print(f\"Error loading {model_name}: {e}\")\n            results[model_name] = {'error': str(e)}\n    \n    return results</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_54cfeb0b3d05b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>few_shot_sentiment_part_1()</code>?",
        "<pre><code>def few_shot_sentiment(model, tokenizer, examples: List[Tuple[str, str]], \n                      query: str) -> Tuple[str, float]:\n    \"\"\"Few-shot learning with prompts (for GPT-style models).\"\"\"\n    # Create prompt\n    prompt = \"Classify the sentiment of the following texts:\\n\\n\"\n    \n    for text, label in examples:\n        prompt += f\"Text: {text}\\nSentiment: {label}\\n\\n\"\n    \n    prompt += f\"Text: {query}\\nSentiment:\"\n    \n    # This is a simplified version - in practice, you'd use a generative model\n    # For BERT, we'll just use the fine-tuned classifier\n    predictions = predict_with_bert(model, [query], tokenizer, return_attention=False)\n    \n    if predictions:\n        sentiment, confidence, _ = predictions[0]\n        return sentiment, confidence</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_8c67d72218f15",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>few_shot_sentiment_part_2()</code>?",
        "<pre><code>    \n    return \"unknown\", 0.0</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_5bf55be454803",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>visualize_attention()</code>?",
        "<b>Purpose:</b> Simple text-based attention visualization<br><br><pre><code>def visualize_attention(text: str, tokens: List[str], attention_scores: List[float]):\n    \"\"\"Simple text-based attention visualization.\"\"\"\n    print(f\"\\nText: '{text}'\")\n    print(\"\\nToken attention scores:\")\n    \n    max_score = max(attention_scores) if attention_scores else 1\n    \n    for token, score in zip(tokens, attention_scores):\n        # Normalize to 0-20 scale for visualization\n        bar_length = int((score / max_score) * 20)\n        bar = '█' * bar_length\n        print(f\"{token:15} {bar} {score:.3f}\")</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_1b9acd5bd81d5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>create_imdb_sample_data_part_1()</code>?",
        "<pre><code>def create_imdb_sample_data() -> Tuple[List[str], List[int], List[str], List[int]]:\n    \"\"\"Create sample IMDB-style movie review data.\"\"\"\n    train_texts = [\n        \"This movie is a masterpiece. The acting is superb and the story is captivating.\",\n        \"Terrible film. Waste of time and money. Poor acting and boring plot.\",\n        \"Absolutely loved it! One of the best movies I've seen this year.\",\n        \"Disappointing. Had high expectations but the movie failed to deliver.\",\n        \"Brilliant cinematography and outstanding performances by all actors.\",\n        \"Couldn't even finish watching it. Extremely boring and predictable.\",\n        \"A true work of art. Every scene is beautifully crafted.\",\n        \"Not worth the hype. Mediocre at best.\",\n        \"Exceptional movie that will be remembered for years to come.\",\n        \"One of the worst movies ever made. Avoid at all costs.\"\n    ]\n    \n    train_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_d1819bafd2a51",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>create_imdb_sample_data_part_2()</code>?",
        "<pre><code>    \n    test_texts = [\n        \"Amazing film with incredible performances!\",\n        \"Boring and poorly executed. Not recommended.\",\n        \"Decent movie but nothing groundbreaking.\"\n    ]\n    \n    test_labels = [1, 0, 1]\n    \n    return train_texts, train_labels, test_texts, test_labels</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_325c756626e0c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>__init__()</code>?",
        "<pre><code>    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_9297ba018705d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>__len__()</code>?",
        "<pre><code>    def __len__(self):\n        return len(self.texts)</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_6de35bdd81165",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>__getitem___part_1()</code>?",
        "<pre><code>    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_8fa9baa02e094",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>__init___part_1()</code>?",
        "<pre><code>    def __init__(self, model_name: str = 'bert-base-uncased', num_classes: int = 2, \n                 dropout: float = 0.3, freeze_bert: bool = False):\n        super().__init__()\n        \n        # Load pre-trained BERT\n        self.bert = BertModel.from_pretrained(model_name, output_attentions=True)\n        \n        # Freeze BERT parameters if specified\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n        # Classification head\n        hidden_size = self.bert.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, num_classes)\n        )</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_c2f53ee012dc1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>How do you implement <code>forward()</code>?",
        "<pre><code>    def forward(self, input_ids, attention_mask):\n        # Get BERT outputs\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Use [CLS] token representation\n        pooled_output = outputs.pooler_output\n        \n        # Classification\n        logits = self.classifier(pooled_output)\n        \n        return logits, outputs.attentions, pooled_output</code></pre>",
        "Transformers",
        "implementation"
      ],
      "guid": "nlp_5172c33c8ebfc",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>logits = self.classifier(pooled_output)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_7c8caed00f55a",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>logits, _, _ = model(input_ids, attention_mask)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_71bb9a0184ae3",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>logits = outputs.logits</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_00024fa17c76b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>loss = criterion(logits, labels)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_3a6a5b3a0c785",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>_, predicted = torch.max(logits, 1)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_20ab487ce49dd",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>logits, attentions, cls_embedding = model(input_ids, attention_mask)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_bcb3e9c9d485b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>logits = outputs.logits</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_00024fa17c76b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>Write the Key Formula",
        "<h3>probs = torch.softmax(logits, dim=-1)</h3><br><p><i>Mathematical relationship used in this algorithm</i></p>",
        "Transformers",
        "formula"
      ],
      "guid": "nlp_436826860ab0c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Transformers enable parallel computation unlike RNNs</strong></p>",
        "Transformers",
        "interview_insights"
      ],
      "guid": "nlp_d392630170e75",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Transformers</b><br>🔍 Why is this important in interviews?",
        "<p><strong>Multi-head attention captures different relationship types</strong></p>",
        "Transformers",
        "interview_insights"
      ],
      "guid": "nlp_e35ff88ba47a6",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "transformers",
        "interview_insights"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement comprehensive text normalization pipeline?",
        "<b>Task:</b> Build a complete text normalization system:",
        "Utilities",
        "problem_understanding"
      ],
      "guid": "nlp_e224edc9d8634",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "problem_understanding"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>expand_contractions_part_1()</code>?",
        "<pre><code>def expand_contractions(text: str) -> str:\n    \"\"\"Expand contractions in text.\"\"\"\n    # Convert to lowercase for matching\n    text_lower = text.lower()\n    \n    # Sort contractions by length (descending) to match longer ones first\n    sorted_contractions = sorted(CONTRACTIONS.items(), key=lambda x: len(x[0]), reverse=True)\n    \n    for contraction, expansion in sorted_contractions:\n        # Use word boundaries for accurate matching\n        pattern = r'\\b' + re.escape(contraction) + r'\\b'\n        text_lower = re.sub(pattern, expansion, text_lower, flags=re.IGNORECASE)\n    \n    # Preserve original capitalization pattern\n    result = []\n    for i, char in enumerate(text):\n        if i < len(text_lower):\n            if char.isupper():\n                result.append(text_lower[i].upper())\n            else:\n                result.append(text_lower[i])\n        else:\n            result.append(char)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_8526c2b794d09",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>expand_contractions_part_2()</code>?",
        "<pre><code>    \n    return ''.join(result)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_3526354b06c58",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>expand_abbreviations()</code>?",
        "<b>Purpose:</b> Expand common abbreviations<br><br><pre><code>def expand_abbreviations(text: str) -> str:\n    \"\"\"Expand common abbreviations.\"\"\"\n    for abbr, expansion in ABBREVIATIONS.items():\n        # Handle period after abbreviation\n        pattern1 = r'\\b' + re.escape(abbr) + r'\\.\\b'\n        pattern2 = r'\\b' + re.escape(abbr) + r'\\b'\n        \n        text = re.sub(pattern1, expansion, text, flags=re.IGNORECASE)\n        text = re.sub(pattern2, expansion, text, flags=re.IGNORECASE)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_9aa25bbf2d680",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>clean_html_part_1()</code>?",
        "<pre><code>def clean_html(html_text: str) -> str:\n    \"\"\"Remove HTML tags and decode entities.\"\"\"\n    # Remove script and style content\n    html_text = re.sub(r'<script[^>]*>.*?</script>', '', html_text, flags=re.DOTALL | re.IGNORECASE)\n    html_text = re.sub(r'<style[^>]*>.*?</style>', '', html_text, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Remove HTML tags\n    html_text = re.sub(r'<[^>]+>', ' ', html_text)\n    \n    # Decode HTML entities\n    text = html.unescape(html_text)\n    \n    # Clean up whitespace\n    text = ' '.join(text.split())\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_f4591ef5e18bb",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_unicode_part_1()</code>?",
        "<pre><code>def normalize_unicode(text: str) -> str:\n    \"\"\"Normalize Unicode characters.\"\"\"\n    # Normalize to NFKD form\n    text = unicodedata.normalize('NFKD', text)\n    \n    # Replace special quotes and dashes\n    replacements = {\n        '\"': '\"',\n        '\"': '\"',\n        ''': \"'\",\n        ''': \"'\",\n        '–': '-',\n        '—': '-',\n        '…': '...',\n        '•': '*',\n        '®': '(R)',\n        '™': '(TM)',\n        '©': '(C)',\n        '°': ' degrees',\n        '£': 'GBP',\n        '€': 'EUR',\n        '¥': 'JPY',\n        '₹': 'INR'\n    }</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_6d25df359f33d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_unicode_part_2()</code>?",
        "<pre><code>    \n    for old, new in replacements.items():\n        text = text.replace(old, new)\n    \n    # Remove non-ASCII characters that can't be handled\n    # text = ''.join(char if ord(char) < 128 else ' ' for char in text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_1eb2b228f3c4f",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_whitespace()</code>?",
        "<b>Purpose:</b> Normalize whitespace in text<br><br><pre><code>def normalize_whitespace(text: str) -> str:\n    \"\"\"Normalize whitespace in text.\"\"\"\n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove space before punctuation\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n    \n    # Add space after punctuation if missing\n    text = re.sub(r'([.,!?;:])([A-Za-z])', r'\\1 \\2', text)\n    \n    # Remove leading/trailing whitespace\n    text = text.strip()\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_cd1b42e198345",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_punctuation_part_1()</code>?",
        "<pre><code>def normalize_punctuation(text: str) -> str:\n    \"\"\"Normalize punctuation in text.\"\"\"\n    # Replace multiple punctuation with single\n    text = re.sub(r'\\.{2,}', '.', text)  # Multiple dots to single\n    text = re.sub(r'!{2,}', '!', text)   # Multiple exclamations\n    text = re.sub(r'\\?{2,}', '?', text)  # Multiple questions\n    \n    # Normalize ellipsis\n    text = re.sub(r'\\.{3,}', '...', text)\n    \n    # Fix common punctuation errors\n    text = re.sub(r'\\s*,\\s*', ', ', text)  # Space after comma\n    text = re.sub(r'\\s*\\.\\s*', '. ', text)  # Space after period\n    text = re.sub(r'\\s*!\\s*', '! ', text)   # Space after exclamation\n    text = re.sub(r'\\s*\\?\\s*', '? ', text)  # Space after question</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_99609156e1582",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_punctuation_part_2()</code>?",
        "<pre><code>    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_c6f463fc5e10e",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_urls_part_1()</code>?",
        "<pre><code>def handle_urls(text: str, keep_domain: bool = True) -> str:\n    \"\"\"Handle URLs in text.\"\"\"\n    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&/=]*)'\n    \n    if keep_domain:\n        def replace_url(match):\n            url = match.group(0)\n            try:\n                parsed = urlparse(url)\n                domain = parsed.netloc.replace('www.', '')\n                return domain\n            except:\n                return 'URL'\n    else:\n        def replace_url(match):\n            return 'URL'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_c52bdba7a943c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_urls_part_2()</code>?",
        "<pre><code>    \n    text = re.sub(url_pattern, replace_url, text)\n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_a2a774c683f80",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_emails_part_1()</code>?",
        "<pre><code>def handle_emails(text: str, anonymize: bool = True) -> str:\n    \"\"\"Handle email addresses in text.\"\"\"\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    \n    if anonymize:\n        text = re.sub(email_pattern, 'EMAIL', text)\n    else:\n        # Keep domain only\n        def replace_email(match):\n            email = match.group(0)\n            domain = email.split('@')[1]\n            return f'user@{domain}'\n        \n        text = re.sub(email_pattern, replace_email, text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_8201ab34e959c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_phone_numbers()</code>?",
        "<b>Purpose:</b> Handle phone numbers in text<br><br><pre><code>def handle_phone_numbers(text: str) -> str:\n    \"\"\"Handle phone numbers in text.\"\"\"\n    # US phone numbers\n    phone_patterns = [\n        r'\\b\\+?1?\\s*\\(?([0-9]{3})\\)?[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})\\b',\n        r'\\b([0-9]{3})[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})\\b',\n    ]\n    \n    for pattern in phone_patterns:\n        text = re.sub(pattern, 'PHONE', text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_212d0644334c5",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_numbers_part_1()</code>?",
        "<pre><code>def handle_numbers(text: str, spell_out: bool = False) -> str:\n    \"\"\"Handle numbers in text.\"\"\"\n    if spell_out:\n        # Simple number to word conversion for small numbers\n        num_words = {\n            '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n            '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine',\n            '10': 'ten', '11': 'eleven', '12': 'twelve'\n        }\n        \n        for num, word in num_words.items():\n            text = re.sub(r'\\b' + num + r'\\b', word, text)\n    \n    # Normalize number formats\n    text = re.sub(r'\\b(\\d+),(\\d+)\\b', r'\\1\\2', text)  # Remove commas from numbers</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_a01a778738be7",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_numbers_part_2()</code>?",
        "<pre><code>    \n    # Handle percentages\n    text = re.sub(r'(\\d+)\\s*%', r'\\1 percent', text)\n    \n    # Handle currency\n    text = re.sub(r'\\$\\s*(\\d+)', r'\\1 dollars', text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_3d9d7e0bb3c25",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_social_media_part_1()</code>?",
        "<pre><code>def handle_social_media(text: str, preserve_mentions: bool = True, preserve_hashtags: bool = True) -> str:\n    \"\"\"Handle social media specific elements.\"\"\"\n    if not preserve_mentions:\n        # Remove @mentions\n        text = re.sub(r'@\\w+', '', text)\n    else:\n        # Normalize mentions\n        text = re.sub(r'@(\\w+)', r'USER_\\1', text)\n    \n    if not preserve_hashtags:\n        # Remove hashtags\n        text = re.sub(r'#\\w+', '', text)\n    else:\n        # Convert hashtags to normal words\n        text = re.sub(r'#(\\w+)', r'\\1', text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_124f584463005",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>handle_social_media_part_2()</code>?",
        "<pre><code>    \n    # Handle retweet notation\n    text = re.sub(r'\\bRT\\s+:', '', text)\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_afbdb55249611",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>remove_emojis()</code>?",
        "<b>Purpose:</b> Remove emoji characters from text<br><br><pre><code>def remove_emojis(text: str) -> str:\n    \"\"\"Remove emoji characters from text.\"\"\"\n    emoji_pattern = re.compile(\n        \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        \"]+\", flags=re.UNICODE\n    )\n    \n    return emoji_pattern.sub('', text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_8c34a57d02cfe",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_text_part_1()</code>?",
        "<pre><code>def normalize_text(text: str, options: Optional[Dict[str, bool]] = None) -> str:\n    \"\"\"Main text normalization function with configurable options.\"\"\"\n    if options is None:\n        options = {\n            'lowercase': False,\n            'expand_contractions': True,\n            'expand_abbreviations': True,\n            'handle_urls': True,\n            'handle_emails': True,\n            'handle_phones': True,\n            'handle_numbers': True,\n            'remove_emojis': True,\n            'normalize_unicode': True,\n            'normalize_whitespace': True,\n            'normalize_punctuation': True,\n            'handle_social_media': True\n        }</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_b14faae9cd17c",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_text_part_2()</code>?",
        "<pre><code>    \n    # Apply normalization steps in order\n    if options.get('normalize_unicode', True):\n        text = normalize_unicode(text)\n    \n    if options.get('expand_contractions', True):\n        text = expand_contractions(text)\n    \n    if options.get('expand_abbreviations', True):\n        text = expand_abbreviations(text)\n    \n    if options.get('handle_urls', True):\n        text = handle_urls(text)\n    \n    if options.get('handle_emails', True):\n        text = handle_emails(text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_dc7c86c55af48",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_text_part_3()</code>?",
        "<pre><code>    \n    if options.get('handle_phones', True):\n        text = handle_phone_numbers(text)\n    \n    if options.get('handle_numbers', True):\n        text = handle_numbers(text)\n    \n    if options.get('remove_emojis', True):\n        text = remove_emojis(text)\n    \n    if options.get('handle_social_media', True):\n        text = handle_social_media(text)\n    \n    if options.get('normalize_punctuation', True):\n        text = normalize_punctuation(text)</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_f81e2da49c12b",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>normalize_text_part_4()</code>?",
        "<pre><code>    \n    if options.get('normalize_whitespace', True):\n        text = normalize_whitespace(text)\n    \n    if options.get('lowercase', False):\n        text = text.lower()\n    \n    return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_cacdaa7ea5998",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>batch_normalize()</code>?",
        "<b>Purpose:</b> Normalize multiple texts with the same options<br><br><pre><code>def batch_normalize(texts: List[str], options: Optional[Dict[str, bool]] = None) -> List[str]:\n    \"\"\"Normalize multiple texts with the same options.\"\"\"\n    return [normalize_text(text, options) for text in texts]</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_4264d52db4d90",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>create_custom_normalizer_part_1()</code>?",
        "<pre><code>def create_custom_normalizer(preserve_entities: List[str]) -> callable:\n    \"\"\"Create a custom normalizer that preserves specific entities.\"\"\"\n    def custom_normalize(text: str) -> str:\n        # Store entities and their positions\n        preserved = []\n        \n        for entity in preserve_entities:\n            pattern = re.escape(entity)\n            for match in re.finditer(pattern, text, re.IGNORECASE):\n                preserved.append((match.start(), match.end(), match.group()))\n        \n        # Sort by position (reverse order for replacement)\n        preserved.sort(reverse=True)\n        \n        # Replace with placeholders\n        for i, (start, end, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text[:start] + placeholder + text[end:]</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_30ffdff5bff9d",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>create_custom_normalizer_part_2()</code>?",
        "<pre><code>        \n        # Normalize\n        text = normalize_text(text)\n        \n        # Restore entities\n        for i, (_, _, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text.replace(placeholder, entity)\n        \n        return text\n    \n    return custom_normalize</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_6d3feb57987ad",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>get_normalization_stats()</code>?",
        "<b>Purpose:</b> Get statistics about the normalization process<br><br><pre><code>def get_normalization_stats(original: str, normalized: str) -> Dict[str, int]:\n    \"\"\"Get statistics about the normalization process.\"\"\"\n    return {\n        'original_length': len(original),\n        'normalized_length': len(normalized),\n        'characters_removed': len(original) - len(normalized),\n        'original_words': len(original.split()),\n        'normalized_words': len(normalized.split()),\n        'reduction_percentage': round((1 - len(normalized) / len(original)) * 100, 2)\n    }</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_6898ae13d6404",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>custom_normalize_part_1()</code>?",
        "<pre><code>    def custom_normalize(text: str) -> str:\n        # Store entities and their positions\n        preserved = []\n        \n        for entity in preserve_entities:\n            pattern = re.escape(entity)\n            for match in re.finditer(pattern, text, re.IGNORECASE):\n                preserved.append((match.start(), match.end(), match.group()))\n        \n        # Sort by position (reverse order for replacement)\n        preserved.sort(reverse=True)\n        \n        # Replace with placeholders\n        for i, (start, end, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text[:start] + placeholder + text[end:]</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_75e2126997fd1",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>custom_normalize_part_2()</code>?",
        "<pre><code>        \n        # Normalize\n        text = normalize_text(text)\n        \n        # Restore entities\n        for i, (_, _, entity) in enumerate(preserved):\n            placeholder = f\"__ENTITY_{i}__\"\n            text = text.replace(placeholder, entity)\n        \n        return text</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_934b9754f0318",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>replace_url()</code>?",
        "<pre><code>        def replace_url(match):\n            url = match.group(0)\n            try:\n                parsed = urlparse(url)\n                domain = parsed.netloc.replace('www.', '')\n                return domain\n            except:\n                return 'URL'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_dd38192501915",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>replace_url()</code>?",
        "<pre><code>        def replace_url(match):\n            return 'URL'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_f0c02615ffe66",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "<b>Utilities</b><br>How do you implement <code>replace_email()</code>?",
        "<pre><code>        def replace_email(match):\n            email = match.group(0)\n            domain = email.split('@')[1]\n            return f'user@{domain}'</code></pre>",
        "Utilities",
        "implementation"
      ],
      "guid": "nlp_e99922757d992",
      "note_model_uuid": "nlp-model-rules-optimized",
      "tags": [
        "utilities",
        "implementation"
      ]
    }
  ]
}