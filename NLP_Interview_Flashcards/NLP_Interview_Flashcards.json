{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-optimal-interview-deck-2024",
  "deck_config_uuid": "nlp-optimal-deck-config",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "autoplay": true,
      "crowdanki_uuid": "nlp-optimal-deck-config",
      "dyn": false,
      "name": "NLP Interview Flashcards (Optimal)",
      "new": {
        "delays": [
          1,
          10
        ],
        "initialFactor": 2500,
        "ints": [
          1,
          4,
          7
        ],
        "order": 1,
        "perDay": 20
      },
      "rev": {
        "ease4": 1.3,
        "hardFactor": 1.2,
        "ivlFct": 1.0,
        "maxIvl": 36500,
        "perDay": 100
      }
    }
  ],
  "desc": "Research-backed NLP interview flashcards. High-level concepts, formulas with definitions, 15-20 second review time.",
  "dyn": 0,
  "extendNew": 10,
  "extendRev": 50,
  "media_files": [],
  "name": "NLP Interview Flashcards (Optimal)",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "nlp-optimal-model",
      "css": "\n.card {\n    font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n    font-size: 18px;\n    line-height: 1.5;\n    color: #2c3e50;\n    background: #f8f9fa;\n    text-align: left;\n    padding: 20px;\n    border-radius: 8px;\n    max-width: 100%;\n    margin: 0 auto;\n}\n\n@media (max-width: 480px) {\n    .card {\n        font-size: 16px;\n        padding: 15px;\n        line-height: 1.4;\n    }\n}\n\nb, strong {\n    color: #2980b9;\n    font-weight: 600;\n}\n\n.metadata {\n    margin-top: 15px;\n    padding-top: 10px;\n    border-top: 1px solid #bdc3c7;\n    font-size: 12px;\n    color: #7f8c8d;\n    text-align: center;\n}\n",
      "flds": [
        {
          "name": "Front",
          "ord": 0,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Back",
          "ord": 1,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 20
        },
        {
          "name": "Topic",
          "ord": 2,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 16
        },
        {
          "name": "Type",
          "ord": 3,
          "sticky": false,
          "rtl": false,
          "font": "Arial",
          "size": 14
        }
      ],
      "latexPost": "\\end{document}",
      "latexPre": "\\documentclass[12pt]{article}\\special{papersize=3in,5in}\\usepackage{amssymb,amsmath}\\pagestyle{empty}\\setlength{\\parindent}{0in}\\begin{document}",
      "name": "NLP Optimal",
      "req": [
        [
          0,
          "all",
          [
            0
          ]
        ]
      ],
      "sortf": 0,
      "tags": [],
      "tmpls": [
        {
          "afmt": "{{FrontSide}}<hr id=answer>{{Back}}<br><br><div class='metadata'><span style='background: #3498db; color: white; padding: 4px 8px; border-radius: 4px; font-weight: 600;'>{{Topic}}</span> • <span style='background: #95a5a6; color: white; padding: 4px 8px; border-radius: 4px; margin-left: 8px;'>{{Type}}</span></div>",
          "bafmt": "",
          "bqfmt": "",
          "did": null,
          "name": "Card 1",
          "ord": 0,
          "qfmt": "{{Front}}"
        }
      ],
      "type": 0,
      "vers": []
    }
  ],
  "notes": [
    {
      "__type__": "Note",
      "fields": [
        "What is self-attention?",
        "<b>Intuition:</b> Weigh tokens by relevance to each other in sequence<br>\n<b>Formula:</b> Attention(Q,K,V) = softmax(QK^T/√d_k)V<br>\n<b>Symbols:</b> Q=query, K=key, V=value, d_k=key dimension<br>\n<b>Why:</b> Enables parallel computation and long-range dependencies",
        "Attention Mechanisms",
        "concept"
      ],
      "guid": "nlp_a1f4140e5bd06",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "attention_mechanisms",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Write the attention formula with scaling",
        "<b>Formula:</b> softmax(QK^T/√d_k)V<br>\n<b>Symbols:</b> Q=query matrix, K=key matrix, V=value matrix<br>\n<b>Scaling:</b> √d_k prevents vanishing gradients in softmax<br>\n<b>Output:</b> Weighted sum of values based on attention scores",
        "Attention Mechanisms",
        "formula"
      ],
      "guid": "nlp_11e847e058bc6",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "attention_mechanisms",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Why use attention over RNNs?",
        "<b>Parallelization:</b> All positions computed simultaneously<br>\n<b>Long-range:</b> Direct connections between any two positions<br>\n<b>Interpretability:</b> Attention weights show model focus<br>\n<b>Performance:</b> Better at capturing dependencies in long sequences",
        "Attention Mechanisms",
        "comparison"
      ],
      "guid": "nlp_1462479a37b26",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "attention_mechanisms",
        "comparison"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is TF-IDF?",
        "<b>Intuition:</b> Weight terms by frequency and rarity across documents<br>\n<b>Formula:</b> TF-IDF = TF × log(N/df)<br>\n<b>Symbols:</b> TF=term freq, N=total docs, df=docs with term<br>\n<b>Use:</b> Information retrieval and document similarity",
        "TF-IDF",
        "concept"
      ],
      "guid": "nlp_a157a3fc41b6c",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "tf-idf",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Write the TF-IDF formula and explain components",
        "<b>Formula:</b> TF(t,d) × IDF(t,D) = (count/|d|) × log(N/df)<br>\n<b>TF:</b> Term frequency in document (normalized by doc length)<br>\n<b>IDF:</b> Inverse document frequency (rarity measure)<br>\n<b>Result:</b> High for frequent terms in few documents",
        "TF-IDF",
        "formula"
      ],
      "guid": "nlp_9e5503c53d8d9",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "tf-idf",
        "formula"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Common TF-IDF edge case and fix?",
        "<b>Problem:</b> Division by zero when df=0<br>\n<b>Fix:</b> Add smoothing: log(N/(df+1))<br>\n<b>Alternative:</b> Filter out terms not in vocabulary<br>\n<b>Impact:</b> Prevents runtime errors in production",
        "TF-IDF",
        "edge_case"
      ],
      "guid": "nlp_9da18271c6424",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "tf-idf",
        "edge_case"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What are word embeddings?",
        "<b>Intuition:</b> Map discrete tokens to dense continuous vectors<br>\n<b>Property:</b> Similar words have similar vectors<br>\n<b>Training:</b> Word2Vec (skip-gram/CBOW) or context prediction<br>\n<b>Benefit:</b> Captures semantic relationships in vector space",
        "Embeddings",
        "concept"
      ],
      "guid": "nlp_758b782f3d10b",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "embeddings",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Word2Vec vs contextual embeddings?",
        "<b>Word2Vec:</b> One vector per word type (fast, fixed)<br>\n<b>Contextual:</b> Different vectors per word occurrence<br>\n<b>Example:</b> \"bank\" (river) vs \"bank\" (money)<br>\n<b>Trade-off:</b> Contextual more accurate but computationally expensive",
        "Embeddings",
        "comparison"
      ],
      "guid": "nlp_4eaf56e3219f4",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "embeddings",
        "comparison"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Why do transformers work so well?",
        "<b>Parallelization:</b> No sequential bottleneck like RNNs<br>\n<b>Attention:</b> Direct connections between all positions<br>\n<b>Scalability:</b> Performance improves with more data/compute<br>\n<b>Transfer:</b> Pre-trained models adapt well to new tasks",
        "Transformers",
        "concept"
      ],
      "guid": "nlp_b53c77db0a3b5",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "transformers",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Key components of transformer architecture?",
        "<b>Multi-head attention:</b> Parallel attention mechanisms<br>\n<b>Position encoding:</b> Inject sequence order information<br>\n<b>Layer norm + residual:</b> Training stability<br>\n<b>Feed-forward:</b> Non-linear transformations per position",
        "Transformers",
        "architecture"
      ],
      "guid": "nlp_19be557e22714",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "transformers",
        "architecture"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is cosine similarity?",
        "<b>Intuition:</b> Measure angle between vectors (direction, not magnitude)<br>\n<b>Formula:</b> cos(θ) = A·B / (||A|| × ||B||)<br>\n<b>Range:</b> [-1,1] where 1=same direction, 0=orthogonal<br>\n<b>Use:</b> Document similarity, recommendation systems",
        "Similarity",
        "concept"
      ],
      "guid": "nlp_f745833893aaa",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "similarity",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Cosine vs Euclidean similarity?",
        "<b>Cosine:</b> Measures direction (angle) - length independent<br>\n<b>Euclidean:</b> Measures distance - sensitive to magnitude<br>\n<b>Text:</b> Cosine better (document length varies)<br>\n<b>Images:</b> Euclidean often used (pixel intensities matter)",
        "Similarity",
        "comparison"
      ],
      "guid": "nlp_9438a4be44c24",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "similarity",
        "comparison"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What are the gates in LSTM?",
        "<b>Forget:</b> What to discard from cell state<br>\n<b>Input:</b> What new information to store<br>\n<b>Output:</b> What parts of cell state to output<br>\n<b>Purpose:</b> Control information flow and prevent vanishing gradients",
        "Sequence Models",
        "concept"
      ],
      "guid": "nlp_89bcf7fa724ea",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "sequence_models",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the curse of dimensionality in NLP?",
        "<b>Problem:</b> Sparse data in high-dimensional spaces<br>\n<b>Effect:</b> Most vectors are nearly orthogonal<br>\n<b>Solution:</b> Embeddings reduce to dense, lower dimensions<br>\n<b>Example:</b> One-hot vectors vs 300d word embeddings",
        "NLP Fundamentals",
        "concept"
      ],
      "guid": "nlp_e9482b82a4d6e",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "nlp_fundamentals",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "Why do we need tokenization?",
        "<b>Purpose:</b> Split text into meaningful units for processing<br>\n<b>Challenges:</b> Punctuation, contractions, out-of-vocabulary words<br>\n<b>Modern:</b> Subword tokenization (BPE, SentencePiece)<br>\n<b>Benefit:</b> Balance between vocabulary size and coverage",
        "Tokenization",
        "concept"
      ],
      "guid": "nlp_94ccec6358cd0",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "tokenization",
        "concept"
      ]
    },
    {
      "__type__": "Note",
      "fields": [
        "What is the difference between precision and recall?",
        "<b>Precision:</b> TP/(TP+FP) - accuracy of positive predictions<br>\n<b>Recall:</b> TP/(TP+FN) - fraction of positives found<br>\n<b>Trade-off:</b> High precision → low recall, vice versa<br>\n<b>F1:</b> Harmonic mean balances both metrics",
        "Evaluation",
        "concept"
      ],
      "guid": "nlp_74a3c6e971e06",
      "note_model_uuid": "nlp-optimal-model",
      "tags": [
        "evaluation",
        "concept"
      ]
    }
  ]
}