# NLP Interview Question Bank ‚Äî Comprehensive Resource

**The most efficient NLP interview preparation system**: 26 topics, 117+ Anki cards, company-specific guides, and solution patterns.

## üöÄ Quick Start Paths

### By Time Available
- **1 Week**: [Top 10 Problems](#top-10-must-practice) + [Company Guide](COMPANY_SPECIFIC_GUIDE.md)
- **2 Weeks**: [Full Practice Plan](#review-plans) + [Solution Patterns](SOLUTION_PATTERNS.md)
- **1 Month**: Complete all problems + [Anki Memorization](#anki-integration)

### By Target Company
- **OpenAI/Anthropic**: [LLM Track](COMPANY_SPECIFIC_GUIDE.md#-openai--anthropic-llm-focus)
- **Google**: [Search & Scale Track](COMPANY_SPECIFIC_GUIDE.md#-google-search--scale)
- **Meta**: [Applied ML Track](COMPANY_SPECIFIC_GUIDE.md#-meta-social--applied-ml)
- **Amazon**: [Customer Focus Track](COMPANY_SPECIFIC_GUIDE.md#-amazon-product--customer-focus)

### By Learning Style
- **Visual Learner**: [Memory Palace](NLP_MEMORY_PALACE.md)
- **Repetition-Based**: [Anki Decks](#anki-integration)
- **Problem-Solver**: [Solution Patterns](SOLUTION_PATTERNS.md)

## üìö Comprehensive Resources

| Resource | Description | Best For |
|----------|-------------|----------|
| [Question Bank Index](QUESTION_BANK_INDEX.md) | All problems by difficulty, company, topic | Finding specific problems |
| [Company Guide](COMPANY_SPECIFIC_GUIDE.md) | What each company asks, prep strategies | Targeted preparation |
| [Solution Patterns](SOLUTION_PATTERNS.md) | 10 reusable templates for NLP problems | Efficient problem-solving |
| [Quick Reference](INTERVIEW_QUICK_REFERENCE.md) | Key formulas and talking points | Last-minute review |

## Table of Contents
- **Vision**
- **Top 10 Must-Practice**
- **Full Topic Map (Hyperlinked)**
- **Review Plans: 7-Day and 14-Day**
- **Phone Interview Memorization Plan**
- **Anki Integration (Spaced Repetition)**
- **Company-Specific Focus**
- **Common Interview Traps**
- **Checklist and Quick Commands**
- **Success Metrics**

## Vision
The most comprehensive and efficient NLP interview preparation system:
- **26 Topics** covering classical NLP to modern LLMs
- **Multiple Access Patterns**: By difficulty, company, topic, or time
- **Efficient Learning**: Solution patterns, Anki cards, memory techniques
- **Company-Specific**: Tailored guides for FAANG+ companies
- **Interview-Ready**: Heavily commented solutions with follow-ups

### üìä Repository Statistics
- **Total Problems**: 26 core problems + variations
- **Difficulty Distribution**: 7 Easy (20min) ‚Ä¢ 9 Medium (30min) ‚Ä¢ 10 Hard (45min)
- **Anki Cards**: 117+ bite-sized cards for memorization
- **Solution Patterns**: 10 reusable templates
- **Company Coverage**: OpenAI, Google, Meta, Amazon, Microsoft, Apple
- **Time to Master**: 1 week (top 10) ‚Ä¢ 2 weeks (comprehensive) ‚Ä¢ 1 month (expert)

## üéØ Top 10 Must-Practice (Asked in 90% of interviews)

| Rank | Question | Directory | Time | Why Asked |
|------|----------|-----------|------|-----------|
| 1 | Implement TF-IDF from scratch | [TFIDF](NLP/TFIDF/) | 30min | Fundamental IR/NLP math |
| 2 | Build text classifier pipeline | [Text Classification](NLP/Text_Classification/) | 25min | Most practical task |
| 3 | Handle tokenization edge cases | [Tokenization](NLP/Tokenization/) | 15min | Always tested |
| 4 | Implement self-attention | [Attention](NLP/Attention_Mechanisms/) | 25min | Core of transformers |
| 5 | Word2Vec training step | [Embeddings](NLP/Embeddings/) | 20min | Classic representation |
| 6 | Rule-based sentiment analysis | [Sentiment](NLP/Sentiment_Analysis/) | 20min | Business-relevant |
| 7 | Calculate text similarity | [Similarity](NLP/Similarity/) | 25min | Search/recommendation |
| 8 | BPE tokenization algorithm | [Tokenization Advanced](NLP/Tokenization_Advanced/) | 30min | Modern tokenization |
| 9 | BERT fine-tuning setup | [Transformers](NLP/Transformers/) | 30min | Transfer learning |
| 10 | Extract entities with regex | [NER](NLP/NER/) | 25min | Information extraction |

Direct links (Top 10):
- TF-IDF: [Problem](NLP/TFIDF/tfidf_problem.md) ¬∑ [Solution](NLP/TFIDF/tfidf_solution.py)
- Text Classification: [Problem](NLP/Text_Classification/text_classification_problem.md) ¬∑ [Solution](NLP/Text_Classification/text_classification_solution.py)
- Tokenization: [Problem](NLP/Tokenization/tokenization_problem.md) ¬∑ [Solution](NLP/Tokenization/tokenization_solution.py)
- Self-Attention: [Problem](NLP/Attention_Mechanisms/self_attention_problem.md) ¬∑ [Solution](NLP/Attention_Mechanisms/self_attention_solution.py)
- Word2Vec: [Problem](NLP/Embeddings/word2vec_problem.md) ¬∑ [Solution](NLP/Embeddings/word2vec_solution.py)
- Sentiment (VADER): [Problem](NLP/Sentiment_Analysis/vader_sentiment_problem.md) ¬∑ [Solution](NLP/Sentiment_Analysis/vader_sentiment_solution.py)
- Similarity: [Problem](NLP/Similarity/cosine_similarity_problem.md) ¬∑ [Solution](NLP/Similarity/cosine_similarity_solution.py)
- BPE: [Problem](NLP/Tokenization_Advanced/bpe_tokenization_problem.md) ¬∑ [Solution](NLP/Tokenization_Advanced/bpe_tokenization_solution.py)
- BERT Fine-tuning: [Problem](NLP/Transformers/bert_sentiment_problem.md) ¬∑ [Solution](NLP/Transformers/bert_sentiment_solution.py)
- Regex NER: [Problem](NLP/Regex_NLP/regex_patterns_problem.md) ¬∑ [Solution](NLP/Regex_NLP/regex_patterns_solution.py)

## üìö Full Topic Map (Hyperlinked)

- **Attention Mechanisms**: [Problem](NLP/Attention_Mechanisms/self_attention_problem.md) ¬∑ [Solution](NLP/Attention_Mechanisms/self_attention_solution.py)
- **Bag-of-Words Vectors**: [Problem](NLP/BoW_Vectors/bag_of_words_problem.md) ¬∑ [Solution](NLP/BoW_Vectors/bag_of_words_solution.py)
- **CNN for Text**: [Problem](NLP/CNN_Text/cnn_text_classification_problem.md) ¬∑ [Solution](NLP/CNN_Text/cnn_text_classification_solution.py)
- **Embeddings (Word2Vec)**: [Problem](NLP/Embeddings/word2vec_problem.md) ¬∑ [Solution](NLP/Embeddings/word2vec_solution.py)
- **Fine-Tuning (Classification)**: [Problem](NLP/Fine_Tuning/classification_finetuning_problem.md) ¬∑ [Solution](NLP/Fine_Tuning/classification_finetuning_solution.py)
- **GPT Block (Transformer Block)**: [Problem](NLP/GPT_Implementation/gpt_block_problem.md) ¬∑ [Solution](NLP/GPT_Implementation/gpt_block_solution.py)
- **Instruction Tuning**: [Problem](NLP/Instruction_Tuning/instruction_following_problem.md) ¬∑ [Solution](NLP/Instruction_Tuning/instruction_following_solution.py)
- **LLM Fundamentals (Text Generation)**: [Problem](NLP/LLM_Fundamentals/text_generation_problem.md) ¬∑ [Solution](NLP/LLM_Fundamentals/text_generation_solution.py)
- **Model Evaluation (LLMs)**: [Problem](NLP/Model_Evaluation/llm_evaluation_problem.md) ¬∑ [Solution](NLP/Model_Evaluation/llm_evaluation_solution.py)
- **NER**: [Problem](NLP/NER/ner_problem.md) ¬∑ [Solution](NLP/NER/ner_solution.py)
- **Neural Fundamentals (Perceptron)**: [Problem](NLP/Neural_Fundamentals/perceptron_neural_net_problem.md) ¬∑ [Solution](NLP/Neural_Fundamentals/perceptron_neural_net_solution.py)
- **N-grams**: [Problem](NLP/NGrams/ngrams_problem.md) ¬∑ [Solution](NLP/NGrams/ngrams_solution.py)
- **POS Tagging**: [Problem](NLP/POS_Tagging/pos_tagging_problem.md) ¬∑ [Solution](NLP/POS_Tagging/pos_tagging_solution.py)
- **Regex for NLP**: [Problem](NLP/Regex_NLP/regex_patterns_problem.md) ¬∑ [Solution](NLP/Regex_NLP/regex_patterns_solution.py)
- **Sentiment Analysis (VADER)**: [Problem](NLP/Sentiment_Analysis/vader_sentiment_problem.md) ¬∑ [Solution](NLP/Sentiment_Analysis/vader_sentiment_solution.py)
- **Sequence Models (LSTM Sentiment)**: [Problem](NLP/Sequence_Models/lstm_sentiment_problem.md) ¬∑ [Solution](NLP/Sequence_Models/lstm_sentiment_solution.py)
- **Text Similarity**: [Problem](NLP/Similarity/cosine_similarity_problem.md) ¬∑ [Solution](NLP/Similarity/cosine_similarity_solution.py)
- **Stemming & Lemmatization**: [Problem](NLP/Stemming_Lemmatization/stemming_lemmatization_problem.md) ¬∑ [Solution](NLP/Stemming_Lemmatization/stemming_lemmatization_solution.py)
- **Stop Word Removal**: [Problem](NLP/Stop_Word_Removal/stopword_removal_problem.md) ¬∑ [Solution](NLP/Stop_Word_Removal/stopword_removal_solution.py)
- **Text Classification**: [Problem](NLP/Text_Classification/text_classification_problem.md) ¬∑ [Solution](NLP/Text_Classification/text_classification_solution.py)
- **TF-IDF**: [Problem](NLP/TFIDF/tfidf_problem.md) ¬∑ [Solution](NLP/TFIDF/tfidf_solution.py)
- **Tokenization (Rule-based)**: [Problem](NLP/Tokenization/tokenization_problem.md) ¬∑ [Solution](NLP/Tokenization/tokenization_solution.py)
- **Tokenization (BPE)**: [Problem](NLP/Tokenization_Advanced/bpe_tokenization_problem.md) ¬∑ [Solution](NLP/Tokenization_Advanced/bpe_tokenization_solution.py)
- **Topic Modeling (LSA/LDA)**: [Problem](NLP/TopicModeling/lsa_lda_problem.md) ¬∑ [Solution](NLP/TopicModeling/lsa_lda_solution.py)
- **Transformers (BERT Fine-tuning)**: [Problem](NLP/Transformers/bert_sentiment_problem.md) ¬∑ [Solution](NLP/Transformers/bert_sentiment_solution.py)
- **Utilities (Text Normalization)**: [Problem](NLP/Utilities/text_normalization_problem.md) ¬∑ [Solution](NLP/Utilities/text_normalization_solution.py)

## üó∫Ô∏è Review Plans

### 7-Day Crash (Interview in 1 week)
- Day 1: [TF-IDF](NLP/TFIDF/tfidf_problem.md), [Text Similarity](NLP/Similarity/cosine_similarity_problem.md)
- Day 2: [Text Classification](NLP/Text_Classification/text_classification_problem.md)
- Day 3: [Tokenization](NLP/Tokenization/tokenization_problem.md), [Stop Words](NLP/Stop_Word_Removal/stopword_removal_problem.md)
- Day 4: [Self-Attention](NLP/Attention_Mechanisms/self_attention_problem.md)
- Day 5: [Word2Vec](NLP/Embeddings/word2vec_problem.md)
- Day 6: [BPE](NLP/Tokenization_Advanced/bpe_tokenization_problem.md), [Regex NER](NLP/Regex_NLP/regex_patterns_problem.md)
- Day 7: Mock interview + review weak spots

### 14-Day Full Coverage (All problems)
- Day 1: TF-IDF ¬∑ Text Similarity
- Day 2: Text Classification ¬∑ Stop Words
- Day 3: Tokenization ¬∑ Stemming & Lemmatization
- Day 4: POS Tagging ¬∑ NER (Regex)
- Day 5: NER (Statistical) ¬∑ BoW Vectors
- Day 6: N-grams ¬∑ Word2Vec
- Day 7: Self-Attention ¬∑ Transformers (BERT)
- Day 8: GPT Block ¬∑ Text Generation
- Day 9: Tokenization (BPE) ¬∑ Utilities (Normalization)
- Day 10: Sentiment (VADER) ¬∑ CNN for Text
- Day 11: Sequence Models (LSTM) ¬∑ Neural Fundamentals (Perceptron)
- Day 12: Topic Modeling (LSA/LDA) ¬∑ Model Evaluation (LLMs)
- Day 13: Fine-Tuning (Classification) ¬∑ Instruction Tuning
- Day 14: Mixed review + 2 mocks

How to use each day:
- Implement from memory (no peeking) ‚Üí Compare with solution ‚Üí Explain out loud ‚Üí Note follow-ups.

## ‚òéÔ∏è Phone Interview Memorization Plan
Use the dedicated phone screen plan for fast recall, one‚Äëminute scripts, and spaced repetition:
- See: [PHONE_INTERVIEW_STUDY_PLAN.md](PHONE_INTERVIEW_STUDY_PLAN.md)
- Creative memory techniques: [NLP_MEMORY_PALACE.md](NLP_MEMORY_PALACE.md)

## üß† Anki Integration (Spaced Repetition)
**Automatically convert all problems/solutions to Anki flashcards!**

### Standard Deck (30+ cards)
```bash
python convert_to_anki.py  # Original converter
```

### üéØ **Optimized Deck (117+ bite-sized cards)**
```bash
python convert_to_anki_optimized.py  # Better for memorization!
```

**Features:**
- **6 card types per problem**: Understanding, Implementation, Formulas, Complexity, Edge Cases, Insights
- **Bite-sized chunks**: 5-15 lines per card (perfect for mobile)
- **Mobile-optimized CSS**: Readable on any device
- **Atomic concepts**: One idea per card for better retention

**Guides:**
- Full setup: [ANKI_SETUP.md](ANKI_SETUP.md)
- Best practices: [ANKI_BEST_PRACTICES.md](ANKI_BEST_PRACTICES.md)
- Example refactor: [Example_Anki_Refactor](NLP/Example_Anki_Refactor/)

## üè¢ Company-Specific Questions

### OpenAI/Anthropic (LLM companies)
Must practice: Self-attention, GPT block, text generation
- Implement the transformer attention mechanism
- How would you generate text with different sampling strategies?
- Explain instruction tuning vs pre-training

Focus: [Attention](NLP/Attention_Mechanisms/), [GPT Block](NLP/GPT_Implementation/), [Text Generation](NLP/LLM_Fundamentals/)

### Google/Meta (Research + Scale)
Must practice: BERT fine-tuning, text classification, embeddings
- Fine-tune BERT for spam detection
- Build content moderation classifier
- Compare word-level vs subword tokenization

Focus: [BERT](NLP/Transformers/), [Classification](NLP/Text_Classification/), [BPE](NLP/Tokenization_Advanced/)

### Amazon/Microsoft (Product-focused)
Must practice: Search relevance, sentiment, practical NLP
- Rank search results by relevance
- Analyze customer review sentiment
- Build autocomplete system

Focus: [TF-IDF](NLP/TFIDF/), [Sentiment](NLP/Sentiment_Analysis/), [Text Similarity](NLP/Similarity/)

## üö® Common Interview Traps

### TF-IDF
- ‚ùå Use sklearn ‚Üí ‚úÖ Implement from scratch
- ‚ùå Skip IDF intuition ‚Üí ‚úÖ Explain why IDF matters
- ‚ùå Forget edge cases ‚Üí ‚úÖ Handle empty documents

### Attention
- ‚ùå "It's complex" ‚Üí ‚úÖ Implement step-by-step
- ‚ùå Skip scaling factor ‚Üí ‚úÖ Explain why divide by ‚àöd_k
- ‚ùå Forget causal mask ‚Üí ‚úÖ Handle autoregressive case

### Tokenization
- ‚ùå "Just split on spaces" ‚Üí ‚úÖ Handle punctuation, contractions
- ‚ùå Ignore Unicode ‚Üí ‚úÖ Discuss encoding issues
- ‚ùå Miss subword benefits ‚Üí ‚úÖ Explain OOV handling

## üìã Checklist

Before interview:
- [ ] Implement TF-IDF in 30 minutes
- [ ] Explain self-attention clearly
- [ ] Know tokenization trade-offs
- [ ] Build text classification pipeline end-to-end
- [ ] Understand transformer vs RNN trade-offs

Day of interview:
- [ ] Review complexity cheat sheet
- [ ] Explain attention out loud once
- [ ] Rehearse TF-IDF formula
- [ ] Review your edge-case playbook

## ‚ö° Quick Practice Commands

```bash
# Interactive problem finder
python problem_finder.py

# Practice most important problems
python NLP/TFIDF/tfidf_solution.py
python NLP/Attention_Mechanisms/self_attention_solution.py
python NLP/Text_Classification/text_classification_solution.py
python NLP/Tokenization/tokenization_solution.py
python NLP/Embeddings/word2vec_solution.py

# Generate Anki deck
python convert_to_anki_optimized.py
```

## üìä Success Metrics

Minimum (1 week)
- ‚úÖ Top 5 problems completed
- ‚úÖ Can explain transformer attention
- ‚úÖ Know TF-IDF formula by heart
- ‚úÖ Understand tokenization trade-offs

Strong (2 weeks)
- ‚úÖ Top 10 problems mastered
- ‚úÖ Can implement any on whiteboard
- ‚úÖ Know scaling considerations
- ‚úÖ Ready for follow-ups

Exceptional (1 month)
- ‚úÖ All 26 problems completed
- ‚úÖ Can teach concepts to others
- ‚úÖ Up to date with LLM basics
- ‚úÖ Ready for senior interviews

---

Focus on what matters. Master the top 10 first, then cover the rest with the 14-day plan. üéØ
